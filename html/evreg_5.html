<div class="container">

<table style="width: 100%;"><tr>
<td>ENNreg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Training the ENNreg model</h2>

<h3>Description</h3>

<p><code>ENNreg</code> trains the ENNreg model using batch or minibatch learning procedures.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ENNreg(
  X,
  y,
  init = NULL,
  K = NULL,
  batch = TRUE,
  nstart = 100,
  c = 1,
  lambda = 0.9,
  xi = 0,
  rho = 0,
  eps = NULL,
  nu = 1e-16,
  optimProto = TRUE,
  verbose = TRUE,
  options = list(maxiter = 1000, rel.error = 1e-04, print = 10),
  opt.rmsprop = list(batch_size = 100, epsi = 0.001, rho = 0.9, delta = 1e-08, Dtmax =
    100)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input matrix of size n x p, where n is the number of objects and p the number of
attributes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector of length n containing observations of the response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p>Initial model generated by <code>ENNreg_init</code> (default=NULL).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Number of prototypes (default=NULL; must be supplied if initial model is not supplied).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch</code></td>
<td>
<p>If TRUE (default), batch learning is used; otherwise, online learning is
used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nstart</code></td>
<td>
<p>Number of random starts of the k-means algorithm (default: 100, used only if initial
model is not supplied).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c</code></td>
<td>
<p>Multiplicative coefficient applied to scale parameter gamma (defaut: 1, used only if
initial model is not supplied)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Parameter of the loss function (default=0.9)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xi</code></td>
<td>
<p>Regularization coefficient penalizing precision (default=0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>Regularization coefficient shrinking the solution towards a linear model (default=0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Parameter of the loss function (if NULL, set to 0.01 times the standard deviation of y).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>Parameter of the loss function to avoid a division par zero (default=1e-16).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimProto</code></td>
<td>
<p>If TRUE (default), the initial prototypes are optimized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If TRUE (default) intermediate results are displayed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>options</code></td>
<td>
<p>Parameters of the optimization procedure (see details).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opt.rmsprop</code></td>
<td>
<p>Parameters of the RMSprop algorithm (see details).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>batch=TRUE</code>, function <code>harris</code> from package <code>evclust</code> is used for
optimization. Otherwise, the RMSprop minibatch learning algorithm is used. The three
parameters in list <code>options</code> are:
</p>

<dl>
<dt>maxiter</dt>
<dd>
<p>Maximum number of iterations (default: 100).</p>
</dd>
<dt>rel.error</dt>
<dd>
<p>Relative error for stopping criterion (default: 1e-4).</p>
</dd>
<dt>print</dt>
<dd>
<p>Number of iterations between two displays (default: 10).</p>
</dd>
</dl>
<p>Additional parameters for the RMSprop, used only if <code>batch=FALSE</code>, are contained in
list <code>opt.rmsprop</code>. They are:
' </p>

<dl>
<dt>batch_size</dt>
<dd>
<p>Minibatch size.</p>
</dd>
<dt>epsi</dt>
<dd>
<p>Global learning rate.</p>
</dd>
<dt>rho</dt>
<dd>
<p>Decay rate.</p>
</dd>
<dt>delta</dt>
<dd>
<p>Small constant to stabilize division by small numbers.</p>
</dd>
<dt>Dtmax</dt>
<dd>
<p>The algorithm stops when the loss has not decreased in the last Dtmax
iterations.</p>
</dd>
</dl>
<h3>Value</h3>

<p>An object of class "ENNreg"  with the following components:
</p>

<dl>
<dt>loss</dt>
<dd>
<p>Value of the loss function.</p>
</dd>
<dt>param</dt>
<dd>
<p>Parameter values.</p>
</dd>
<dt>K</dt>
<dd>
<p>Number of prototypes.</p>
</dd>
<dt>pred</dt>
<dd>
<p>Predictions on the training set (a list containing the prototype unit activations,
the output means, variances and precisions, as well as the lower and upper expectations).</p>
</dd>
</dl>
<h3>References</h3>

<p>Thierry Denoeux. An evidential neural network model for regression based on random fuzzy
numbers. In "Belief functions: Theory and applications (proc. of BELIEF 2022)", pages 57-66,
Springer, 2022.
</p>
<p>Thierry Denoeux. Quantifying prediction uncertainty in regression using random fuzzy sets: the ENNreg
model. IEEE Transactions on Fuzzy Systems, Vol. 31, Issue 10, pages 3690-3699, 2023.
</p>


<h3>See Also</h3>

<p><code>predict.ENNreg</code>, <code>ENNreg_init</code>, <code>ENNreg_cv</code>,
<code>ENNreg_holdout</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Boston dataset

library(MASS)
X&lt;-as.matrix(scale(Boston[,1:13]))
y&lt;-Boston[,14]
set.seed(220322)
n&lt;-nrow(Boston)
ntrain&lt;-round(0.7*n)
train &lt;-sample(n,ntrain)
fit &lt;- ENNreg(X[train,],y[train],K=30)
plot(y[train],fit$pred$mux,xlab="observed response",ylab="predicted response")


</code></pre>


</div>