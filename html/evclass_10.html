<div class="container">

<table style="width: 100%;"><tr>
<td>proDSfit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Training of the evidential neural network classifier</h2>

<h3>Description</h3>

<p><code>proDSfit</code> performs parameter optimization for the evidential neural network classifier.
</p>


<h3>Usage</h3>

<pre><code class="language-R">proDSfit(
  x,
  y,
  param,
  lambda = 1/max(as.numeric(y)),
  mu = 0,
  optimProto = TRUE,
  options = list(maxiter = 500, eta = 0.1, gain_min = 1e-04, disp = 10)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Input matrix of size n x d, where n is the number of objects and d the number of
attributes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector of class lables (of length n). May be a factor, or a vector of
integers from 1 to M (number of classes).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>param</code></td>
<td>
<p>Initial parameters (see <code>link{proDSinit}</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Parameter of the cost function. If <code>lambda=1</code>, the
cost function measures the error between the plausibilities and the 0-1 target values.
If <code>lambda=1/M</code>, where M is the number of classes (default), the piginistic probabilities
are considered in the cost function. If <code>lambda=0</code>, the beliefs are used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>Regularization hyperparameter (default=0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimProto</code></td>
<td>
<p>Boolean. If TRUE, the prototypes are optimized (default). Otherwise, they are fixed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>options</code></td>
<td>
<p>A list of parameters for the optimization algorithm: maxiter
(maximum number of iterations), eta (initial step of gradient variation),
gain_min (minimum gain in the optimisation loop), disp (integer; if &gt;0, intermediate
results are displayed every disp iterations).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>optimProto=TRUE</code> (default), the prototypes are optimized. Otherwise, they are fixed to
their initial value.
</p>


<h3>Value</h3>

<p>A list with three elements:
</p>

<dl>
<dt>param</dt>
<dd>
<p>Optimized network parameters.</p>
</dd>
<dt>cost</dt>
<dd>
<p>Final value of the cost function.</p>
</dd>
<dt>err</dt>
<dd>
<p>Training error rate.</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>T. Denoeux. A neural network classifier based on Dempster-Shafer theory.
IEEE Trans. on Systems, Man and Cybernetics A, 30(2):131â€“150, 2000.
</p>


<h3>See Also</h3>

<p><code>proDSinit</code>, <code>proDSval</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Glass dataset
data(glass)
xapp&lt;-glass$x[1:89,]
yapp&lt;-glass$y[1:89]
xtst&lt;-glass$x[90:185,]
ytst&lt;-glass$y[90:185]
## Initialization
param0&lt;-proDSinit(xapp,yapp,nproto=7)
## Training
fit&lt;-proDSfit(xapp,yapp,param0)
</code></pre>


</div>