<div class="container">

<table style="width: 100%;"><tr>
<td>betahat.fun</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculates MLE coefficients of linear fit</h2>

<h3>Description</h3>

<p>Determines the maximum likelihood regression coeffients for the
specified regression basis and correlation matrix <code>A</code>.
</p>
<p>The “<code>.A</code>” form needs only <code>A</code> (and not <code>Ainv</code>),
thus removing the need to calculate a matrix inverse.  Note that this
form is <em>slower</em> than the other if <code>Ainv</code> is known in
advance, as <code>solve(.,.)</code> is slow.
</p>
<p>If <code>Ainv</code> is not known in advance, the two forms seem to
perform similarly in the cases considered here and in the
<code>goldstein</code> package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">betahat.fun(xold, Ainv, d, give.variance=FALSE, func)
betahat.fun.A(xold, A, d, give.variance=FALSE, func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xold</code></td>
<td>
<p>Data frame, each line being the parameters of one run</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>Correlation matrix, typically provided by
<code>corr.matrix()</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Ainv</code></td>
<td>
<p>Inverse of the correlation matrix <code>A</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>Vector of results at the points specified in <code>xold</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>give.variance</code></td>
<td>
<p>Boolean, with <code>TRUE</code> meaning to return
information on the variance of <code class="reqn">\hat{\beta}</code> and
default <code>FALSE</code> meaning to return just the estimator</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>func</code></td>
<td>
<p>Function to generate regression basis; defaults to <code>regressor.basis</code></p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Here, the strategy of using two separate functions, eg <code>foo()</code>
and <code>foo.A()</code>, one of which inverts <code>A</code> and one of which
uses notionally more efficient means.  Compare the other
strategy in which a Boolean flag, <code>use.Ainv</code>, has the same
effect.  An example would be <code>scales.likelihood()</code>.
</p>


<h3>Author(s)</h3>

<p>Robin K. S. Hankin</p>


<h3>References</h3>


<ul>
<li>
<p>J. Oakley and A. O'Hagan, 2002. <em>Bayesian Inference for the
Uncertainty Distribution of Computer Model Outputs</em>, Biometrika
89(4), pp769-784
</p>
</li>
<li>
<p>R. K. S. Hankin 2005. <em>Introducing BACCO, an R bundle for
Bayesian analysis of computer code output</em>, Journal of Statistical
Software, 14(16)
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">data(toy)
val &lt;- toy
H &lt;- regressor.multi(val)
d &lt;- apply(H,1,function(x){sum((0:6)*x)})


fish &lt;- rep(2,6)
A &lt;- corr.matrix(val,scales=fish)
Ainv &lt;- solve(A)

# now add suitably correlated Gaussian noise:
d &lt;-  as.vector(rmvnorm(n=1,mean=d, 0.1*A))

betahat.fun(val , Ainv , d)           # should be close to c(0,1:6)


# Now look at the variances:
betahat.fun(val,Ainv,give.variance=TRUE, d)


     # now find the value of the prior expectation (ie the regression
     # plane) at an unknown point:
x.unknown &lt;- rep(0.5 , 6)
regressor.basis(x.unknown) %*% betahat.fun(val, Ainv, d)

     # compare the prior with the posterior
interpolant(x.unknown, d, val, Ainv,scales=fish)
     # Heh, it's the same!  (of course it is, there is no error here!)


     # OK, put some error on the old observations:
d.noisy &lt;- as.vector(rmvnorm(n=1,mean=d,0.1*A))

     # now compute the regression point:
regressor.basis(x.unknown) %*% betahat.fun(val, Ainv, d.noisy)

     # and compare with the output of interpolant():
interpolant(x.unknown, d.noisy, val, Ainv, scales=fish)
     # there is a difference!



     # now try a basis function that has superfluous degrees of freedom.
     # we need a bigger dataset.  Try 100:
val &lt;- latin.hypercube(100,6)
colnames(val) &lt;- letters[1:6]
d &lt;- apply(val,1,function(x){sum((1:6)*x)})
A &lt;- corr.matrix(val,scales=rep(1,6))
Ainv &lt;- solve(A)

    
betahat.fun(val, Ainv, d, func=function(x){c(1,x,x^2)})
     # should be c(0:6 ,rep(0,6).  The zeroes should be zero exactly
     # because the original function didn't include any squares.


## And finally a sanity check:
f &lt;- function(x){c(1,x,x^2)}
jj1 &lt;- betahat.fun(val, Ainv, d, func=f)
jj2 &lt;- betahat.fun.A(val, A, d, func=f)

abs(jj1-jj2)  # should be small

</code></pre>


</div>