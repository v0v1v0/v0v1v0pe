<div class="container">

<table style="width: 100%;"><tr>
<td>ExhaustiveSearch</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Exhaustive feature selection</h2>

<h3>Description</h3>

<p>Performs an exhaustive feature selection. <code>ExhaustiveSearch()</code> is a fast and
scalable implementation of an exhaustive feature selection framework. It is
particularly suited for huge tasks, which would typically not be possible due
to memory limitations. The current version allows to compute linear and
logistic regression models and compare them with respect to AIC or MSE.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ExhaustiveSearch(
  formula,
  data,
  family = NULL,
  performanceMeasure = NULL,
  combsUpTo = NULL,
  nResults = 5000,
  nThreads = NULL,
  testSetIDs = NULL,
  errorVal = -1,
  quietly = FALSE,
  checkLarge = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>An object of class formula (or one that can be coerced to
that class): a symbolic description of the feature and response structure.
All combinations of features on the right hand side of the formula are
evaluated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data.frame (or object coercible by <code>as.data.frame()</code> to a
data.frame) containing the variables in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>A character string naming the family function similar to the
parameter in <code>glm()</code>. Currently options are 'gaussian' or 'binomial'. If
not specified, the function tries to guess it from the response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>performanceMeasure</code></td>
<td>
<p>A character string naming the performance measure
to compare models by. Currently available options are 'AIC' (Akaike's An
Information Criterion) or 'MSE' (Mean Squared Error).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>combsUpTo</code></td>
<td>
<p>An integer of length 1 to set an upper limit to the number
of features in a combination. This can be useful to drastically reduce the
total number of combinations to a feasible size.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nResults</code></td>
<td>
<p>An integer of length 1 to define the size of the final
ranking list. The default (5000) provides a good trade-off of memory usage
and result size. Set this value to <code>Inf</code> to store all models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nThreads</code></td>
<td>
<p>Number of threads to use. The default is to detect the
available number of threads automatically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testSetIDs</code></td>
<td>
<p>A vector of row indices of data, which define the test set
partition. If this parameter is <code>NULL</code> (default), models are trained and
evaluated on the full data set. If it is set, models are trained on
<code>data[-testSetIDs,]</code> and tested on <code>data[testSetIDs,]</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>errorVal</code></td>
<td>
<p>A numeric value defining what performance result is returned
if the model could not be fitted. The default (-1) makes those models
appear at the top of the result ranking.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quietly</code></td>
<td>
<p>logical. If set to <code>TRUE</code> (default), status and runtime
updates are printed to the console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>checkLarge</code></td>
<td>
<p>logical. Very large calls get stopped by a safety net.
This parameter can be used to execute these calls anyway.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>An exhaustive search evaluates all setups of a combinatorial task. In feature
and model selection application, exhaustive searches are often referred to as
<em>optimal</em> search strategies, as they test each setup and therefore ensure to
find the best solution. The main downside of this approach is the possibly
enormous computational complexity of the task. <code>ExhaustiveSearch()</code> provides
an easy to use and efficient framework for these tasks.
</p>
<p>Its main characteristics are:
</p>

<ul>
<li>
<p> Combinations are iteratively generated on the fly,
</p>
</li>
<li>
<p> Model fitting and evalution is performed multi-threaded in C++,
</p>
</li>
<li>
<p> Only a fixed amount of models are stored to keep memory usage small.
</p>
</li>
</ul>
<p>Therefore, the framework of this package is able to evaluate huge tasks of
billions of models, while only being limited by run-time.
</p>
<p>Currently, ordinary linear regression models similar to <code>lm()</code> and logistic
regression models similar to <code>glm()</code> (with parameter <code>family = "binomial"</code>)
can be fitted. The model type is specified via the <code>family</code> parameter. All
model results of the C++ backend are identical to what would be obtained by
<code>glm()</code> or <code>lm()</code>. For that, the logistic regression also uses the same
<a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> optimizer
as <code>glm()</code>.
</p>
<p>To assess the quality of a model, the <code>performanceMeasure</code> options 'AIC'
(Akaike's An Information Criterion) and 'MSE' (Mean Squared Error) are
implemented. Note that the AIC can only be computed on the training data,
while it is recommended for the MSE to be computed on independent test data.
If <code>performanceMeasure</code> is not set, it will be decided according to the
definition of a test data set.
</p>
<p>While this framework is able to handle very large amounts of combinations, an
exhaustive search of every theoretical combination can still be unfeasible.
However, a possible way to drastically limit the total number of combinations
is to define an upper bound for the size of a combination. For example,
evaluating all combinations of 500 features (3.3e150) is obviously
impossible. But if we only consider combinations of up to 3 features, this
number reduces to around 21 million, which could easily be evaluated by this
framework in less than a minute (16 threads). Setting an upper limit is thus
a very powerful option to enable high dimensional analyses. It is implemented
by the parameter <code>combsUpTo</code>.
</p>
<p>A core element of why this framework does not require more memory if tasks
get larger is that at any point the best models are stored in a list of
fixed size. Therefore, sub-optimal models are not saved and do not take space
and time to be handled. The parameter defining the size of the models, which
are actively stored is <code>nResults</code>. Large values here can impair performance
or even cause errors, if the system memory runs out and should always be set
with care. The function will however warn you beforehand if you set a very
large value here.
</p>
<p>The parameter <code>testSetIDs</code> can be used to split the data into a training and
testing partition. If it is not set, all models will be trained and tested on
the full data set. If it is set, the data will be split beforehand into
<code>data[testSetIDs,]</code> and <code>data[-testSetIDs,]</code>.
</p>
<p>The development version of this package can be found at
<a href="https://github.com/RudolfJagdhuber/ExhaustiveSearch">https://github.com/RudolfJagdhuber/ExhaustiveSearch</a>. Issues or requests
are handled on this page.
</p>


<h3>Value</h3>

<p>Object of class <code>ExhaustiveSearch</code> with elements
</p>
<table>
<tr style="vertical-align: top;">
<td><code>nModels</code></td>
<td>
<p>The total number of evaluated models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>runtimeSec</code></td>
<td>
<p>The total runtime of the exhaustive search in seconds.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ranking</code></td>
<td>
<p>A list of the performance values and the featureIDs. The
i-th element of both correspond. The featureIDs refer to the elements of
<code>featureNames</code>. Formatted results of these rankings can e.g. be obtained
with <code>getFeatures()</code>, or <code>resultTable()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>featureNames  </code></td>
<td>
<p>The feature names in the given data.
<code>featureIDs</code> in the ranking element refer to this vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batchInfo</code></td>
<td>
<p>A list of information on the batches, into which the
total task has been partitioned. List elements are the number of batches,
the number of elements per batch, and the combination boundaries that
define the batches.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>setup</code></td>
<td>
<p>A list of input parameters from the function call.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Rudolf Jagdhuber
</p>


<h3>See Also</h3>

<p><code>resultTable()</code>, <code>getFeatures()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Linear Regression on mtcars data
data(mtcars)

##  Exhaustive search of 1023 models compared by AIC
ES &lt;- ExhaustiveSearch(mpg ~ ., data = mtcars, family = "gaussian",
  performanceMeasure = "AIC")
print(ES)

## Same setup, but compared by MSE on a test set partition
testIDs &lt;- sample(nrow(mtcars), round(1/3 * nrow(mtcars)))
ES2 &lt;- ExhaustiveSearch(mpg ~ ., data = mtcars, family = "gaussian",
  performanceMeasure = "MSE", testSetIDs = testIDs)
print(ES2)


## Not run: 
## Logistic Regression on Ionosphere Data
data("Ionosphere", package = "mlbench")

## Only combinations of up to 3 features! -&gt; 5488 models instead of 4 billion
ES3 &lt;- ExhaustiveSearch((Class == "good") ~ ., data = Ionosphere[,-c(1, 2)],
  family = "binomial", combsUpTo = 3)
print(ES3)

## End(Not run)

</code></pre>


</div>