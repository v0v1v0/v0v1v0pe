<div class="container">

<table style="width: 100%;"><tr>
<td>entropy.MillerMadow</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Miller-Madow Entropy Estimator</h2>

<h3>Description</h3>

<p><code>entropy.MillerMadow</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> using the Miller-Madow correction
to the empirical entropy).
</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy.MillerMadow(y, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Miller-Madow entropy estimator (1955) is the bias-corrected empirical
entropy estimate. 
</p>
<p>Note that the Miller-Madow estimator is not a plug-in estimator, hence there
are no explicit underlying bin frequencies.
</p>


<h3>Value</h3>

<p><code>entropy.MillerMadow</code> returns an estimate of the Shannon entropy. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Miller, G.  1955. Note on the bias of information estimates. 
Info. Theory Psychol. Prob. Methods  <b>II-B</b>:95-100.
</p>


<h3>See Also</h3>

<p><code>entropy.empirical</code></p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# estimate entropy using Miller-Madow method
entropy.MillerMadow(y)

# compare to empirical estimate
entropy.empirical(y)
</code></pre>


</div>