<div class="container">

<table style="width: 100%;"><tr>
<td>eztune</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Supervised Learning Function</h2>

<h3>Description</h3>

<p><code>eztune</code> is a function that automatically tunes adaboost, support
vector machines, gradient boosting machines, and elastic net. An
optimization algorithm is used to find a good set of tuning parameters
for the selected model. The function optimizes on a validation dataset,
cross validated accuracy, or resubstitution accuracy.
</p>


<h3>Usage</h3>

<pre><code class="language-R">eztune(
  x,
  y,
  method = "svm",
  optimizer = "hjn",
  fast = TRUE,
  cross = NULL,
  loss = "default"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Matrix or data frame containing the dependent variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector of responses. Can either be a factor or a numeric vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Model to be fit. Choices are "<code>ada</code>" for adaboost,
"<code>en</code>" for elastic net, "<code>gbm</code>" for gradient boosting machines,
and "<code>svm</code>" for support
vector machines.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>Optimization method. Options are "<code>ga</code>" for a genetic
algorithm and "<code>hjn</code>" for a Hooke-Jeeves optimizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fast</code></td>
<td>
<p>Indicates if the function should use a subset of the
observations when optimizing to speed up calculation time. A value
of <code>TRUE</code> will use the smaller of 50% of the data or 200 observations
for model fitting, a number between <code>0</code> and <code>1</code> specifies the
proportion of data to be used to fit the model, and a positive integer
specifies the number of observations to be used to fit the
model. A model is computed using a random selection of data and
the remaining data are used to validate model performance. The
validation error measure is used as the optimization criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cross</code></td>
<td>
<p>If an integer k \&gt; 1 is specified, k-fold cross-validation
is used to fit the model. This method is very slow for large datasets.
This parameter is ignored unless <code>fast = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>The type of loss function used for optimization. Options
for models with a binary response are "<code>class</code>" for classification
error and "<code>auc</code>" for area under the curve. Options for models with a
continuous response are "<code>mse</code>" for mean squared error and
"<code>mae</code>" for mean absolute error. If the option "default" is selected,
or no loss is specified, the classification accuracy will be used for a binary
response model and the MSE will be use for models with a continuous
model.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Function returns an object of class "<code>eztune</code>" which contains
a summary of the tuning parameters for the best model, the best loss
measure achieved (classification accuracy, AUC, MSE, or MAE), and the best
model.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Best loss measure obtained by the optimizer. This is
the measure specified by the user that the optimizer uses to choose a
"best" model (classification accuracy, AUC, MSE, or MAE). Note that
if the default option is used it is the classification
accuracy for a binary response and the MSE for a continuous response.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>Best model found by the optimizer. Adaboost model
comes from package <code>ada</code> (<code>ada</code> object), elastic net model
comes from package <code>glmnet</code> (<code>glmnet</code> object), gbm model
comes from package <code>gbm</code> (<code>gbm.object</code> object), svm (<code>svm</code>
object) model comes from package <code>e1071</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>Number of observations used in model training when
fast option is used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfold</code></td>
<td>
<p>Number of folds used if cross validation is used
for optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Tuning parameter for adaboost.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>Tuning parameter for adaboost.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinkage</code></td>
<td>
<p>Tuning parameter for adaboost and gbm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Tuning parameter for elastic net</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Tuning parameter for elastic net</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.trees</code></td>
<td>
<p>Tuning parameter for gbm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>interaction.depth</code></td>
<td>
<p>Tuning parameter for gbm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.minobsinnode</code></td>
<td>
<p>Tuning parameter for gbm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cost</code></td>
<td>
<p>Tuning parameter for svm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>Tuning parameter for svm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Tuning parameter for svm regression.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>levels</code></td>
<td>
<p>If the model has a binary response, the levels of y are listed.</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">library(mlbench)
data(Sonar)
sonar &lt;- Sonar[sample(1:nrow(Sonar), 100), ]

y &lt;- sonar[, 61]
x &lt;- sonar[, 1:10]

# Optimize an SVM using the default fast setting and Hooke-Jeeves
eztune(x, y)

# Optimize an SVM with 3-fold cross validation and Hooke-Jeeves
eztune(x, y, fast = FALSE, cross = 3)

# Optimize GBM using training set of 50 observations and Hooke-Jeeves
eztune(x, y, method = "gbm", fast = 50, loss = "auc")

# Optimize SVM with 25% of the observations as a training dataset
# using a genetic algorithm
eztune(x, y, method = "svm", optimizer = "ga", fast = 0.25)

</code></pre>


</div>