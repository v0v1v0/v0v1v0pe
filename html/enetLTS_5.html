<div class="container">

<table style="width: 100%;"><tr>
<td>lambda00</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Upper limit of the penalty parameter for <code>family="binomial"</code>
</h2>

<h3>Description</h3>

<p>Use bivariate winsorization to estimate the smallest value of the upper limit for the penalty
parameter.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lambda00(x,y,normalize=TRUE,intercept=TRUE,const=2,prob=0.95,
      tol=.Machine$double.eps^0.5,eps=.Machine$double.eps,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric matrix containing the predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a numeric vector containing the response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>a logical indicating whether the winsorized predictor
variables should be normalized or not (the
default is <code>TRUE</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>a logical indicating whether a constant term should be
included in the model (the default is <code>TRUE</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>const</code></td>
<td>
<p>numeric; tuning constant to be used in univariate
winsorization (the default is 2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prob</code></td>
<td>
<p>numeric; probability for the quantile of the
<code class="reqn">\chi^{2}</code> distribution to be used in bivariate
winsorization (the default is 0.95).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>a small positive numeric value used to determine singularity
issues in the computation of correlation estimates for bivariate
winsorization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>a small positive numeric value used to determine whether the
robust scale estimate of a variable is too small (an effective zero).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments if needed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The estimation procedure is done with similar approach as in Alfons et al. (2013).
But the Pearson correlation between y and the jth predictor variable xj on winsorized data is
replaced to a robustified point-biserial correlation for logistic regression.
</p>


<h3>Value</h3>

<p>A robust estimate of the smallest value of the penalty parameter for
enetLTS regression (for <code>family="binomial"</code>).
</p>


<h3>Note</h3>

<p>For linear regression, we take exactly same procedure as in Alfons et al., which is based on
the Pearson correlation between y and the jth predictor variable xj on winsorized
data. See Alfons et al. (2013).
</p>


<h3>Author(s)</h3>

<p>Fatma Sevinc KURNAZ, Irene HOFFMANN, Peter FILZMOSER
<br> Maintainer: Fatma Sevinc KURNAZ &lt;fatmasevinckurnaz@gmail.com&gt;;&lt;fskurnaz@yildiz.edu.tr&gt;</p>


<h3>References</h3>

<p>Kurnaz, F.S., Hoffmann, I. and Filzmoser, P. (2017) Robust and sparse estimation methods
for high dimensional linear and logistic regression.
<em>Chemometrics and Intelligent Laboratory Systems.</em>
</p>
<p>Alfons, A., Croux, C. and Gelper, S. (2013) Sparse least trimmed squares regression for
analyzing high-dimensional large data sets. <em>The Annals of Applied Statistics</em>, 7(1), 226â€“248.
</p>


<h3>See Also</h3>

<p><code>enetLTS</code>,
<code>sparseLTS</code>,
<code>lambda0</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(86)
n &lt;- 100; p &lt;- 25                             # number of observations and variables
beta &lt;- rep(0,p); beta[1:6] &lt;- 1              # 10% nonzero coefficients
sigma &lt;- 0.5                                  # controls signal-to-noise ratio
x &lt;- matrix(rnorm(n*p, sigma),nrow=n)
e &lt;- rnorm(n,0,1)                             # error terms
eps &lt;-0.05                                    # %10 contamination to only class 0
m &lt;- ceiling(eps*n)
y &lt;- sample(0:1,n,replace=TRUE)
xout &lt;- x
xout[y==0,][1:m,] &lt;- xout[1:m,] + 10;         # class 0
yout &lt;- y                                     # wrong classification for vertical outliers

# compute smallest value of the upper limit for the penalty parameter
l00 &lt;- lambda00(xout,yout)
</code></pre>


</div>