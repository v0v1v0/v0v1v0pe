<div class="container">

<table style="width: 100%;"><tr>
<td>rapt</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Expand an input matrix X using raptObj.</h2>

<h3>Description</h3>

<p>Expand an input matrix X using
a Random Affine Projection Transformation (RAPT) object.
Such objects use random affine projection transformation to the
resulting matrix.  This allows RAPT objects serve as a basis
for a large number of kinds of expansions.  If p are the
number of features of X, and q are number of expanded features,
the applications fall into two broad categories:
</p>

<ul>
<li>
<p>p &gt; q using the Johnson-Lindenstrauss theorem:
</p>

<ul>
<li>
<p>Compressed sensing.
</p>
</li>
<li>
<p>Manifold learning.
</p>
</li>
<li>
<p>Dimension reduction.
</p>
</li>
<li>
<p>Graph embedding.
</p>
</li>
<li>
<p>...
</p>
</li>
</ul>
</li>
<li>
<p>p &lt; q using Bochner's theorem:
</p>

<ul>
<li>
<p>Approximate kernel projection.
</p>
</li>
<li>
<p>Fast approximate SVD.
</p>
</li>
<li>
<p>Estimation of dependence.
</p>
</li>
<li>
<p>...
</p>
</li>
</ul>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">rapt(X, raptObj)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input data matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>raptObj</code></td>
<td>
<p>raptObj generated by raptMake</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Computes
</p>
<p style="text-align: center;"><code class="reqn">X W + b</code>
</p>

<p>where
</p>
<p>W = raptObj$W
</p>
<p>b = raptObj$b
</p>


<h3>Value</h3>

<p>A matrix of randomly (but repeatable) features.
</p>


<h3>References</h3>

<p><a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma</a>,
<a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">https://en.wikipedia.org/wiki/Bochner%27s_theorem</a>
</p>


<h3>See Also</h3>

<p>Details of how the rapt object is built
are in <code>raptMake</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Toy problem
set.seed(1)
nObs &lt;- 100 # Number of observations
X &lt;- matrix(seq(-4,+4,length.out = nObs),ncol=1)
Ytrue &lt;- sin(5*X) + 2*cos(2*X) # True value Ytrue = g(X)
Y &lt;- Ytrue + rnorm(nObs) # Noisy measurement Y

# Standardize X
Xstd &lt;- scale(X)
attributes(Xstd) &lt;- attributes(X)

# Bochner (random Fourier) projection object
nDim &lt;- NCOL(X)
h &lt;- 10 # Estimated by goodness of fit Adj R^2.
  # Normally this would be fit by cross validation.
raptObj &lt;- raptMake(nDim,nDim*200,WdistOpt = list(sd=h),
                    bDistOpt=list(min=-pi,max=+pi))

# Apply raptObj to Xstd to generate features,
# keeping unaltered features Xstd as well.
Xrapt &lt;- cbind( Xstd, cos( rapt(Xstd,raptObj) ) )

# Standardize results
XraptStd &lt;- scale(Xrapt)
attributes(XraptStd) &lt;- attributes(Xrapt)

# A linear fitting of Y to the features XraptStd
lmObj &lt;- lm(Y ~ XraptStd)
summary(lmObj)

# Plot measurements (Y), predictions (Yhat),
# Kernel smoothing with Gaussian kernel and same bandwidth,
# true Y without noise.
Yhat &lt;- predict(lmObj)
plot (X,Y   ,main="Linear Fitting", ylim=c(-6,10))
lines(X,Yhat,col="red",lty=1,lwd=2)
grid(col="darkgray")
kFit &lt;- ksmooth(X,Y,kernel="normal",bandwidth=1/h)
lines(kFit$x,kFit$y,lty=1,col="green",lwd=2)
lines(X,Ytrue,lty=1,col="blue",lwd=2)
legend("topleft",
        legend=c("Noisy measurements",
                 "Estimated Y from RAPT",
                 "Estimated Y from Kernel Smooth",
                 "True Y"),
        col=1:4,
        pch=c( 1,NA,NA,NA),
        lty=c(NA, 1, 1, 1),
        lwd=2,
        bty="n")

# Fit sparse model w/LASSO and
# lambda criteria = 1 standard deviation.
# This avoids overgeneralization errors usually
# associated with fitting large numbers of features
# to relatively few data points.  It also improves
# the end effects, which are of paramount importance
# in high dimensional problems (since by the curse
# of dimensionality, almost all points are close an edge
# in high dimensional problems).
lassoObj &lt;- easyLASSO(XraptStd,Y)
Yhat &lt;- predict(lassoObj, newx = XraptStd)
# Use linear fit of prediction Yhat as goodness of fit.
summary(lm(Y ~ Yhat))

# Plot results of LASSO fitting
# These show LASSO does a better job fitting edges.
plot(X,Y,main="LASSO Fitting",ylim=c(-6,10))
lines(X,Yhat,col="red",lty=1,lwd=2)
grid(col="darkgray")
kFit &lt;- ksmooth(X,Y,kernel="normal",bandwidth=1/h)
lines(kFit$x,kFit$y,lty=1,col="green",lwd=2)
lines(X,Ytrue,lty=1,col="blue",lwd=2)
legend("topleft",
        legend=c("Noisy measurements",
                 "Estimated Y from RAPT",
                 "Estimated Y from Kernel Smooth",
                 "True Y"),
        col=1:4,
        pch=c( 1,NA,NA,NA),
        lty=c(NA, 1, 1, 1),
        lwd=2,
        bty="n")
</code></pre>


</div>