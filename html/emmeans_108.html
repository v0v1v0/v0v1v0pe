<div class="container">

<table style="width: 100%;"><tr>
<td>summary.emmGrid</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Summaries, predictions, intervals, and tests for <code>emmGrid</code> objects</h2>

<h3>Description</h3>

<p>These are the primary methods for obtaining numerical or tabular results from
an <code>emmGrid</code> object. <code>summary.emmGrid</code> is the general function for
summarizing <code>emmGrid</code> objects. It also serves as the print method for
these objects; so for convenience, <code>summary()</code> arguments may be included
in calls to functions such as <code>emmeans</code> and
<code>contrast</code> that construct <code>emmGrid</code> objects. Note that by
default, summaries for Bayesian models are diverted to
<code>hpd.summary</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'emmGrid'
summary(object, infer, level, adjust, by,
  cross.adjust = "none", type, df, calc, null, delta, side, frequentist,
  bias.adjust = get_emm_option("back.bias.adj"), sigma, ...)

## S3 method for class 'emmGrid'
confint(object, parm, level = 0.95, ...)

test(object, null, ...)

## S3 method for class 'emmGrid'
test(object, null = 0, joint = FALSE, verbose = FALSE,
  rows, by, status = FALSE, ...)

## S3 method for class 'emmGrid'
predict(object, type, interval = c("none", "confidence",
  "prediction"), level = 0.95,
  bias.adjust = get_emm_option("back.bias.adj"), sigma, ...)

## S3 method for class 'emmGrid'
as.data.frame(x, row.names = NULL, optional,
  check.names = TRUE, destroy.annotations = FALSE, ...)

## S3 method for class 'summary_emm'
x[..., as.df = FALSE]
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>An object of class <code>"emmGrid"</code> (see emmGrid-class)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>infer</code></td>
<td>
<p>A vector of one or two logical values. The first determines
whether confidence intervals are displayed, and the second determines
whether <em>t</em> tests and <em>P</em> values are displayed. If only one value
is provided, it is used for both.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>level</code></td>
<td>
<p>Numerical value between 0 and 1. Confidence level for confidence
intervals, if <code>infer[1]</code> is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjust</code></td>
<td>
<p>Character value naming the method used to adjust <code class="reqn">p</code> values
or confidence limits; or to adjust comparison arrows in <code>plot</code>. See
the P-value adjustments section below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>by</code></td>
<td>
<p>Character name(s) of variables to use for grouping into separate 
tables. This affects the family of tests considered in adjusted <em>P</em>
values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cross.adjust</code></td>
<td>
<p>Character: <code class="reqn">p</code>-value adjustment method to 
additionally apply <em>across</em> 
the <code>by</code> groups. See the section on P-value adjustments for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Character: type of prediction desired. This only has an effect if
there is a known transformation or link function. <code>"response"</code> 
specifies that the inverse transformation be applied. <code>"mu"</code> (or 
equivalently, <code>"unlink"</code>) is usually the same as <code>"response"</code>,
but in the case where the model has both a link function and a response 
transformation, only the link part is back-transformed. Other valid values 
are <code>"link"</code>, <code>"lp"</code>, and <code>"linear.predictor"</code>; these are
equivalent, and request that results be shown for the linear predictor,
with no back-transformation. The default is <code>"link"</code>, unless the 
<code>"predict.type"</code> option is in force; see <code>emm_options</code>,
and also the section below on transformations and links.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>Numeric. If non-missing, a constant number of degrees of freedom to
use in constructing confidence intervals and <em>P</em> values (<code>NA</code>
specifies asymptotic results).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>calc</code></td>
<td>
<p>Named list of character value(s) or formula(s).
The expressions in <code>char</code> are evaluated and appended to the
summary, just after the <code>df</code> column. The expression may include
any names up through <code>df</code> in the summary, any additional names in 
<code>object@grid</code> (such as <code>.wgt.</code> or <code>.offset.</code>), or any
earlier elements of <code>calc</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>null</code></td>
<td>
<p>Numeric. Null hypothesis value(s), on the linear-predictor scale,
against which estimates are tested. May be a single value used for all, or
a numeric vector of length equal to the number of tests in each family
(i.e., <code>by</code> group in the displayed table).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>Numeric value (on the linear-predictor scale). If zero, ordinary
tests of significance are performed. If positive, this specifies a
threshold for testing equivalence (using the TOST or two-one-sided-test
method), non-inferiority, or non-superiority, depending on <code>side</code>. See
Details for how the test statistics are defined.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>side</code></td>
<td>
<p>Numeric or character value specifying whether the test is
left-tailed (<code>-1</code>, <code>"-"</code>, <code>"&lt;"</code>, <code>"left"</code>, or
<code>"nonsuperiority"</code>); right-tailed (<code>1</code>, <code>"+"</code>, <code>"&gt;"</code>,
<code>"right"</code>, or <code>"noninferiority"</code>); or two-sided (<code>0</code>,
<code>2</code>, <code>"!="</code>, <code>"two-sided"</code>, <code>"both"</code>,
<code>"equivalence"</code>, or <code>"="</code>). See the special section below for
more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>frequentist</code></td>
<td>
<p>Ignored except if a Bayesian model was fitted. If missing
or <code>FALSE</code>, the object is passed to <code>hpd.summary</code>. Otherwise, 
a logical value of <code>TRUE</code> will have it return a frequentist summary.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias.adjust</code></td>
<td>
<p>Logical value for whether to adjust for bias in
back-transforming (<code>type = "response"</code>). This requires a valid value of 
<code>sigma</code> to exist in the object or be specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>Error SD assumed for bias correction (when 
<code>type = "response"</code> and a transformation
is in effect), or for constructing prediction intervals. If not specified,
<code>object@misc$sigma</code> is used, and a warning is issued if it is not found
or not valid.
<em>Note:</em> <code>sigma</code> may be a vector, but be careful that it correctly
corresponds (perhaps after recycling) to the order of the reference grid.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments such as <code>scheffe.rank</code> 
(see “P-value adjustments”). 
In <code>confint.emmGrid</code>, 
<code>predict.emmGrid</code>, and 
<code>test.emmGrid</code>, these arguments are passed to
<code>summary.emmGrid</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parm</code></td>
<td>
<p>(Required argument for <code>confint</code> methods, but not used)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>joint</code></td>
<td>
<p>Logical value. If <code>FALSE</code>, the arguments are passed to 
<code>summary.emmGrid</code> with <code>infer=c(FALSE, TRUE)</code>. If <code>joint = 
TRUE</code>, a joint test of the hypothesis L beta = null is performed, where L 
is <code>object@linfct</code> and beta is the vector of fixed effects estimated 
by <code>object@betahat</code>. This will be either an <em>F</em> test or a 
chi-square (Wald) test depending on whether degrees of freedom are 
available. See also <code>joint_tests</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical value. If <code>TRUE</code> and <code>joint = TRUE</code>, a table
of the effects being tested is printed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rows</code></td>
<td>
<p>Integer values. The rows of L to be tested in the joint test. If
missing, all rows of L are used. If not missing, <code>by</code> variables are
ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>status</code></td>
<td>
<p>logical. If <code>TRUE</code>, a <code>note</code> column showing status
flags (for rank deficiencies and estimability issues) is displayed even 
when empty. If <code>FALSE</code>, the column is included only if there are 
such issues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>interval</code></td>
<td>
<p>Type of interval desired (partial matching is allowed): 
<code>"none"</code> for no intervals,
otherwise confidence or prediction intervals with given arguments, 
via <code>confint.emmGrid</code>. 
Note: prediction intervals are not available
unless the model family is <code>"gaussian"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>object of the given class</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>row.names</code></td>
<td>
<p>passed to <code>as.data.frame</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optional</code></td>
<td>
<p>required argument, but ignored in <code>as.data.frame.emmGrid</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check.names</code></td>
<td>
<p>passed to <code>data.frame</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>destroy.annotations</code></td>
<td>
<p>Logical value. If <code>FALSE</code>, an object of class
<code>summary_emm</code> is returned (which inherits from <code>data.frame</code>),
but if displayed, details like confidence levels, P-value adjustments, 
transformations, etc. are also shown. But unlike the result
of <code>summary</code>, the number of digits displayed
is obtained from <code>getOption("digits")</code> rather than using the
optimal digits algorithm we usually use. Thus, it is formatted more like a 
regular data frame, but with any annotations and groupings still intact.
If <code>TRUE</code> (not recommended), a “plain vanilla” data frame is 
returned, based on <code>row.names</code> and <code>check.names</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>as.df</code></td>
<td>
<p>Logical value. With <code>x[..., as.df = TRUE]</code>, the result is
object is coerced to a <code>data.frame</code> before the subscripting 
is applied. With <code>as.df = FALSE</code>, the result is
returned as a <code>summary_emm</code> object when possible.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>confint.emmGrid</code> is equivalent to <code>summary.emmGrid with 
infer = c(TRUE, FALSE)</code>. The function <code>test.emmGrid</code>, when called with 
<code>joint = FALSE</code>, is equivalent to <code>summary.emmGrid</code> with <code>infer = c(FALSE, TRUE)</code>. 
</p>
<p>With <code>joint = TRUE</code>, <code>test.emmGrid</code> calculates the Wald test of the
hypothesis <code>linfct %*% bhat = null</code>, where <code>linfct</code> and
<code>bhat</code> refer to slots in <code>object</code> (possibly subsetted according to
<code>by</code> or <code>rows</code>). An error is thrown if any row of <code>linfct</code> is
non-estimable. It is permissible for the rows of <code>linfct</code> to be linearly
dependent, as long as <code>null == 0</code>, in which case a reduced set of 
contrasts is tested. Linear dependence and nonzero <code>null</code> cause an 
error. The returned object has an additional <code>"est.fcns"</code> attribute, which
is a list of the linear functions associated with the joint test.
</p>


<h3>Value</h3>

<p><code>summary.emmGrid</code>, <code>confint.emmGrid</code>, and
<code>test.emmGrid</code> return an object of class <code>"summary_emm"</code>, which
is an extension of <code>data.frame</code> but with a special <code>print</code>
method that displays it with custom formatting. For models fitted using
MCMC methods, the call is diverted to <code>hpd.summary</code> (with 
<code>prob</code> set to <code>level</code>, if specified); one may
alternatively use general MCMC summarization tools with the 
results of <code>as.mcmc</code>.
</p>
<p><code>predict</code> returns a vector of predictions for each row of <code>object@grid</code>.
</p>
<p>The <code>as.data.frame</code> method returns an object that inherits 
from <code>"data.frame"</code>.
</p>


<h3>Defaults</h3>

<p>The <code>misc</code> slot in <code>object</code> may contain default values for
<code>by</code>, <code>calc</code>, <code>infer</code>, <code>level</code>, <code>adjust</code>, 
<code>type</code>, <code>null</code>, <code>side</code>, and <code>delta</code>. 
These defaults vary depending
on the code that created the object. The <code>update</code> method may be
used to change these defaults. In addition, any options set using 
‘<span class="samp">⁠emm_options(summary = ...)⁠</span>’ will trump those stored in the object's 
<code>misc</code> slot.
</p>


<h3>Transformations and links</h3>

<p>With <code>type = "response"</code>, the transformation assumed can be found in
‘<span class="samp">⁠object@misc$tran⁠</span>’, and its label, for the summary is in
‘<span class="samp">⁠object@misc$inv.lbl⁠</span>’. Any <code class="reqn">t</code> or <code class="reqn">z</code> tests are still performed
on the scale of the linear predictor, not the inverse-transformed one.
Similarly, confidence intervals are computed on the linear-predictor scale,
then inverse-transformed. 
</p>
<p>Be aware that only univariate transformations and links are
supported in this way. Some multivariate transformations are supported by 
<code>mvregrid</code>.
</p>


<h3>Bias adjustment when back-transforming</h3>

<p>When <code>bias.adjust</code> is <code>TRUE</code>, then back-transformed estimates
are adjusted by adding 
<code class="reqn">0.5 h''(u)\sigma^2</code>, where <code class="reqn">h</code> is the inverse transformation and
<code class="reqn">u</code> is the linear predictor. This is based on a second-order Taylor
expansion. There are better or exact adjustments for certain specific
cases, and these may be incorporated in future updates.
</p>
<p>Note: In certain models, e.g., those with non-gaussian families,
<code>sigma</code> is initialized as <code>NA</code>, and so by default, bias adjustment
is skipped and a warning is issued. You may override this by specifying a
value for <code>sigma</code>. However, <em>with ordinary generalized linear models,
bias adjustment is inappropriate</em> and you should not try to do it. With GEEs and GLMMs,
you probably should <em>not</em> use <code>sigma(model)</code>, and instead you should create an
appropriate value using the estimated random effects, e.g., from <code>VarCorr(model)</code>.
An example is provided in the “transformations” vignette.
</p>


<h3>P-value adjustments</h3>

<p>The <code>adjust</code> argument specifies a multiplicity adjustment for tests or
confidence intervals. This adjustment always is applied <em>separately</em>
to each table or sub-table that you see in the printed output (see
<code>rbind.emmGrid</code> for how to combine tables). If there are non-estimable
cases in a <code>by</code> group, those cases are <em>excluded</em> before determining
the adjustment; that means there could be different adjustments in different groups.
</p>
<p>The valid values of <code>adjust</code> are as follows:
</p>

<dl>
<dt><code>"tukey"</code></dt>
<dd>
<p>Uses the Studentized range distribution with the number
of means in the family. (Available for two-sided cases only.)</p>
</dd>
<dt><code>"scheffe"</code></dt>
<dd>
<p>Computes <code class="reqn">p</code> values from the <code class="reqn">F</code>
distribution, according to the Scheffe critical value of
<code class="reqn">\sqrt{rF(\alpha; r, d)}</code>, where <code class="reqn">d</code> is
the error degrees of freedom and <code class="reqn">r</code> is the rank of the set of linear
functions under consideration. By default, the value of <code>r</code> is
computed from <code>object@linfct</code> for each by group; however, if the
user specifies an argument matching <code>scheffe.rank</code>, its value will
be used instead. Ordinarily, if there are <code class="reqn">k</code> means involved, then
<code class="reqn">r = k - 1</code> for a full set of contrasts involving all <code class="reqn">k</code> means, and
<code class="reqn">r = k</code> for the means themselves. (The Scheffe adjustment is available
for two-sided cases only.)</p>
</dd>
<dt><code>"sidak"</code></dt>
<dd>
<p>Makes adjustments as if the estimates were independent
(a conservative adjustment in many cases).</p>
</dd>
<dt><code>"bonferroni"</code></dt>
<dd>
<p>Multiplies <code class="reqn">p</code> values, or divides significance
levels by the number of estimates. This is a conservative adjustment.</p>
</dd>
<dt><code>"dunnettx"</code></dt>
<dd>
<p>Uses our own<em>ad hoc</em> approximation to the 
Dunnett distribution for a family of estimates having pairwise
correlations of <code class="reqn">0.5</code> (as is true when comparing treatments with a
control with equal sample sizes). The accuracy of the approximation
improves with the number of simultaneous estimates, and is much faster
than <code>"mvt"</code>. (Available for two-sided cases only.)</p>
</dd>
<dt><code>"mvt"</code></dt>
<dd>
<p>Uses the multivariate <code class="reqn">t</code> distribution to assess the
probability or critical value for the maximum of <code class="reqn">k</code> estimates. This
method produces the same <code class="reqn">p</code> values and intervals as the default
<code>summary</code> or <code>confint</code> methods to the results of
<code>as.glht</code>. In the context of pairwise comparisons or comparisons
with a control, this produces “exact” Tukey or Dunnett adjustments,
respectively. However, the algorithm (from the <span class="pkg">mvtnorm</span> package) uses a
Monte Carlo method, so results are not exactly repeatable unless the same
random-number seed is used (see <code>set.seed</code>). As the family
size increases, the required computation time will become noticeable or even
intolerable, making the <code>"tukey"</code>, <code>"dunnettx"</code>, or others more
attractive.</p>
</dd>
<dt><code>"none"</code></dt>
<dd>
<p>Makes no adjustments to the <code class="reqn">p</code> values.</p>
</dd>
</dl>
<p>For tests, not confidence intervals, the Bonferroni-inequality-based adjustment
methods in <code>p.adjust</code> are also available (currently, these
include <code>"holm"</code>, <code>"hochberg"</code>, <code>"hommel"</code>,
<code>"bonferroni"</code>, <code>"BH"</code>, <code>"BY"</code>, <code>"fdr"</code>, and
<code>"none"</code>). If a <code>p.adjust.methods</code> method other than
<code>"bonferroni"</code> or <code>"none"</code> is specified for confidence limits, the
straight Bonferroni adjustment is used instead. Also, if an adjustment method
is not appropriate (e.g., using <code>"tukey"</code> with one-sided tests, or with
results that are not pairwise comparisons), a more appropriate method
(usually <code>"sidak"</code>) is substituted.
</p>
<p>In some cases, confidence and <code class="reqn">p</code>-value adjustments are only approximate
– especially when the degrees of freedom or standard errors vary greatly
within the family of tests. The <code>"mvt"</code> method is always the correct
one-step adjustment, but it can be very slow. One may use
<code>as.glht</code> with methods in the <span class="pkg">multcomp</span> package to obtain
non-conservative multi-step adjustments to tests.
</p>
<p><em>Warning:</em> Non-estimable cases are <em>included</em> in the family to which adjustments
are applied. You may wish to subset the object using the <code>[]</code> operator
to work around this problem.
</p>
<p>The <code>cross.adjust</code> argument is a way of specifying a multiplicity
adjustment across the <code>by</code> groups (otherwise by default, each group is
treated as a separate family in regards to multiplicity adjustments). It
applies only to <code class="reqn">p</code> values. Valid options are one of the
<code>p.adjust.methods</code> or <code>"sidak"</code>. This argument is ignored unless
it is other than <code>"none"</code>, there is more than one <code>by</code> group, and
they are all the same size. Under those conditions, we first use
<code>adjust</code> to determine the within-group adjusted <code class="reqn">p</code> values.
Imagine each group's adjusted <code class="reqn">p</code> values arranged in side-by-side
columns, thus forming a matrix with the number of columns equal to the
number of <code>by</code> groups. Then we use the <code>cross.adjust</code> method to
further adjust the adjusted <code class="reqn">p</code> values in each row of this matrix. Note
that an <em>overall</em> Bonferroni (or Sidak) adjustment is obtainable by
specifying <em>both</em> <code>adjust</code> and <code>cross.adjust</code> as
<code>"bonferroni"</code> (or <code>"sidak"</code>). However, less conservative (but
yet conservative) overall adjustments are available when it is possible to
use an “exact” within-group method (e.g., <code>adjust = "tukey"</code>
for pairwise comparisons) and <code>cross.adjust</code> as a conservative
adjustment. [<code>cross.adjust</code> methods other than <code>"none"</code>, 
<code>"bonferroni"</code>, or <code>"sidak"</code> do not seem advisable, but other 
<code>p.adjust</code> methods are available if you can make sense of them.]
</p>


<h3>Tests of significance, nonsuperiority, noninferiority, or equivalence</h3>

<p>When <code>delta = 0</code>, test statistics are the usual tests of significance.
They are of the form 
‘<span class="samp">⁠(estimate - null)/SE⁠</span>’. Notationally: 
</p>

<dl>
<dt>Significance</dt>
<dd>
<p><code class="reqn">H_0: \theta = \theta_0</code>  versus <br><code class="reqn">H_1: \theta &lt; \theta_0</code> (left-sided), or<br><code class="reqn">H_1 \theta &gt; \theta_0</code> (right-sided), or<br><code class="reqn">H_1: \theta \ne \theta_0</code> (two-sided)<br>
The test statistic is<br><code class="reqn">t = (Q - \theta_0)/SE</code><br> 
where <code class="reqn">Q</code> is our estimate of <code class="reqn">\theta</code>;
then left, right, or two-sided <code class="reqn">p</code> values are produced, 
depending on <code>side</code>.</p>
</dd>
</dl>
<p>When <code>delta</code> is positive, the test statistic depends on <code>side</code> as
follows.
</p>

<dl>
<dt>Left-sided (nonsuperiority)</dt>
<dd>
<p><code class="reqn">H_0: \theta \ge \theta_0 + \delta</code>
versus <code class="reqn">H_1: \theta &lt; \theta_0 + \delta</code><br><code class="reqn">t = (Q - \theta_0 - \delta)/SE</code><br> 
The <code class="reqn">p</code> value is the lower-tail probability.</p>
</dd>
<dt>Right-sided (noninferiority)</dt>
<dd>
<p><code class="reqn">H_0: \theta \le \theta_0 - \delta</code>
versus <code class="reqn">H_1: \theta &gt; \theta_0 - \delta</code><br><code class="reqn">t = (Q - \theta_0 + \delta)/SE</code><br>
The <code class="reqn">p</code> value is the upper-tail probability.</p>
</dd>
<dt>Two-sided (equivalence)</dt>
<dd>
<p><code class="reqn">H_0: |\theta - \theta_0| \ge \delta</code>
versus <code class="reqn">H_1: |\theta - \theta_0| &lt; \delta</code><br><code class="reqn">t = (|Q - \theta_0| - \delta)/SE</code><br>
The <code class="reqn">p</code> value is the <em>lower</em>-tail probability.<br>
Note that <code class="reqn">t</code> is the maximum of <code class="reqn">t_{nonsup}</code> and <code class="reqn">-t_{noninf}</code>. 
This is equivalent to choosing the less 
significant result in the two-one-sided-test (TOST) procedure.</p>
</dd>
</dl>
<h3>Non-estimable cases</h3>

<p>When the model is rank-deficient, each row <code>x</code> of <code>object</code>'s
<code>linfct</code> slot is checked for estimability. If <code>sum(x*bhat)</code>
is found to be non-estimable, then the string <code>NonEst</code> is displayed for the
estimate, and associated statistics are set to <code>NA</code>. 
The estimability check is performed
using the orthonormal basis <code>N</code> in the <code>nbasis</code> slot for the null
space of the rows of the model matrix. Estimability fails when
<code class="reqn">||Nx||^2 / ||x||^2</code> exceeds <code>tol</code>, which by default is
<code>1e-8</code>. You may change it via <code>emm_options</code> by setting
<code>estble.tol</code> to the desired value.
</p>
<p>See the warning above that non-estimable cases are still included when
determining the family size for <em>P</em>-value adjustments.
</p>


<h3>Warning about potential misuse of P values</h3>

<p>Some in the statistical and scientific community argue that
the term “statistical significance” should be completely abandoned, and
that criteria such as “p &lt; 0.05” never be used to assess the
importance of an effect. These practices can be too misleading and are prone to abuse.
See <a href="../doc/basics.html#pvalues">the “basics” vignette</a> for more
discussion.
</p>


<h3>Note</h3>

<p>In doing testing and a transformation and/or link is in force, any
<code>null</code> and/or <code>delta</code> values specified must always be on the
scale of the linear predictor, regardless of the setting for 'type'. If
<code>type = "response"</code>, the null value displayed in the summary table 
will be back-transformed from the value supplied by the user. But the
displayed <code>delta</code> will not be changed, because there (often) is
not a natural way to back-transform it.
</p>
<p>When we have <code>type = "response"</code>, and <code>bias.adj = TRUE</code>,
the <code>null</code> value displayed in the output is both back-transformed
and bias-adjusted, leading to a rather non-intuitive-looking null value.
However, since the tests themselves are performed on the link scale,
this is the response value at which a *P* value of 1 would be obtained.
</p>
<p>The default <code>show</code> method for <code>emmGrid</code> objects (with the
exception of newly created reference grids) is <code>print(summary())</code>.
Thus, with ordinary usage of <code>emmeans</code> and such, it is
unnecessary to call <code>summary</code> unless there is a need to
specify other than its default options.
</p>
<p>If a data frame is needed, <code>summary</code>, <code>confint</code>,
and <code>test</code> serve this need. <code>as.data.frame</code> routes to
<code>summary</code> by default; calling it with <code>destroy.annotations = TRUE</code>
is not recommended for exactly that reason.
If you want to see more digits in the output, use
<code>print(summary(object), digits = ...)</code>; and if you <em>always</em> want
to see more digits, use <code>emm_options(opt.digits = FALSE)</code>.
</p>


<h3>See Also</h3>

<p><code>hpd.summary</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">warp.lm &lt;- lm(breaks ~ wool * tension, data = warpbreaks)
warp.emm &lt;- emmeans(warp.lm, ~ tension | wool)
warp.emm    # implicitly runs 'summary'

confint(warp.emm, by = NULL, level = .90)

# --------------------------------------------------------------
pigs.lm &lt;- lm(log(conc) ~ source + factor(percent), data = pigs)
pigs.emm &lt;- emmeans(pigs.lm, "percent", type = "response")
summary(pigs.emm)    # (inherits type = "response")
summary(pigs.emm, calc = c(n = ".wgt."))  # Show sample size

# For which percents is EMM non-inferior to 35, based on a 10% threshold?
# Note the test is done on the log scale even though we have type = "response"
test(pigs.emm, null = log(35), delta = log(1.10), side = "&gt;")

con &lt;- contrast(pigs.emm, "consec")
test(con)

test(con, joint = TRUE)

# default Scheffe adjustment - rank = 3
summary(con, infer = c(TRUE, TRUE), adjust = "scheffe")

# Consider as some of many possible contrasts among the six cell means
summary(con, infer = c(TRUE, TRUE), adjust = "scheffe", scheffe.rank = 5)

# Show estimates to more digits
print(test(con), digits = 7)

# --------------------------------------------------------------
# Cross-adjusting P values
prs &lt;- pairs(warp.emm)   # pairwise comparisons of tension, by wool
test(prs, adjust = "tukey", cross.adjust = "bonferroni")

# Same comparisons taken as one big family (more conservative)
test(prs, adjust = "bonferroni", by = NULL)

</code></pre>


</div>