<div class="container">

<table style="width: 100%;"><tr>
<td>valstat</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Validation statistics for model assessment</h2>

<h3>Description</h3>

<p>This function helps users to generate 16 fit indices for model assessment based on independent/validation data set. In addition to empirical models, the function <code>valstat()</code> can generate fit indices for AI-based models such as artificial neural network, supervise vector machine, etc.
</p>


<h3>Usage</h3>

<pre><code class="language-R">valstat(obs.y, pred.y, p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>obs.y</code></td>
<td>
<p>observed values from the independent/validation data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred.y</code></td>
<td>
<p>predicted values from the model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>number of parameters in the model. This is needed to compute the 'criteria-based indices' and adjusted coefficient of determination. Users could enter any value for AI-based models with an unknown number of parameters (p) and assess their models using the indices that are invariant of p. See the section on note.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>n: number of observation in the validation data, SSR: residual sum of squares, TRE: total relative error, Bias: mean bias, MRB: mean relative bias, MAB: mean absolute bias, MAPE: mean absolute percentage error, MSE: mean squared error, RMSE: root mean squared error, Percent.RMSE: percentage root mean squared error, R2: coefficient of determination, R2adj: adjusted coefficient of determination, APC: Amemiya's prediction criterion, logL: Log-likelihood, AIC: Akaike information criterion, AICc: corrected Akaike information criterion, BIC: Bayesian information criterion, HQC: Hannan-Quin information criterion.
</p>


<h3>Note</h3>

<p>The lower the better for the SSR, TRE, Bias, MRB, MAB, MAPE, MSE, RMSE, Percent.RMSE, APC, logL, AIC, AICc, BIC and HQC indices. The higher the better for R2 and R2adj indices. Users can choose which indices to use to evaluate their models from the output. Given the difficulty of determining the number of parameters (p) in AI-based models, users might consider using error-based indices, and coefficients of determination (R2).
</p>


<h3>Author(s)</h3>

<p>Ogana F.N. and Corral-Rivas S.
</p>


<h3>See Also</h3>

<p><code>fitstat()</code>, which gives the fit indices of the model based on the fitting data
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(EMAR)

# fitting data
Age &lt;- 1:50
Yield &lt;- exp(6.5 - 39.5/Age)
dat &lt;- data.frame(Age, Yield)

# fit the model to the fitting data
Eq01 &lt;- lm(Yield ~ Age, data=dat)

# independent/validation data
test_data &lt;- data.frame(Age=1:50, Yield=2.5*Age^1.4)

# predict with the model i.e. Eq01, using the independent/validation data
test_data$pred.Yield &lt;- predict(Eq01, test_data)

# Evaluate the quality of the prediction using the 'valstat' function.
# You need the observed and predicted values. Specify the number of parameters in the model.

valstat(test_data$Yield, test_data$pred.Yield, 2)
</code></pre>


</div>