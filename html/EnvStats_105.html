<div class="container">

<table style="width: 100%;"><tr>
<td>elnormAltCensored</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Estimate Parameters for a Lognormal Distribution (Original Scale) Based on Type I Censored Data
</h2>

<h3>Description</h3>

<p>Estimate the mean and coefficient of variation of a
lognormal distribution given a
sample of data that has been subjected to Type I censoring,
and optionally construct a confidence interval for the mean.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  elnormAltCensored(x, censored, method = "mle", censoring.side = "left",
    ci = FALSE, ci.method = "profile.likelihood", ci.type = "two-sided",
    conf.level = 0.95, n.bootstraps = 1000, pivot.statistic = "z", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>numeric vector of observations.  Missing (<code>NA</code>), undefined (<code>NaN</code>), and
infinite (<code>Inf</code>, <code>-Inf</code>) values are allowed but will be removed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>censored</code></td>
<td>

<p>numeric or logical vector indicating which values of <code>x</code> are censored.
This must be the same length as <code>x</code>.  If the mode of <code>censored</code> is
<code>"logical"</code>, <code>TRUE</code> values correspond to elements of <code>x</code> that
are censored, and <code>FALSE</code> values correspond to elements of <code>x</code> that
are not censored.  If the mode of <code>censored</code> is <code>"numeric"</code>,
it must contain only <code>1</code>'s and <code>0</code>'s; <code>1</code> corresponds to
<code>TRUE</code> and <code>0</code> corresponds to <code>FALSE</code>.  Missing (<code>NA</code>)
values are allowed but will be removed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>character string specifying the method of estimation.
</p>
<p>For singly censored data, the possible values are: <br><code>"mle"</code> (maximum likelihood; the default), <br><code>"qmvue"</code> (quasi minimum variance unbiased estimation), <br><code>"bcmle"</code> (bias-corrected maximum likelihood), <br><code>"rROS"</code> or <code>"impute.w.qq.reg"</code> (moment estimation based on imputation using
quantile-quantile regression; also called <em>robust regression on order statistics</em>
and abbreviated rROS), <br><code>"impute.w.qq.reg.w.cen.level"</code> (moment estimation based on imputation
using the <code>qq.reg.w.cen.level method</code>), <br><code>"impute.w.mle"</code> (moment estimation based on imputation using the mle), and <br><code>"half.cen.level"</code> (moment estimation based on setting the censored
observations to half the censoring level).
</p>
<p>For multiply censored data, the possible values are: <br><code>"mle"</code> (maximum likelihood; the default), <br><code>"qmvue"</code> (quasi minimum variance unbiased estimation), <br><code>"bcmle"</code> (bias-corrected maximum likelihood), <br><code>"rROS"</code> or <code>"impute.w.qq.reg"</code> (moment estimation based on imputation using
quantile-quantile regression; also called <em>robust regression on order statistics</em>
and abbreviated rROS), and <br><code>"half.cen.level"</code> (moment estimation based on setting the censored
observations to half the censoring level).
</p>
<p>See the DETAILS section for more information.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>censoring.side</code></td>
<td>

<p>character string indicating on which side the censoring occurs.  The possible
values are <code>"left"</code> (the default) and <code>"right"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci</code></td>
<td>

<p>logical scalar indicating whether to compute a confidence interval for the
mean or variance.  The default value is <code>ci=FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci.method</code></td>
<td>

<p>character string indicating what method to use to construct the confidence interval
for the mean.  The possible values are
<code>"profile.likelihood"</code> (profile likelihood; the default), <br><code>"cox"</code> (Cox's approximation), <br><code>"delta"</code> (normal approximation based on the delta method), <br><code>"normal.approx"</code> (normal approximation), and <br><code>"bootstrap"</code> (based on bootstrapping).
</p>
<p>The confidence interval method <code>"profile.likelihood"</code> is valid only
when <code>method="mle"</code>. <br>
The confidence interval methods <code>"delta"</code> and <code>"cox"</code> are valid only
when <code>method</code> is one of <code>"mle"</code>, <code>"bcmle"</code>, or <code>"qmvue"</code>.  <br>
The confidence interval method <code>"normal.approx"</code> is valid only when
<code>method</code> is one of <code>"rROS"</code>, <code>"impute.w.qq.reg"</code>,
<code>"impute.w.qq.reg.w.cen.level"</code>, <code>"impute.w.mle"</code>, or
<code>"half.cen.level"</code>.
</p>
<p>See the DETAILS section for more information.
This argument is ignored if <code>ci=FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci.type</code></td>
<td>

<p>character string indicating what kind of confidence interval to compute.  The
possible values are <code>"two-sided"</code> (the default), <code>"lower"</code>, and
<code>"upper"</code>.  This argument is ignored if <code>ci=FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf.level</code></td>
<td>

<p>a scalar between 0 and 1 indicating the confidence level of the confidence interval.
The default value is <code>conf.level=0.95</code>. This argument is ignored if
<code>ci=FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.bootstraps</code></td>
<td>

<p>numeric scalar indicating how many bootstraps to use to construct the
confidence interval for the mean when <code>ci.type="bootstrap"</code>.  This
argument is ignored if <code>ci=FALSE</code> and/or <code>ci.method</code> does not
equal <code>"bootstrap"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pivot.statistic</code></td>
<td>

<p>character string indicating which pivot statistic to use in the construction
of the confidence interval for the mean when <code>ci.method</code> is equal to
<code>"delta"</code>, <code>"cox"</code>, or <code>"normal.approx"</code> (see the DETAILS section).
The possible
values are <code>pivot.statistic="z"</code> (the default) and <code>pivot.statistic="t"</code>.
When <code>pivot.statistic="t"</code> you may supply the argument
<code>ci.sample size</code> (see below).  The argument <code>pivot.statistic</code> is
ignored if <code>ci=FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>additional arguments to pass to other functions.
</p>

<ul>
<li> <p><code>prob.method</code>.  Character string indicating what method to use to
compute the plotting positions (empirical probabilities) when <code>method</code>
is one of <code>"rROS"</code>, <code>"impute.w.qq.reg"</code>, <code>"impute.w.qq.reg.w.cen.level"</code>, or
<code>"impute.w.mle"</code>.  Possible values are
<code>"kaplan-meier"</code> (product-limit method of Kaplan and Meier (1958)),
<code>"nelson"</code> (hazard plotting method of Nelson (1972)),
<code>"michael-schucany"</code> (generalization of the product-limit method due to Michael and Schucany (1986)), and
<code>"hirsch-stedinger"</code> (generalization of the product-limit method due to Hirsch and Stedinger (1987)).
The default value is <code>prob.method="hirsch-stedinger"</code>.  The <code>"nelson"</code>
method is only available for <code>censoring.side="right"</code>.
See the DETAILS section and the help file for <code>ppointsCensored</code>
for more information.
</p>
</li>
<li> <p><code>plot.pos.con</code>.  Numeric scalar between 0 and 1 containing the
value of the plotting position constant to use when <code>method</code> is one of
<code>"rROS"</code>, <br><code>"impute.w.qq.reg"</code>, <code>"impute.w.qq.reg.w.cen.level"</code>, or
<code>"impute.w.mle"</code>.  The default value is <code>plot.pos.con=0.375</code>.
See the DETAILS section and the help file for <code>ppointsCensored</code>
for more information.
</p>
</li>
<li> <p><code>ci.sample.size</code>.  Numeric scalar indicating what sample size to
assume to construct the confidence interval for the mean if
<code>pivot.statistic="t"</code> and <code>ci.method</code> is equal to
<code>"delta"</code>, <code>"cox"</code>, or <code>"normal.approx"</code>.
When <code>method</code> equals
<code>"mle"</code>, <code>"bcmle"</code>, or <code>"qmvue"</code>, the default value is the
expected number of
uncensored observations, otherwise it is the observed number of
uncensored observations.
</p>
</li>
<li> <p><code>lb.impute</code>.  Numeric scalar indicating the lower bound for imputed
observations when method is one of <code>"rROS"</code>, <code>"impute.w.qq.reg"</code>, <br><code>"impute.w.qq.reg.w.cen.level"</code>, or <code>"impute.w.mle"</code>.
Imputed values smaller than this
value will be set to this value.  The default is <code>lb.impute=-Inf</code>.
</p>
</li>
<li> <p><code>ub.impute</code>.  Numeric scalar indicating the upper bound for imputed
observations when method is one of <code>"rROS"</code>, <code>"impute.w.qq.reg"</code>, <br><code>"impute.w.qq.reg.w.cen.level"</code>, or <code>"impute.w.mle"</code>.
Imputed values larger than this value
will be set to this value.  The default is <code>ub.impute=Inf</code>.
</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>x</code> or <code>censored</code> contain any missing (<code>NA</code>), undefined (<code>NaN</code>) or
infinite (<code>Inf</code>, <code>-Inf</code>) values, they will be removed prior to
performing the estimation.
</p>
<p>Let <code class="reqn">\underline{x}</code> be a vector of <code class="reqn">n</code> observations from a
lognormal distribution with
parameters <code>mean=</code><code class="reqn">\theta</code> and <code>cv=</code><code class="reqn">\tau</code>.  Let <code class="reqn">\eta</code> denote the
standard deviation of this distribution, so that <code class="reqn">\eta = \theta \tau</code>.  Set
<code class="reqn">\underline{y} = log(\underline{x})</code>.  Then <code class="reqn">\underline{y}</code> is a vector of observations
from a normal distribution with parameters <code>mean=</code><code class="reqn">\mu</code> and <code>sd=</code><code class="reqn">\sigma</code>.
See the help file for LognormalAlt for the relationship between
<code class="reqn">\theta, \tau, \eta, \mu</code>, and <code class="reqn">\sigma</code>.
</p>
<p>Let <code class="reqn">\underline{x}</code> denote a vector of <code class="reqn">N</code> observations from a
lognormal distribution with parameters
<code>mean=</code><code class="reqn">\theta</code> and <code>cv=</code><code class="reqn">\tau</code>.  Let <code class="reqn">\eta</code> denote the
standard deviation of this distribution, so that <code class="reqn">\eta = \theta \tau</code>.  Set
<code class="reqn">\underline{y} = log(\underline{x})</code>.  Then <code class="reqn">\underline{y}</code> is a
vector of observations from a normal distribution with parameters
<code>mean=</code><code class="reqn">\mu</code> and <code>sd=</code><code class="reqn">\sigma</code>.  See the help file for
LognormalAlt for the relationship between
<code class="reqn">\theta, \tau, \eta, \mu</code>, and <code class="reqn">\sigma</code>.
</p>
<p>Assume <code class="reqn">n</code> (<code class="reqn">0 &lt; n &lt; N</code>) of the <code class="reqn">N</code> observations are known and
<code class="reqn">c</code> (<code class="reqn">c=N-n</code>) of the observations are all censored below (left-censored)
or all censored above (right-censored) at <code class="reqn">k</code> fixed censoring levels
</p>
<p style="text-align: center;"><code class="reqn">T_1, T_2, \ldots, T_k; \; k \ge 1 \;\;\;\;\;\; (1)</code>
</p>

<p>For the case when <code class="reqn">k \ge 2</code>, the data are said to be Type I
<b><em>multiply censored</em></b>.  For the case when <code class="reqn">k=1</code>,
set <code class="reqn">T = T_1</code>.  If the data are left-censored
and all <code class="reqn">n</code> known observations are greater
than or equal to <code class="reqn">T</code>, or if the data are right-censored and all <code class="reqn">n</code>
known observations are less than or equal to <code class="reqn">T</code>, then the data are
said to be Type I <b><em>singly censored</em></b> (Nelson, 1982, p.7), otherwise
they are considered to be Type I multiply censored.
</p>
<p>Let <code class="reqn">c_j</code> denote the number of observations censored below or above censoring
level <code class="reqn">T_j</code> for <code class="reqn">j = 1, 2, \ldots, k</code>, so that
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^k c_j = c \;\;\;\;\;\; (2)</code>
</p>

<p>Let <code class="reqn">x_{(1)}, x_{(2)}, \ldots, x_{(N)}</code> denote the “ordered” observations,
where now “observation” means either the actual observation (for uncensored
observations) or the censoring level (for censored observations).  For
right-censored data, if a censored observation has the same value as an
uncensored one, the uncensored observation should be placed first.
For left-censored data, if a censored observation has the same value as an
uncensored one, the censored observation should be placed first.
</p>
<p>Note that in this case the quantity <code class="reqn">x_{(i)}</code> does not necessarily represent
the <code class="reqn">i</code>'th “largest” observation from the (unknown) complete sample.
</p>
<p>Finally, let <code class="reqn">\Omega</code> (omega) denote the set of <code class="reqn">n</code> subscripts in the
“ordered” sample that correspond to uncensored observations.
<br></p>
<p><b>ESTIMATION</b> <br>
This section explains how each of the estimators of <code>mean=</code><code class="reqn">\theta</code> and
<code>cv=</code><code class="reqn">\tau</code> are computed.  The approach is to first compute estimates of
<code class="reqn">\theta</code> and <code class="reqn">\eta^2</code> (the mean and variance of the lognormal distribution),
say <code class="reqn">\hat{\theta}</code> and <code class="reqn">\hat{\eta}^2</code>, then compute the estimate of the cv
<code class="reqn">\tau</code> by <code class="reqn">\hat{\tau} = \hat{\eta}/\hat{\theta}</code>.
</p>
<p><em>Maximum Likelihood Estimation</em> (<code>method="mle"</code>) <br>
The maximum likelihood estimators of <code class="reqn">\theta</code>, <code class="reqn">\tau</code>, and <code class="reqn">\eta</code> are
computed as:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}_{mle} = exp(\hat{\mu}_{mle} + \frac{\hat{\sigma}^2_{mle}}{2}) \;\;\;\;\;\; (3)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\tau}_{mle} = [exp(\hat{\sigma}^2_{mle})  - 1]^{1/2} \;\;\;\;\;\; (4)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\eta}_{mle} = \hat{\theta}_{mle} \; \hat{\tau}_{mle} \;\;\;\;\;\; (5)</code>
</p>

<p>where <code class="reqn">\hat{\mu}_{mle}</code> and <code class="reqn">\hat{\sigma}_{mle}</code> denote the maximum
likelihood estimators of <code class="reqn">\mu</code> and <code class="reqn">\sigma</code>.  See the help for for
<code>enormCensored</code> for information on how <code class="reqn">\hat{\mu}_{mle}</code> and
<code class="reqn">\hat{\sigma}_{mle}</code> are computed.
<br></p>
<p><em>Quasi Minimum Variance Unbiased Estimation Based on the MLE's</em> (<code>method="qmvue"</code>) <br>
The maximum likelihood estimators of <code class="reqn">\theta</code> and <code class="reqn">\eta^2</code> are biased.
Even for complete (uncensored) samples these estimators are biased
(see equation (12) in the help file for <code>elnormAlt</code>).
The bias tends to 0 as the sample size increases, but it can be considerable for
small sample sizes.
(Cohn et al., 1989, demonstrate the bias for complete data sets.)
For the case of complete samples, the minimum variance unbiased estimators (mvue's)
of <code class="reqn">\theta</code> and <code class="reqn">\eta^2</code> were derived by Finney (1941) and are discussed in
Gilbert (1987, pp.164-167) and Cohn et al. (1989).  These estimators are computed as:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}_{mvue} = e^{\bar{y}} g_{n-1}(\frac{s^2}{2}) \;\;\;\;\;\; (6)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\eta}^2_{mvue} = e^{2 \bar{y}} \{g_{n-1}(2s^2) - g_{n-1}[\frac{(n-2)s^2}{n-1}]\} \;\;\;\;\;\; (7)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \;\;\;\;\;\; (8)</code>
</p>

<p style="text-align: center;"><code class="reqn">s^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \;\;\;\;\;\; (9)</code>
</p>

<p style="text-align: center;"><code class="reqn">g_m(z) = \sum_{i=0}^\infty \frac{m^i (m+2i)}{m(m+2) \cdots (m+2i)} (\frac{m}{m+1})^i (\frac{z^i}{i!}) \;\;\;\;\;\; (10)</code>
</p>

<p>(see the help file for <code>elnormAlt</code>).
</p>
<p>For Type I censored samples, the quasi minimum variance unbiased estimators
(qmvue's) of <code class="reqn">\theta</code> and <code class="reqn">\eta^2</code> are computed using equations (6) and (7)
and estimating <code class="reqn">\mu</code> and <code class="reqn">\sigma</code> with their mle's (see
<code>elnormCensored</code>).
</p>
<p>For singly censored data, this is apparently the LM method of Gilliom and Helsel
(1986, p.137) (it is not clear from their description on page 137 whether their
LM method is the straight <code>method="mle"</code> described above or
<code>method="qmvue"</code> described here).  This method was also used by
Newman et al. (1989, p.915, equations 10-11).
</p>
<p>For multiply censored data, this is apparently the MM method of Helsel and Cohn
(1988, p.1998).  (It is not clear from their description on page 1998 and the
description in Gilliom and Helsel, 1986, page 137 whether Helsel and Cohn's (1988)
MM method is the straight <code>method="mle"</code> described above or <code>method="qmvue"</code>
described here.)
<br></p>
<p><em>Bias-Corrected Maximum Likelihood Estimation</em> (<code>method="bcmle"</code>) <br>
This method was derived by El-Shaarawi (1989) and can be applied to complete or
censored data sets.  For complete data, the exact relative bias of the mle of
the mean <code class="reqn">\theta</code> is given as:
</p>
<p style="text-align: center;"><code class="reqn">B_{mle} = \frac{E[\hat{\theta}_{mle}]}{\theta} = exp[\frac{-(n-1)\sigma^2}{2n}] (1 - \frac{\sigma^2}{n})^{-(n-1)/2} \;\;\;\;\;\; (11)</code>
</p>

<p>(see equation (12) in the help file for <code>elnormAlt</code>).
</p>
<p>For the case of complete or censored data, El-Shaarawi (1989) proposed the
following “bias-corrected” maximum likelihood estimator:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}_{bcmle} = \frac{\hat{\theta}_{mle}}{\hat{B}_{mle}} \;\;\;\;\;\; (12)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\hat{B}_{mle} = exp[\frac{1}{2}(\hat{V}_{11} + 2\hat{\sigma}_{mle} \hat{V}_{12} + \hat{\sigma}^2_{mle} \hat{V}_{22})] \;\;\;\;\;\; (13)</code>
</p>

<p>and <code class="reqn">V</code> denotes the asymptotic variance-covariance of the mle's of <code class="reqn">\mu</code>
and <code class="reqn">\sigma</code>, which is based on the observed information matrix, formulas for
which are given in Cohen (1991).  El-Shaarawi (1989) does not propose a
bias-corrected estimator of the variance <code class="reqn">\eta^2</code>, so the mle of <code class="reqn">\eta</code>
is computed when <code>method="bcmle"</code>.
<br></p>
<p><em>Robust Regression on Order Statistics</em> (<code>method="rROS"</code>) or <br><em>Imputation Using Quantile-Quantile Regression</em> (<code>method ="impute.w.qq.reg"</code>) <br>
This is the robust Regression on Order Statistics (rROS) method discussed in USEPA (2009)
and Helsel (2012).  This method involves using quantile-quantile regression on the
log-transformed observations to fit a regression line (and thus initially estimate the mean
<code class="reqn">\mu</code> and standard deviation <code class="reqn">\sigma</code> in log-space), imputing the
log-transformed values of the <code class="reqn">c</code> censored observations by predicting them
from the regression equation, transforming the log-scale imputed values back to
the original scale, and then computing the method of moments estimates of the
mean and standard deviation based on the observed and imputed values.
</p>
<p>The steps are:
</p>

<ol>
<li>
<p> Estimate <code class="reqn">\mu</code> and <code class="reqn">\sigma</code> by computing the least-squares
estimates in the following model:
</p>
<p style="text-align: center;"><code class="reqn">y_{(i)} = \mu + \sigma \Phi^{-1}(p_i) + \epsilon_i, \; i \in \Omega \;\;\;\;\;\; (14)</code>
</p>

<p>where <code class="reqn">p_i</code> denotes the plotting position associated with the <code class="reqn">i</code>'th
largest value, <code class="reqn">a</code> is a constant such that <code class="reqn">0 \le a \le 1</code>
(the default value is 0.375), <code class="reqn">\Phi</code> denotes the cumulative
distribution function (cdf) of the standard normal distribution and
<code class="reqn">\Omega</code> denotes the set of <code class="reqn">n</code> subscripts associated with the
uncensored observations in the ordered sample.  The plotting positions are
computed by calling the function <code>ppointsCensored</code>.
<br></p>
</li>
<li>
<p> Compute the log-scale imputed values as:
</p>
<p style="text-align: center;"><code class="reqn">\hat{y}_{(i)} = \hat{\mu}_{qqreg} + \hat{\sigma}_{qqreg} \Phi^{-1}(p_i), \; i \not \in \Omega \;\;\;\;\;\; (15)</code>
</p>

<p><br></p>
</li>
<li>
<p> Retransform the log-scale imputed values:
</p>
<p style="text-align: center;"><code class="reqn">\hat{x}_{(i)} = exp[\hat{y}_{(i)}], \; i \not \in \Omega  \;\;\;\;\;\; (16)</code>
</p>

<p><br></p>
</li>
<li>
<p> Compute the usual method of moments estimates of the mean and variance.
</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta} = \frac{1}{N} [\sum_{i \not \in \Omega} \hat{x}_{(i)} + \sum_{i \in \Omega} x_{(i)}] \;\;\;\;\;\; (17)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\eta}^2 = \frac{1}{N-1} [\sum_{i \not \in \Omega} (\hat{x}_{(i)} - \hat{\theta}^2) + \sum_{i \in \Omega} (x_{(i)} - \hat{\theta}^2)] \;\;\;\;\;\; (18)</code>
</p>

<p>Note that the estimate of variance is actually the usual unbiased one
(not the method of moments one) in the case of complete data.
</p>
</li>
</ol>
<p>For sinlgy censored data, this method is discussed by Hashimoto and Trussell (1983),
Gilliom and Helsel (1986), and El-Shaarawi (1989), and is referred to as the LR
(Log-Regression) or Log-Probability Method.
</p>
<p>For multiply censored data, this is the MR method of Helsel and Cohn (1988, p.1998).
They used it with the probability method of Hirsch and Stedinger (1987) and
Weibull plotting positions (i.e., <code>prob.method="hirsch-stedinger"</code> and
<code>plot.pos.con=0</code>).
</p>
<p>The argument <code>plot.pos.con</code> (see the entry for ... in the ARGUMENTS
section above) determines the value of the plotting positions computed in
equations (14) and (15) when <code>method</code> equals <code>"hirsch-stedinger"</code> or
<code>"michael-schucany"</code>.  The default value is <code>plot.pos.con=0.375</code>.
See the help file for <code>ppointsCensored</code> for more information.
</p>
<p>The arguments <code>lb.impute</code> and <code>ub.impute</code> (see the entry for ... in
the ARGUMENTS section above) determine the lower and upper bounds for the
imputed values.  Imputed values smaller than <code>lb.impute</code> are set to this
value.  Imputed values larger than <code>ub.impute</code> are set to this value.
The default values are <code>lb.impute=0</code> and <code>ub.impute=Inf</code>.
<br></p>
<p><em>Imputation Using Quantile-Quantile Regression Including the Censoring Level</em><br>
(<code>method ="impute.w.qq.reg.w.cen.level"</code>) <br><b>This method is only available for sinlgy censored data</b>. This method was
proposed by El-Shaarawi (1989), which he denoted as the Modified LR Method.
It is exactly the same method as imputation
using quantile-quantile regression (<code>method="impute.w.qq.reg"</code>), except that
the quantile-quantile regression includes the censoring level.  For left singly
censored data, the modification involves adding the point
<code class="reqn">[\Phi^{-1}(p_c), T]</code> to the plot before fitting the least-squares line.
For right singly censored data, the point  <code class="reqn">[\Phi^{-1}(p_{n+1}), T]</code>
is added to the plot before fitting the least-squares line.
<br></p>
<p><em>Imputation Using Maximum Likelihood</em> (<code>method ="impute.w.mle"</code>) <br><b>This method is only available for sinlgy censored data</b>.
This is exactly the same method as robust Regression on Order Statistics (i.e.,
the same as using <code>method="rROS"</code> or <br><code>method="impute.w.qq.reg"</code>),
except that the maximum likelihood method (<code>method="mle"</code>) is used to compute
the initial estimates of the mean and standard deviation.
In the context of lognormal data, this method is discussed
by El-Shaarawi (1989), which he denotes as the Modified Maximum Likelihood Method.
<br></p>
<p><em>Setting Censored Observations to Half the Censoring Level</em> (<code>method="half.cen.level"</code>) <br>
This method is applicable only to left censored data that is bounded below by 0.
This method involves simply replacing all the censored observations with half their
detection limit, and then computing the usual moment estimators of the mean and
variance.  That is, all censored observations are imputed to be half the detection
limit, and then Equations (17) and (18) are used to estimate the mean and varaince.
</p>
<p>This method is included only to allow comparison of this method to other methods.
<b><em>Setting left-censored observations to half the censoring level is not
recommended</em></b>.  In particular, El-Shaarawi and Esterby (1992) show that these
estimators are biased and inconsistent (i.e., the bias remains even as the sample
size increases).
<br><br></p>
<p><b>CONFIDENCE INTERVALS</b> <br>
This section explains how confidence intervals for the mean <code class="reqn">\theta</code> are
computed.
f<br></p>
<p><em>Likelihood Profile</em> (<code>ci.method="profile.likelihood"</code>) <br>
This method was proposed by Cox (1970, p.88), and Venzon and Moolgavkar (1988)
introduced an efficient method of computation.  This method is also discussed by
Stryhn and Christensen (2003) and Royston (2007).
The idea behind this method is to invert the likelihood-ratio test to obtain a
confidence interval for the mean <code class="reqn">\theta</code> while treating the coefficient of
variation <code class="reqn">\tau</code> as a nuisance parameter.
</p>
<p>For Type I left censored data, the likelihood function is given by:
</p>
<p style="text-align: center;"><code class="reqn">L(\theta, \tau | \underline{x}) = {N \choose c_1 c_2 \ldots c_k n} \prod_{j=1}^k [F(T_j)]^{c_j} \prod_{i \in \Omega} f[x_{(i)}] \;\;\;\;\;\; (19)</code>
</p>

<p>where <code class="reqn">f</code> and <code class="reqn">F</code> denote the probability density function (pdf) and
cumulative distribution function (cdf) of the population. That is,
</p>
<p style="text-align: center;"><code class="reqn">f(t) = \phi(\frac{t-\mu}{\sigma}) \;\;\;\;\;\; (20)</code>
</p>

<p style="text-align: center;"><code class="reqn">F(t) = \Phi(\frac{t-\mu}{\sigma}) \;\;\;\;\;\; (21)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\mu = log(\frac{\theta}{\sqrt{\tau^2 + 1}}) \;\;\;\;\;\; (22)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma = [log(\tau^2 + 1)]^{1/2} \;\;\;\;\;\; (23)</code>
</p>

<p>and <code class="reqn">\phi</code> and <code class="reqn">\Phi</code> denote the pdf and cdf of the standard normal
distribution, respectively (Cohen, 1963; 1991, pp.6, 50).  For left singly
censored data, equation (3) simplifies to:
</p>
<p style="text-align: center;"><code class="reqn">L(\mu, \sigma | \underline{x}) = {N \choose c} [F(T)]^{c} \prod_{i = c+1}^n f[x_{(i)}] \;\;\;\;\;\; (24)</code>
</p>

<p>Similarly, for Type I right censored data, the likelihood function is given by:
</p>
<p style="text-align: center;"><code class="reqn">L(\mu, \sigma | \underline{x}) = {N \choose c_1 c_2 \ldots c_k n} \prod_{j=1}^k [1 - F(T_j)]^{c_j} \prod_{i \in \Omega} f[x_{(i)}] \;\;\;\;\;\; (25)</code>
</p>

<p>and for right singly censored data this simplifies to:
</p>
<p style="text-align: center;"><code class="reqn">L(\mu, \sigma | \underline{x}) = {N \choose c} [1 - F(T)]^{c} \prod_{i = 1}^n f[x_{(i)}] \;\;\;\;\;\; (26)</code>
</p>

<p>Following Stryhn and Christensen (2003), denote the maximum likelihood estimates
of the mean and coefficient of variation by <code class="reqn">(\theta^*, \tau^*)</code>.
The likelihood ratio test statistic (<code class="reqn">G^2</code>) of the hypothesis
<code class="reqn">H_0: \theta = \theta_0</code> (where <code class="reqn">\theta_0</code> is a fixed value) equals the
drop in <code class="reqn">2 log(L)</code> between the “full” model and the reduced model with
<code class="reqn">\theta</code> fixed at <code class="reqn">\theta_0</code>, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">G^2 = 2 \{log[L(\theta^*, \tau^*)] - log[L(\theta_0, \tau_0^*)]\} \;\;\;\;\;\; (27)</code>
</p>

<p>where <code class="reqn">\tau_0^*</code> is the maximum likelihood estimate of <code class="reqn">\tau</code> for the
reduced model (i.e., when <code class="reqn">\theta = \theta_0</code>).  Under the null hypothesis,
the test statistic <code class="reqn">G^2</code> follows a
chi-squared distribution with 1 degree of freedom.
</p>
<p>Alternatively, we may
express the test statistic in terms of the profile likelihood function <code class="reqn">L_1</code>
for the mean <code class="reqn">\theta</code>, which is obtained from the usual likelihood function by
maximizing over the parameter <code class="reqn">\tau</code>, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">L_1(\theta) = max_{\tau} L(\theta, \tau) \;\;\;\;\;\; (28)</code>
</p>

<p>Then we have
</p>
<p style="text-align: center;"><code class="reqn">G^2 = 2 \{log[L_1(\theta^*)] - log[L_1(\theta_0)]\} \;\;\;\;\;\; (29)</code>
</p>

<p>A two-sided <code class="reqn">(1-\alpha)100\%</code> confidence interval for the mean <code class="reqn">\theta</code>
consists of all values of <code class="reqn">\theta_0</code> for which the test is not significant at
level <code class="reqn">alpha</code>:
</p>
<p style="text-align: center;"><code class="reqn">\theta_0: G^2 \le \chi^2_{1, {1-\alpha}} \;\;\;\;\;\; (30)</code>
</p>

<p>where <code class="reqn">\chi^2_{\nu, p}</code> denotes the <code class="reqn">p</code>'th quantile of the
chi-squared distribution with <code class="reqn">\nu</code> degrees of freedom.
One-sided lower and one-sided upper confidence intervals are computed in a similar
fashion, except that the quantity <code class="reqn">1-\alpha</code> in Equation (30) is replaced with
<code class="reqn">1-2\alpha</code>.
<br></p>
<p><b><em>Direct Normal Approximations</em></b> (<code>ci.method="delta"</code> or <code>ci.method="normal.approx"</code>) <br>
An approximate <code class="reqn">(1-\alpha)100\%</code> confidence interval for <code class="reqn">\theta</code> can be
constructed assuming the distribution of the estimator of <code class="reqn">\theta</code> is
approximately normally distributed.  That is, a two-sided <code class="reqn">(1-\alpha)100\%</code>
confidence interval for <code class="reqn">\theta</code> is constructed as:
</p>
<p style="text-align: center;"><code class="reqn">[\hat{\theta} - t_{1-\alpha/2, m-1}\hat{\sigma}_{\hat{\theta}}, \; \hat{\theta} + t_{1-\alpha/2, m-1}\hat{\sigma}_{\hat{\theta}}] \;\;\;\;\;\; (31)</code>
</p>

<p>where <code class="reqn">\hat{\theta}</code> denotes the estimate of <code class="reqn">\theta</code>,
<code class="reqn">\hat{\sigma}_{\hat{\theta}}</code> denotes the estimated asymptotic standard
deviation of the estimator of <code class="reqn">\theta</code>, <code class="reqn">m</code> denotes the assumed sample
size for the confidence interval, and <code class="reqn">t_{p,\nu}</code> denotes the <code class="reqn">p</code>'th
quantile of Student's t-distribuiton with <code class="reqn">\nu</code>
degrees of freedom.  One-sided confidence intervals are computed in a
similar fashion.
</p>
<p>The argument <code>ci.sample.size</code> determines the value of <code class="reqn">m</code> (see
see the entry for ... in the ARGUMENTS section above).
When <code>method</code> equals <code>"mle"</code>, <code>"qmvue"</code>, or <code>"bcmle"</code>
and the data are singly censored, the default value is the
expected number of uncensored observations, otherwise it is <code class="reqn">n</code>,
the observed number of uncensored observations.  This is simply an ad-hoc
method of constructing confidence intervals and is not based on any
published theoretical results.
</p>
<p>When <code>pivot.statistic="z"</code>, the <code class="reqn">p</code>'th quantile from the
standard normal distribution is used in place of the
<code class="reqn">p</code>'th quantile from Student's t-distribution.
</p>
<p><em>Direct Normal Approximation Based on the Delta Method</em> (<code>ci.method="delta"</code>) <br>
This method is usually applied with the maximum likelihood estimators
(<code>method="mle"</code>).  It should also work approximately for the quasi minimum
variance unbiased estimators (<code>method="qmvue"</code>) and the bias-corrected maximum
likelihood estimators (<code>method="bcmle"</code>).
</p>
<p>When <code>method="mle"</code>, the variance of the mle of <code class="reqn">\theta</code> can be estimated
based on the variance-covariance matrix of the mle's of <code class="reqn">\mu</code> and <code class="reqn">\sigma</code>
(denoted <code class="reqn">V</code>), and the delta method:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}^2_{\hat{\theta}} = (\frac{\partial \theta}{\partial \underline{\lambda}})^{'}_{\hat{\underline{\lambda}}} \hat{V} (\frac{\partial \theta}{\partial \underline{\lambda}})_{\hat{\underline{\lambda}}} \;\;\;\;\;\; (32)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\underline{\lambda}' = (\mu, \sigma) \;\;\;\;\;\; (33)</code>
</p>

<p style="text-align: center;"><code class="reqn">\frac{\partial \theta}{\partial \mu} = exp(\mu + \frac{\sigma^2}{2}) \;\;\;\;\;\; (34)</code>
</p>

<p style="text-align: center;"><code class="reqn">\frac{\partial \theta}{\partial \sigma} = \sigma exp(\mu + \frac{\sigma^2}{2}) \;\;\;\;\;\; (35)</code>
</p>

<p>(Shumway et al., 1989).  The variance-covariance matrix <code class="reqn">V</code> of the mle's of
<code class="reqn">\mu</code> and <code class="reqn">\sigma</code> is estimated based on the inverse of the observed Fisher
Information matrix, formulas for which are given in Cohen (1991).
<br></p>
<p><em>Direct Normal Approximation Based on the Moment Estimators</em> (<code>ci.method="normal.approx"</code>) <br>
This method is valid only for the moment estimators based on imputed values <br>
(i.e., <code>method="impute.w.qq.reg"</code> or <code>method="half.cen.level"</code>).  For
these cases, the standard deviation of the estimated mean is assumed to be
approximated by
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}_{\hat{\theta}} = \frac{\hat{\eta}}{\sqrt{m}} \;\;\;\;\;\; (36)</code>
</p>

<p>where, as already noted, <code class="reqn">m</code> denotes the assumed sample size.
This is simply an ad-hoc method of constructing confidence intervals and is not
based on any published theoretical results.
<br></p>
<p><em>Cox's Method</em> (<code>ci.method="cox"</code>) <br>
This method may be applied with the maximum likelihood estimators
(<code>method="mle"</code>), the quasi minimum variance unbiased estimators
(<code>method="qmvue"</code>), and the bias-corrected maximum likelihood estimators
(<code>method="bcmle"</code>).
</p>
<p>This method was proposed by El-Shaarawi (1989) and is an extension of the
method derived by Cox and presented in Land (1972) for the case of
complete data (see the explanation of <code>ci.method="cox"</code> in the help file
for <code>elnormAlt</code>).  The idea is to construct an approximate
<code class="reqn">(1-\alpha)100\%</code> confidence interval for the quantity
</p>
<p style="text-align: center;"><code class="reqn">\beta = exp(\mu + \frac{\sigma^2}{2}) \;\;\;\;\;\; (37)</code>
</p>

<p>assuming the estimate of <code class="reqn">\beta</code>
</p>
<p style="text-align: center;"><code class="reqn">\hat{\beta} = exp(\hat{\mu} + \frac{\hat{\sigma}^2}{2}) \;\;\;\;\;\; (38)</code>
</p>

<p>is approximately normally distributed, and then exponentiate the confidence limits.
That is, a two-sided <code class="reqn">(1-\alpha)100\%</code> confidence interval for <code class="reqn">\theta</code>
is constructed as:
</p>
<p style="text-align: center;"><code class="reqn">[ exp(\hat{\beta} - h), \; exp(\hat{\beta} + h) ]\;\;\;\;\;\; (39)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">h = t_{1-\alpha/2, m-1}\hat{\sigma}_{\hat{\beta}} \;\;\;\;\;\; (40)</code>
</p>

<p>and <code class="reqn">\hat{\sigma}_{\hat{\beta}}</code> denotes the estimated asymptotic standard
deviation of the estimator of <code class="reqn">\beta</code>, <code class="reqn">m</code> denotes the assumed sample
size for the confidence interval, and <code class="reqn">t_{p,\nu}</code> denotes the <code class="reqn">p</code>'th
quantile of Student's t-distribuiton with <code class="reqn">\nu</code>
degrees of freedom.
</p>
<p>El-Shaarawi (1989) shows that the standard deviation of the mle of <code class="reqn">\beta</code> can
be estimated by:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}_{\hat{\beta}} = \sqrt{ \hat{V}_{11} + 2 \hat{\sigma} \hat{V}_{12} + \hat{\sigma}^2 \hat{V}_{22} } \;\;\;\;\;\; (41)</code>
</p>

<p>where <code class="reqn">V</code> denotes the variance-covariance matrix of the mle's of <code class="reqn">\mu</code> and
<code class="reqn">\sigma</code> and is estimated based on the inverse of the Fisher Information matrix.
</p>
<p>One-sided confidence intervals are computed in a similar fashion.
<br></p>
<p><b><em>Bootstrap and Bias-Corrected Bootstrap Approximation</em></b> (<code>ci.method="bootstrap"</code>) <br>
The bootstrap is a nonparametric method of estimating the distribution
(and associated distribution parameters and quantiles) of a sample statistic,
regardless of the distribution of the population from which the sample was drawn.
The bootstrap was introduced by Efron (1979) and a general reference is
Efron and Tibshirani (1993).
</p>
<p>In the context of deriving an approximate <code class="reqn">(1-\alpha)100\%</code> confidence interval
for the population mean <code class="reqn">\theta</code>, the bootstrap can be broken down into the
following steps:
</p>

<ol>
<li>
<p> Create a bootstrap sample by taking a random sample of size <code class="reqn">N</code> from
the observations in <code class="reqn">\underline{x}</code>, where sampling is done with
replacement.  Note that because sampling is done with replacement, the same
element of <code class="reqn">\underline{x}</code> can appear more than once in the bootstrap
sample.  Thus, the bootstrap sample will usually not look exactly like the
original sample (e.g., the number of censored observations in the bootstrap
sample will often differ from the number of censored observations in the
original sample).
</p>
</li>
<li>
<p> Estimate <code class="reqn">\theta</code> based on the bootstrap sample created in Step 1, using
the same method that was used to estimate <code class="reqn">\theta</code> using the original
observations in <code class="reqn">\underline{x}</code>. Because the bootstrap sample usually
does not match the original sample, the estimate of <code class="reqn">\theta</code> based on the
bootstrap sample will usually differ from the original estimate based on
<code class="reqn">\underline{x}</code>.
</p>
</li>
<li>
<p> Repeat Steps 1 and 2 <code class="reqn">B</code> times, where <code class="reqn">B</code> is some large number.
The number of bootstraps <code class="reqn">B</code> is determined by the argument
<code>n.bootstraps</code> (see the section ARGUMENTS above).
The default value of <code>n.bootstraps</code> is <code>1000</code>.
</p>
</li>
<li>
<p> Use the <code class="reqn">B</code> estimated values of <code class="reqn">\theta</code> to compute the empirical
cumulative distribution function of this estimator of <code class="reqn">\theta</code> (see
<code>ecdfPlot</code>), and then create a confidence interval for <code class="reqn">\theta</code>
based on this estimated cdf.
</p>
</li>
</ol>
<p>The two-sided percentile interval (Efron and Tibshirani, 1993, p.170) is computed as:
</p>
<p style="text-align: center;"><code class="reqn">[\hat{G}^{-1}(\frac{\alpha}{2}), \; \hat{G}^{-1}(1-\frac{\alpha}{2})] \;\;\;\;\;\; (42)</code>
</p>

<p>where <code class="reqn">\hat{G}(t)</code> denotes the empirical cdf evaluated at <code class="reqn">t</code> and thus
<code class="reqn">\hat{G}^{-1}(p)</code> denotes the <code class="reqn">p</code>'th empirical quantile, that is,
the <code class="reqn">p</code>'th quantile associated with the empirical cdf.  Similarly, a one-sided lower
confidence interval is computed as:
</p>
<p style="text-align: center;"><code class="reqn">[\hat{G}^{-1}(\alpha), \; \infty] \;\;\;\;\;\; (43)</code>
</p>

<p>and a one-sided upper confidence interval is computed as:
</p>
<p style="text-align: center;"><code class="reqn">[0, \; \hat{G}^{-1}(1-\alpha)] \;\;\;\;\;\; (44)</code>
</p>

<p>The function <code>elnormAltCensored</code> calls the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function <code>quantile</code>
to compute the empirical quantiles used in Equations (42)-(44).
</p>
<p>The percentile method bootstrap confidence interval is only first-order
accurate (Efron and Tibshirani, 1993, pp.187-188), meaning that the probability
that the confidence interval will contain the true value of <code class="reqn">\theta</code> can be
off by <code class="reqn">k/\sqrt{N}</code>, where <code class="reqn">k</code>is some constant.  Efron and Tibshirani
(1993, pp.184-188) proposed a bias-corrected and accelerated interval that is
second-order accurate, meaning that the probability that the confidence interval
will contain the true value of <code class="reqn">\theta</code> may be off by <code class="reqn">k/N</code> instead of
<code class="reqn">k/\sqrt{N}</code>.  The two-sided bias-corrected and accelerated confidence interval is
computed as:
</p>
<p style="text-align: center;"><code class="reqn">[\hat{G}^{-1}(\alpha_1), \; \hat{G}^{-1}(\alpha_2)] \;\;\;\;\;\; (45)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\alpha_1 = \Phi[\hat{z}_0 + \frac{\hat{z}_0 + z_{\alpha/2}}{1 - \hat{a}(z_0 + z_{\alpha/2})}] \;\;\;\;\;\; (46)</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha_2 = \Phi[\hat{z}_0 + \frac{\hat{z}_0 + z_{1-\alpha/2}}{1 - \hat{a}(z_0 + z_{1-\alpha/2})}] \;\;\;\;\;\; (47)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{z}_0 = \Phi^{-1}[\hat{G}(\hat{\theta})] \;\;\;\;\;\; (48)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{a} = \frac{\sum_{i=1}^N (\hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)})^3}{6[\sum_{i=1}^N (\hat{\theta}_{(\cdot)} - \hat{\theta}_{(i)})^2]^{3/2}} \;\;\;\;\;\; (49)</code>
</p>

<p>where the quantity <code class="reqn">\hat{\theta}_{(i)}</code> denotes the estimate of <code class="reqn">\theta</code> using
all the values in <code class="reqn">\underline{x}</code> except the <code class="reqn">i</code>'th one, and
</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}{(\cdot)} = \frac{1}{N} \sum_{i=1}^N \hat{\theta_{(i)}} \;\;\;\;\;\; (50)</code>
</p>

<p>A one-sided lower confidence interval is given by:
</p>
<p style="text-align: center;"><code class="reqn">[\hat{G}^{-1}(\alpha_1), \; \infty] \;\;\;\;\;\; (51)</code>
</p>

<p>and a one-sided upper confidence interval is given by:
</p>
<p style="text-align: center;"><code class="reqn">[0, \; \hat{G}^{-1}(\alpha_2)] \;\;\;\;\;\; (52)</code>
</p>

<p>where <code class="reqn">\alpha_1</code> and <code class="reqn">\alpha_2</code> are computed as for a two-sided confidence
interval, except <code class="reqn">\alpha/2</code> is replaced with <code class="reqn">\alpha</code> in Equations (51) and (52).
</p>
<p>The constant <code class="reqn">\hat{z}_0</code> incorporates the bias correction, and the constant
<code class="reqn">\hat{a}</code> is the acceleration constant.  The term “acceleration” refers
to the rate of change of the standard error of the estimate of <code class="reqn">\theta</code> with
respect to the true value of <code class="reqn">\theta</code> (Efron and Tibshirani, 1993, p.186).  For a
normal (Gaussian) distribution, the standard error of the estimate of <code class="reqn">\theta</code>
does not depend on the value of <code class="reqn">\theta</code>, hence the acceleration constant is not
really necessary.
</p>
<p>When <code>ci.method="bootstrap"</code>, the function <code>elnormAltCensored</code> computes both
the percentile method and bias-corrected and accelerated method bootstrap confidence
intervals.
</p>
<p>This method of constructing confidence intervals for censored data was studied by
Shumway et al. (1989).
</p>


<h3>Value</h3>

<p>a list of class <code>"estimateCensored"</code> containing the estimated parameters
and other information.  See <code>estimateCensored.object</code> for details.
</p>


<h3>Note</h3>

<p>A sample of data contains censored observations if some of the observations are
reported only as being below or above some censoring level.  In environmental
data analysis, Type I left-censored data sets are common, with values being
reported as “less than the detection limit” (e.g., Helsel, 2012).  Data
sets with only one censoring level are called <em>singly censored</em>; data sets with
multiple censoring levels are called <em>multiply</em> or <em>progressively censored</em>.
</p>
<p>Statistical methods for dealing with censored data sets have a long history in the
field of survival analysis and life testing.  More recently, researchers in the
environmental field have proposed alternative methods of computing estimates and
confidence intervals in addition to the classical ones such as maximum likelihood
estimation.
</p>
<p>Helsel (2012, Chapter 6) gives an excellent review of past studies of the
properties of various estimators based on censored environmental data.
</p>
<p>In practice, it is better to use a confidence interval for the mean or a
joint confidence region for the mean and standard deviation, rather than rely on a
single point-estimate of the mean.  Since confidence intervals and regions depend
on the properties of the estimators for both the mean and standard deviation, the
results of studies that simply evaluated the performance of the mean and standard
deviation separately cannot be readily extrapolated to predict the performance of
various methods of constructing confidence intervals and regions.  Furthermore,
for several of the methods that have been proposed to estimate the mean based on
type I left-censored data, standard errors of the estimates are not available,
hence it is not possible to construct confidence intervals
(El-Shaarawi and Dolan, 1989).
</p>
<p>Few studies have been done to evaluate the performance of methods for constructing
confidence intervals for the mean or joint confidence regions for the mean and
standard deviation <b>on the original scale, not the log-scale,</b> when data are
subjected to single or multiple censoring.  See, for example, Singh et al. (2006).
</p>


<h3>Author(s)</h3>

<p>Steven P. Millard (<a href="mailto:EnvStats@ProbStatInfo.com">EnvStats@ProbStatInfo.com</a>)
</p>


<h3>References</h3>

<p>Bain, L.J., and M. Engelhardt. (1991).  <em>Statistical Analysis of
Reliability and Life-Testing Models</em>.  Marcel Dekker, New York, 496pp.
</p>
<p>Cohen, A.C. (1959).  Simplified Estimators for the Normal Distribution When
Samples are Singly Censored or Truncated.  <em>Technometrics</em> <b>1</b>(3), 217–237.
</p>
<p>Cohen, A.C. (1963).  Progressively Censored Samples in Life Testing.
<em>Technometrics</em> <b>5</b>, 327–339
</p>
<p>Cohen, A.C. (1991).  <em>Truncated and Censored Samples</em>.  Marcel Dekker,
New York, New York, 312pp.
</p>
<p>Cox, D.R. (1970).  <em>Analysis of Binary Data</em>.  Chapman &amp; Hall, London.  142pp.
</p>
<p>Efron, B. (1979).  Bootstrap Methods: Another Look at the Jackknife.
<em>The Annals of Statistics</em> <b>7</b>, 1–26.
</p>
<p>Efron, B., and R.J. Tibshirani. (1993).  <em>An Introduction to the Bootstrap</em>.
Chapman and Hall, New York, 436pp.
</p>
<p>El-Shaarawi, A.H. (1989).  Inferences About the Mean from Censored Water Quality
Data.  <em>Water Resources Research</em> <b>25</b>(4) 685–690.
</p>
<p>El-Shaarawi, A.H., and D.M. Dolan. (1989).  Maximum Likelihood Estimation of
Water Quality Concentrations from Censored Data.
<em>Canadian Journal of Fisheries and Aquatic Sciences</em> <b>46</b>, 1033–1039.
</p>
<p>El-Shaarawi, A.H., and S.R. Esterby. (1992).  Replacement of Censored Observations
by a Constant: An Evaluation.  <em>Water Research</em> <b>26</b>(6), 835–844.
</p>
<p>El-Shaarawi, A.H., and A. Naderi. (1991).  Statistical Inference from Multiply
Censored Environmental Data.
<em>Environmental Monitoring and Assessment</em> <b>17</b>, 339–347.
</p>
<p>Gibbons, R.D., D.K. Bhaumik, and S. Aryal. (2009).
<em>Statistical Methods for Groundwater Monitoring</em>, Second Edition.
John Wiley &amp; Sons, Hoboken.
</p>
<p>Gilliom, R.J., and D.R. Helsel. (1986).  Estimation of Distributional Parameters for
Censored Trace Level Water Quality Data: 1. Estimation Techniques.
<em>Water Resources Research</em> <b>22</b>, 135–146.
</p>
<p>Gleit, A. (1985).  Estimation for Small Normal Data Sets with Detection Limits.
<em>Environmental Science and Technology</em> <b>19</b>, 1201–1206.
</p>
<p>Haas, C.N., and P.A. Scheff. (1990).  Estimation of Averages in Truncated Samples.
<em>Environmental Science and Technology</em> <b>24</b>(6), 912–919.
</p>
<p>Hashimoto, L.K., and R.R. Trussell. (1983).  Evaluating Water Quality Data Near
the Detection Limit.  Paper presented at the Advanced Technology Conference,
American Water Works Association, Las Vegas, Nevada, June 5-9, 1983.
</p>
<p>Helsel, D.R. (1990).  Less than Obvious: Statistical Treatment of Data Below the
Detection Limit.  <em>Environmental Science and Technology</em> <b>24</b>(12),
1766–1774.
</p>
<p>Helsel, D.R. (2012). <em>Statistics for Censored Environmental Data Using Minitab and R,
Second Edition</em>.  John Wiley &amp; Sons, Hoboken, New Jersey.
</p>
<p>Helsel, D.R., and T.A. Cohn. (1988).  Estimation of Descriptive Statistics for
Multiply Censored Water Quality Data.  <em>Water Resources Research</em>
<b>24</b>(12), 1997–2004.
</p>
<p>Hirsch, R.M., and J.R. Stedinger. (1987).  Plotting Positions for Historical
Floods and Their Precision.  <em>Water Resources Research</em> <b>23</b>(4), 715–727.
</p>
<p>Korn, L.R., and D.E. Tyler. (2001).  Robust Estimation for Chemical Concentration
Data Subject to Detection Limits.  In Fernholz, L., S. Morgenthaler, and W. Stahel,
eds.  <em>Statistics in Genetics and in the Environmental Sciences</em>.
Birkhauser Verlag, Basel, pp.41–63.
</p>
<p>Krishnamoorthy K., and T. Mathew. (2009).
<em>Statistical Tolerance Regions: Theory, Applications, and Computation</em>.
John Wiley and Sons, Hoboken.
</p>
<p>Michael, J.R., and W.R. Schucany. (1986).  Analysis of Data from Censored Samples.
In D'Agostino, R.B., and M.A. Stephens, eds. <em>Goodness-of Fit Techniques</em>.
Marcel Dekker, New York, 560pp, Chapter 11, 461–496.
</p>
<p>Millard, S.P., P. Dixon, and N.K. Neerchal. (2014; in preparation).
<em>Environmental Statistics with R</em>.  CRC Press, Boca Raton, Florida.
</p>
<p>Nelson, W. (1982).  <em>Applied Life Data Analysis</em>.
John Wiley and Sons, New York, 634pp.
</p>
<p>Newman, M.C., P.M. Dixon, B.B. Looney, and J.E. Pinder. (1989).  Estimating Mean
and Variance for Environmental Samples with Below Detection Limit Observations.
<em>Water Resources Bulletin</em> <b>25</b>(4), 905–916.
</p>
<p>Pettitt, A. N. (1983).  Re-Weighted Least Squares Estimation with Censored and
Grouped Data: An Application of the EM Algorithm.
<em>Journal of the Royal Statistical Society, Series B</em> <b>47</b>, 253–260.
</p>
<p>Regal, R. (1982).  Applying Order Statistic Censored Normal Confidence Intervals
to Time Censored Data.  Unpublished manuscript, University of Minnesota, Duluth,
Department of Mathematical Sciences.
</p>
<p>Royston, P. (2007).  Profile Likelihood for Estimation and Confdence Intervals.
<em>The Stata Journal</em> <b>7</b>(3), pp. 376–387.
</p>
<p>Saw, J.G. (1961b).  The Bias of the Maximum Likelihood Estimators of Location and
Scale Parameters Given a Type II Censored Normal Sample.
<em>Biometrika</em> <b>48</b>, 448–451.
</p>
<p>Schmee, J., D.Gladstein, and W. Nelson. (1985).  Confidence Limits for Parameters
of a Normal Distribution from Singly Censored Samples, Using Maximum Likelihood.
<em>Technometrics</em> <b>27</b>(2) 119–128.
</p>
<p>Schneider, H. (1986).  <em>Truncated and Censored Samples from Normal Populations</em>.
Marcel Dekker, New York, New York, 273pp.
</p>
<p>Shumway, R.H., A.S. Azari, and P. Johnson. (1989).  Estimating Mean Concentrations
Under Transformations for Environmental Data With Detection Limits.
<em>Technometrics</em> <b>31</b>(3), 347–356.
</p>
<p>Singh, A., R. Maichle, and S. Lee. (2006).  <em>On the Computation of a 95%
Upper Confidence Limit of the Unknown Population Mean Based Upon Data Sets
with Below Detection Limit Observations</em>.  EPA/600/R-06/022, March 2006.
Office of Research and Development, U.S. Environmental Protection Agency,
Washington, D.C.
</p>
<p>Stryhn, H., and J. Christensen. (2003).  <em>Confidence Intervals by the Profile
Likelihood Method, with Applications in Veterinary Epidemiology</em>.  Contributed paper
at ISVEE X (November 2003, Chile).
<a href="https://gilvanguedes.com/wp-content/uploads/2019/05/Profile-Likelihood-CI.pdf">https://gilvanguedes.com/wp-content/uploads/2019/05/Profile-Likelihood-CI.pdf</a>.
</p>
<p>Travis, C.C., and M.L. Land. (1990).  Estimating the Mean of Data Sets with
Nondetectable Values.  <em>Environmental Science and Technology</em> <b>24</b>, 961–962.
</p>
<p>USEPA. (2009).  <em>Statistical Analysis of Groundwater Monitoring Data at RCRA Facilities, Unified Guidance</em>.
EPA 530/R-09-007, March 2009.  Office of Resource Conservation and Recovery Program Implementation and Information Division.
U.S. Environmental Protection Agency, Washington, D.C. Chapter 15.
</p>
<p>USEPA. (2010).  <em>Errata Sheet - March 2009 Unified Guidance</em>.
EPA 530/R-09-007a, August 9, 2010.  Office of Resource Conservation and Recovery, Program Information and Implementation Division.
U.S. Environmental Protection Agency, Washington, D.C.
</p>
<p>Venzon, D.J., and S.H. Moolgavkar. (1988).  A Method for Computing
Profile-Likelihood-Based Confidence Intervals.  <em>Journal of the Royal
Statistical Society, Series C (Applied Statistics)</em> <b>37</b>(1), pp. 87–94.
</p>


<h3>See Also</h3>

<p><code>LognormalAlt</code>, <code>elnormAlt</code>,
<code>elnormCensored</code>, <code>enormCensored</code>,
<code>estimateCensored.object</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">  # Chapter 15 of USEPA (2009) gives several examples of estimating the mean
  # and standard deviation of a lognormal distribution on the log-scale using
  # manganese concentrations (ppb) in groundwater at five background wells.
  # In EnvStats these data are stored in the data frame
  # EPA.09.Ex.15.1.manganese.df.

  # Here we will estimate the mean and coefficient of variation
  # ON THE ORIGINAL SCALE using the MLE, QMVUE,
  # and robust ROS (imputation with Q-Q regression).

  # First look at the data:
  #-----------------------

  EPA.09.Ex.15.1.manganese.df

  #   Sample   Well Manganese.Orig.ppb Manganese.ppb Censored
  #1       1 Well.1                 &lt;5           5.0     TRUE
  #2       2 Well.1               12.1          12.1    FALSE
  #3       3 Well.1               16.9          16.9    FALSE
  #...
  #23      3 Well.5                3.3           3.3    FALSE
  #24      4 Well.5                8.4           8.4    FALSE
  #25      5 Well.5                 &lt;2           2.0     TRUE

  longToWide(EPA.09.Ex.15.1.manganese.df,
    "Manganese.Orig.ppb", "Sample", "Well",
    paste.row.name = TRUE)

  #         Well.1 Well.2 Well.3 Well.4 Well.5
  #Sample.1     &lt;5     &lt;5     &lt;5    6.3   17.9
  #Sample.2   12.1    7.7    5.3   11.9   22.7
  #Sample.3   16.9   53.6   12.6     10    3.3
  #Sample.4   21.6    9.5  106.3     &lt;2    8.4
  #Sample.5     &lt;2   45.9   34.5   77.2     &lt;2


  # Now estimate the mean and coefficient of variation
  # using the MLE:
  #---------------------------------------------------

  with(EPA.09.Ex.15.1.manganese.df,
    elnormAltCensored(Manganese.ppb, Censored))

  #Results of Distribution Parameter Estimation
  #Based on Type I Censored Data
  #--------------------------------------------
  #
  #Assumed Distribution:            Lognormal
  #
  #Censoring Side:                  left
  #
  #Censoring Level(s):              2 5
  #
  #Estimated Parameter(s):          mean = 23.003987
  #                                 cv   =  2.300772
  #
  #Estimation Method:               MLE
  #
  #Data:                            Manganese.ppb
  #
  #Censoring Variable:              Censored
  #
  #Sample Size:                     25
  #
  #Percent Censored:                24%

  # Now compare the MLE with the QMVUE and the
  # estimator based on robust ROS
  #-------------------------------------------

  with(EPA.09.Ex.15.1.manganese.df,
    elnormAltCensored(Manganese.ppb, Censored))$parameters
  #     mean        cv
  #23.003987  2.300772

  with(EPA.09.Ex.15.1.manganese.df,
    elnormAltCensored(Manganese.ppb, Censored,
    method = "qmvue"))$parameters
  #     mean        cv
  #21.566945  1.841366

  with(EPA.09.Ex.15.1.manganese.df,
    elnormAltCensored(Manganese.ppb, Censored,
    method = "rROS"))$parameters
  #     mean        cv
  #19.886180  1.298868

  #----------

  # The method used to estimate quantiles for a Q-Q plot is
  # determined by the argument prob.method.  For the function
  # elnormCensoredAlt, for any estimation method that involves
  # Q-Q regression, the default value of prob.method is
  # "hirsch-stedinger" and the default value for the
  # plotting position constant is plot.pos.con=0.375.

  # Both Helsel (2012) and USEPA (2009) also use the Hirsch-Stedinger
  # probability method but set the plotting position constant to 0.

  with(EPA.09.Ex.15.1.manganese.df,
    elnormAltCensored(Manganese.ppb, Censored,
    method = "rROS", plot.pos.con = 0))$parameters
  #     mean        cv
  #19.827673  1.304725

  #----------

  # Using the same data as above, compute a confidence interval
  # for the mean using the profile-likelihood method.

  with(EPA.09.Ex.15.1.manganese.df,
    elnormAltCensored(Manganese.ppb, Censored, ci = TRUE))

  #Results of Distribution Parameter Estimation
  #Based on Type I Censored Data
  #--------------------------------------------
  #
  #Assumed Distribution:            Lognormal
  #
  #Censoring Side:                  left
  #
  #Censoring Level(s):              2 5
  #
  #Estimated Parameter(s):          mean = 23.003987
  #                                 cv   =  2.300772
  #
  #Estimation Method:               MLE
  #
  #Data:                            Manganese.ppb
  #
  #Censoring Variable:              Censored
  #
  #Sample Size:                     25
  #
  #Percent Censored:                24%
  #
  #Confidence Interval for:         mean
  #
  #Confidence Interval Method:      Profile Likelihood
  #
  #Confidence Interval Type:        two-sided
  #
  #Confidence Level:                95%
  #
  #Confidence Interval:             LCL = 12.37629
  #                                 UCL = 69.87694
</code></pre>


</div>