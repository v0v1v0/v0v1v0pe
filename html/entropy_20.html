<div class="container">

<table style="width: 100%;"><tr>
<td>entropy.plugin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Plug-In Entropy Estimator</h2>

<h3>Description</h3>

<p><code>entropy.plugin</code> computes the Shannon entropy H 
of a discrete random variable with the specified frequencies (probability mass function).
</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy.plugin(freqs, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>freqs</code></td>
<td>
<p>frequencies (probability mass function).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Shannon entropy of a discrete random variable is 
defined as <code class="reqn">H = -\sum_k p(k) \log( p(k) )</code>, where <code class="reqn">p</code> is its probability mass function.
</p>


<h3>Value</h3>

<p><code>entropy.plugin</code> returns the Shannon entropy. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code>entropy</code>, <code>entropy.empirical</code>, <code>entropy.shrink</code>, 
<code>mi.plugin</code>, <code>KL.plugin</code>, <code>discretize</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

# some frequencies
freqs = c(0.2, 0.1, 0.15, 0.05, 0, 0.3, 0.2)  

# and corresponding entropy
entropy.plugin(freqs)
</code></pre>


</div>