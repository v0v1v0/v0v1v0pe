<div class="container">

<table style="width: 100%;"><tr>
<td>SymKL.z</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>SymKL.z</h2>

<h3>Description</h3>

<p>Returns the Z estimator of Symetrized Kullback-Leibler Divergence, which has exponentialy decaying bias.  See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class="language-R">SymKL.z(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>Vector of counts from first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>Vector of counts from second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, DOI 10.1162/NECO_a_00646.
</p>


<h3>Examples</h3>

<pre><code class="language-R"> x = c(1,3,7,4,8) 
 y = c(2,5,1,3,6) 
 SymKL.z(x,y) 
</code></pre>


</div>