<div class="container">

<table style="width: 100%;"><tr>
<td>erboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>ER-Boost Expectile Regression Modeling</h2>

<h3>Description</h3>

<p>Fits ER-Boost Expectile Regression models.</p>


<h3>Usage</h3>

<pre><code class="language-R">erboost(formula = formula(data),
    distribution = list(name="expectile",alpha=0.5),
    data = list(),
    weights,
    var.monotone = NULL,
    n.trees = 3000,
    interaction.depth = 3,
    n.minobsinnode = 10,
    shrinkage = 0.001,
    bag.fraction = 0.5,
    train.fraction = 1.0,
    cv.folds=0,
    keep.data = TRUE,
    verbose = TRUE)

erboost.fit(x,y,
        offset = NULL,
        misc = NULL,
        distribution = list(name="expectile",alpha=0.5),
        w = NULL,
        var.monotone = NULL,
        n.trees = 3000,
        interaction.depth = 3,
        n.minobsinnode = 10,
        shrinkage = 0.001,
        bag.fraction = 0.5,
        train.fraction = 1.0,
        keep.data = TRUE,
        verbose = TRUE,
        var.names = NULL,
        response.name = NULL)

erboost.more(object,
         n.new.trees = 3000,
         data = NULL,
         weights = NULL,
         offset = NULL,
         verbose = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>a symbolic description of the model to be fit. The formula may 
include an offset term (e.g. y~offset(n)+x). If <code>keep.data=FALSE</code> in 
the initial call to <code>erboost</code> then it is the user's responsibility to 
resupply the offset to <code>erboost.more</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution</code></td>
<td>
<p>a list with a component <code>name</code> specifying the distribution 
and any additional parameters needed. Expectile regression is available and <code>distribution</code> must a list of the form 
<code>list(name="expectile",alpha=0.25)</code> where <code>alpha</code> is the expectile 
to estimate. The current version's expectile regression methods do 
not handle non-constant weights and will stop.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>an optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically 
the environment from which <code>erboost</code> is called. If <code>keep.data=TRUE</code> in 
the initial call to <code>erboost</code> then <code>erboost</code> stores a copy with the 
object. If <code>keep.data=FALSE</code> then subsequent calls to 
<code>erboost.more</code> must resupply the same dataset. It becomes the user's 
responsibility to resupply the same data at this point.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting process. 
Must be positive but do not need to be normalized. If <code>keep.data=FALSE</code> 
in the initial call to <code>erboost</code> then it is the user's responsibility to 
resupply the weights to <code>erboost.more</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion. The default number is 3000. <b>Users should not always use the default value, but choose 
the appropriate value of <code>n.trees</code> based on their data.</b> Please see "details" section below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If <code>cv.folds</code>&gt;1 then
<code>erboost</code>, in addition to the usual fit, will perform a cross-validation, calculate
an estimate of generalization error returned in <code>cv.error</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>interaction.depth</code></td>
<td>
<p>The maximum depth of variable interactions. 1 implies
an additive model, 2 implies a model with up to 2-way interactions, etc. 
The default value is 3. <b>Users should not always use the default value, but choose 
the appropriate value of <code>interaction.depth</code> based on their data.</b>
Please see "details" section below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.minobsinnode</code></td>
<td>
<p>minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the expansion.
Also known as the learning rate or step-size reduction.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces randomnesses
into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice
will result in similar but different fits. <code>erboost</code> uses the R random number
generator so <code>set.seed</code> can ensure that the model can be
reconstructed. Preferably, the user can save the returned
<code>erboost.object</code> using <code>save</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>erboost</code> and the remainder are used for
computing out-of-sample estimates of the loss function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index makes
subsequent calls to <code>erboost.more</code> faster at the cost of storing an
extra copy of the dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>a <code>erboost</code> object created from an initial call to
<code>erboost</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.new.trees</code></td>
<td>
<p>the number of additional trees to add to <code>object</code>. The default number is 3000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If TRUE, erboost will print out progress and performance indicators.
If this option is left unspecified for erboost.more then it uses <code>verbose</code> from
<code>object</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x, y</code></td>
<td>
<p>For <code>erboost.fit</code>: <code>x</code> is a data frame or data matrix containing the
predictor variables and <code>y</code> is the vector of outcomes. The number of rows
in <code>x</code> must be the same as the length of <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p>a vector of values for the offset</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>misc</code></td>
<td>
<p>For <code>erboost.fit</code>: <code>misc</code> is an R object that is simply passed on to
the erboost engine.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>For <code>erboost.fit</code>: <code>w</code> is a vector of weights of the same
length as the <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.names</code></td>
<td>
<p>For <code>erboost.fit</code>: A vector of strings of length equal to the
number of columns of <code>x</code> containing the names of the predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>response.name</code></td>
<td>
<p>For <code>erboost.fit</code>: A character string label for the response
variable.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Expectile regression (Newey &amp; Powell 1987) is a nice tool for estimating the conditional expectiles of a response variable given a set of covariates. This package implements a regression tree based gradient boosting estimator for nonparametric multiple expectile regression. The code is a modified version of <code>gbm</code> library (<a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a>) originally written by Greg Ridgeway.
</p>
<p>Boosting is the process of iteratively adding basis functions in a greedy
fashion so that each additional basis function further reduces the selected
loss function. This implementation closely follows Friedman's Gradient
Boosting Machine (Friedman, 2001).
</p>
<p>In addition to many of the features documented in the Gradient Boosting Machine,
<code>erboost</code> offers additional features including the out-of-bag estimator for
the optimal number of iterations, the ability to store and manipulate the
resulting <code>erboost</code> object.
</p>
<p>Concerning tuning parameters, <b><code>interaction.depth</code></b> and <b><code>n.trees</code></b> are two of the most important tuning parameters in <span class="pkg">erboost</span>. <b>Users should not always use the default values of those two parameters, instead they should choose the appropriate values of <code>interaction.depth</code> and <code>n.trees</code> according to their data.</b> For example, if <code>n.trees</code>, which is the maximal number of trees to fit, is set to be too small, then it is possible that the actual optimal number of trees (which is <code>best.iter</code> selected by the function <code>erboost.perf</code> in "example" section) for a particular data  exceeds this number, resulting a sub-optimal model. <b>Therefore, users should always fit the model with a large enough <code>n.trees</code> such that <code>n.trees</code> is greater than the potential optimal number of trees. The same principle also applies on <code>interaction.depth</code></b>.
</p>
<p><code>erboost.fit</code> provides the link between R and the C++ erboost engine. <code>erboost</code>
is a front-end to <code>erboost.fit</code> that uses the familiar R modeling formulas.
However, <code>model.frame</code> is very slow if there are many
predictor variables. For power-users with many variables use <code>erboost.fit</code>.
For general practice <code>erboost</code> is preferable.
</p>


<h3>Value</h3>

<p><code>erboost</code>, <code>erboost.fit</code>, and <code>erboost.more</code> return a
<code>erboost.object</code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yiyang@umn.edu">yiyang@umn.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a>
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), “Nonparametric Multiple Expectile Regression via ER-Boost,” <em>Journal of Statistical Computation and Simulation</em>, 84(1), 84-95.
</p>
<p>G. Ridgeway (1999). “The state of boosting,” <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p><a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a><br></p>
<p>J.H. Friedman (2001). “Greedy Function Approximation: A Gradient Boosting
Machine,” <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). “Stochastic Gradient Boosting,” <em>Computational Statistics
and Data Analysis</em> 38(4):367-378.
</p>


<h3>See Also</h3>

<p><code>erboost.object</code>,
<code>erboost.perf</code>,
<code>plot.erboost</code>,
<code>predict.erboost</code>,
<code>summary.erboost</code>,
</p>


<h3>Examples</h3>

<pre><code class="language-R">
N &lt;- 200
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])
X4 &lt;- factor(sample(letters[1:6],N,replace=TRUE))
X5 &lt;- factor(sample(letters[1:3],N,replace=TRUE))
X6 &lt;- 3*runif(N)
mu &lt;- c(-1,0,1,2)[as.numeric(X3)]

SNR &lt;- 10 # signal-to-noise ratio
Y &lt;- X1**1.5 + 2 * (X2**.5) + mu
sigma &lt;- sqrt(var(Y)/SNR)
Y &lt;- Y + rnorm(N,0,sigma)

# introduce some missing values
X1[sample(1:N,size=50)] &lt;- NA
X4[sample(1:N,size=30)] &lt;- NA

data &lt;- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

# fit initial model
erboost1 &lt;- erboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data,                   # dataset
    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
                                 # +1: monotone increase,
                                 #  0: no monotone restrictions
    distribution=list(name="expectile",alpha=0.5),
                                 # expectile
    n.trees=3000,                # number of trees
    shrinkage=0.005,             # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 5,                # do 5-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=TRUE)                # print out progress


# check performance using a 50% heldout test set
best.iter &lt;- erboost.perf(erboost1,method="test")
print(best.iter)

# check performance using 5-fold cross-validation
best.iter &lt;- erboost.perf(erboost1,method="cv")
print(best.iter)

# plot the performance
# plot variable influence
summary(erboost1,n.trees=1)         # based on the first tree
summary(erboost1,n.trees=best.iter) # based on the estimated best number of trees

# make some new data
N &lt;- 20
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- ordered(sample(letters[1:4],N,replace=TRUE))
X4 &lt;- factor(sample(letters[1:6],N,replace=TRUE))
X5 &lt;- factor(sample(letters[1:3],N,replace=TRUE))
X6 &lt;- 3*runif(N)
mu &lt;- c(-1,0,1,2)[as.numeric(X3)]

Y &lt;- X1**1.5 + 2 * (X2**.5) + mu + rnorm(N,0,sigma)

data2 &lt;- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale
f.predict &lt;- predict.erboost(erboost1,data2,best.iter)

# least squares error
print(sum((data2$Y-f.predict)^2))

# create marginal plots
# plot variable X1 after "best" iterations
plot.erboost(erboost1,1,best.iter)
# contour plot of variables 1 and 3 after "best" iterations
plot.erboost(erboost1,c(1,3),best.iter)

# do another 20 iterations
erboost2 &lt;- erboost.more(erboost1,20,
                 verbose=FALSE) # stop printing detailed progress
</code></pre>


</div>