<div class="container">

<table style="width: 100%;"><tr>
<td>computeIndicators</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computation of EMOA performance indicators.</h2>

<h3>Description</h3>

<p>Given a data.frame of Pareto-front approximations for different
sets of problems, algorithms and replications, the function computes sets
of unary and binary EMOA performance indicators.
This function makes use of <code>parallelMap</code> to
parallelize the computation of indicators.
</p>


<h3>Usage</h3>

<pre><code class="language-R">computeIndicators(
  df,
  obj.cols = c("f1", "f2"),
  unary.inds = NULL,
  binary.inds = NULL,
  normalize = FALSE,
  offset = 0,
  ref.points = NULL,
  ref.sets = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>[<code>data.frame</code>]<br>
Data frame with columns <code>obj.cols</code>, “prob”, “algorithm”
and “repl”.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj.cols</code></td>
<td>
<p>[<code>character(&gt;= 2)</code>]<br>
Column names of the objective functions.
Default is <code>c("f1", "f2")</code>, i.e., the bi-objective case is assumed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unary.inds</code></td>
<td>
<p>[<code>list</code>]<br>
Named list of unary indicators which shall be calculated.
Each component must be another list with mandatory argument <code>fun</code> (the
function which calculates the indicator) and optional argument <code>pars</code> (a named
list of parameters for <code>fun</code>). Function <code>fun</code> must have the
signiture “function(points, arg1, ..., argk, ...)”.
The arguments “points” and “...” are mandatory, the remaining are
optional.
The names of the components on the first level are used for the column names
of the output data.frame.
Default is <code>list(HV = list(fun = computeHV))</code>, i.e., the dominated
Hypervolume indicator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>binary.inds</code></td>
<td>
<p>[<code>list</code>]<br>
Named list of binary indicators which shall be applied for each algorithm
combination. Parameter <code>binary.inds</code> needs the same structure as <code>unary.inds</code>.
However, the function signature of <code>fun</code> is slighly different:
“function(points1, points2, arg1, ..., argk, ...)”.
See function <code>emoaIndEps</code> for an example.
Default is <code>list(EPS = list(fun = emoaIndEps))</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>[<code>logical(1)</code>]<br>
Normalize approximation sets to <code class="reqn">[0, 1]^p</code> where <code class="reqn">p</code> is the number of
objectives? Normalization is done on the union of all approximation sets for each
problem.
Default is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p>[<code>numeric(1)</code>]<br>
Offset added to reference point estimations.
Default is 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ref.points</code></td>
<td>
<p>[<code>list</code>]<br>
Named list of numeric vectors (the reference points). The names must be the
unique problem names in <code>df$prob</code> or a subset of these.
If <code>NULL</code> (the default), reference points are estimated from the
approximation sets for each problem.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ref.sets</code></td>
<td>
<p>[<code>list</code>]<br>
Named list matrizes (the reference sets). The names must be the
unique problem names in <code>df$prob</code> or a subset of these.
If <code>NULL</code> (the default), reference points are estimated from the
approximation sets for each problem.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>[<code>list</code>] List with components “unary” (data frame of
unary indicators), “binary” (list of matrizes of binary indicators),
“ref.points” (list of reference points used) and “ref.sets”
(reference sets used).
</p>


<h3>References</h3>

<p>[1] Knowles, J., Thiele, L., &amp; Zitzler, E. (2006). A Tutorial on the Performance Assessment
of Stochastic Multiobjective Optimizers. Retrieved from https://sop.tik.ee.ethz.ch/KTZ2005a.pdf
[2] Knowles, J., &amp; Corne, D. (2002). On Metrics for Comparing Non-Dominated Sets.
In Proceedings of the 2002 Congress on Evolutionary Computation Conference (CEC02)
(pp. 711–716). Honolulu, HI, USA: Institute of Electrical and Electronics Engineers.
[3] Okabe, T., Yaochu, Y., &amp; Sendhoff, B. (2003). A Critical Survey of Performance
Indices for Multi-Objective Optimisation. In Proceedings of the 2003 Congress on Evolutionary
Computation Conference (CEC03) (pp. 878–885). Canberra, ACT, Australia: IEEE.
</p>


</div>