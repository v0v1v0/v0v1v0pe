<div class="container">

<table style="width: 100%;"><tr>
<td>kpcca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kernel Pairwise Constrained Component Analysis (KPCCA)</h2>

<h3>Description</h3>

<p>Using must-link and cannot-link constaints, KPCCA (Mignon &amp; Jury, 2012) learns a projection into a 
low-dimensional space where the distances between pairs of data points respect the desired constraints, 
exhibiting good generalization properties in presence of high dimensional data. This is a kernelized
version of <code>pcca</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kpcca(K, d1, ML, CL, beta = 1, epsi = 1e-04, etamax = 0.1, disp = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Gram matrix of size n*n</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d1</code></td>
<td>
<p>Number of extracted features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ML</code></td>
<td>
<p>Matrix nbML x 2 of must-link constraints. Each row of ML contains the indices
of objects that belong to the same class.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CL</code></td>
<td>
<p>Matrix nbCL x 2 of cannot-link constraints. Each row of CL contains the indices
of objects that belong to different classes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>Sharpness parameter in the loss function (default: 1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsi</code></td>
<td>
<p>Minimal rate of change of the cost function (default: 1e-4).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>etamax</code></td>
<td>
<p>Maximum step in the line search algorithm (default: 0.1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>disp</code></td>
<td>
<p>If TRUE (default), intermediate results are displayed.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with three attributes:
</p>

<dl>
<dt>z</dt>
<dd>
<p>The n*d1 matrix of extracted features.</p>
</dd>
<dt>A</dt>
<dd>
<p>The projection matrix of size d1*n.</p>
</dd>
<dt>D</dt>
<dd>
<p>The Euclidean distance matrix in the projected space.</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Thierry Denoeux.
</p>


<h3>References</h3>

<p>A. Mignon and F. Jurie. PCCA: a new approach for distance learning from sparse 
pairwise constraints. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, 
pages 2666-2672, 2012.
</p>


<h3>See Also</h3>

<p><code>pcca</code>, <code>create_MLCL</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(kernlab)
data&lt;-bananas(400)
plot(data$x,pch=data$y,col=data$y)
const&lt;-create_MLCL(data$y,1000)
rbf &lt;- rbfdot(sigma = 0.2)
K&lt;-kernelMatrix(rbf,data$x)
res.kpcca&lt;-kpcca(K,d1=1,ML=const$ML,CL=const$CL,beta=1)
plot(res.kpcca$z,col=data$y)

## End(Not run)

</code></pre>


</div>