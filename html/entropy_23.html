<div class="container">

<table style="width: 100%;"><tr>
<td>entropy.Dirichlet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Dirichlet Prior Bayesian Estimators of Entropy, Mutual Information 
and Other Related Quantities</h2>

<h3>Description</h3>

<p><code>freqs.Dirichlet</code> computes the Bayesian estimates
of the bin frequencies using the  Dirichlet-multinomial       
pseudocount model.
</p>
<p><code>entropy.Dirichlet</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> by plug-in of Bayesian estimates
of the bin frequencies using the  Dirichlet-multinomial       
pseudocount model.
</p>
<p><code>KL.Dirichlet</code> computes a Bayesian estimate of the Kullback-Leibler (KL) divergence 
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>chi2.Dirichlet</code> computes a Bayesian version of the chi-squared divergence
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>mi.Dirichlet</code> computes a Bayesian estimate of mutual information of two random variables.
</p>
<p><code>chi2indep.Dirichlet</code> computes a Bayesian version of the chi-squared divergence of 
independence from a table of counts <code>y2d</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">freqs.Dirichlet(y, a)
entropy.Dirichlet(y, a, unit=c("log", "log2", "log10"))
KL.Dirichlet(y1, y2, a1, a2, unit=c("log", "log2", "log10"))
chi2.Dirichlet(y1, y2, a1, a2, unit=c("log", "log2", "log10"))
mi.Dirichlet(y2d, a, unit=c("log", "log2", "log10"))
chi2indep.Dirichlet(y2d, a, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y1</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2d</code></td>
<td>
<p>matrix of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>
<p>pseudocount per bin.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a1</code></td>
<td>
<p>pseudocount per bin for first random variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a2</code></td>
<td>
<p>pseudocount per bin for second random variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Dirichlet-multinomial pseudocount entropy estimator
is a Bayesian plug-in estimator: 
in the definition of the Shannon entropy the
bin probabilities are replaced by the respective Bayesian estimates
of the frequencies, using a model with a Dirichlet prior and a multinomial likelihood.
</p>
<p>The parameter <code>a</code> is a parameter of the Dirichlet prior, and in effect
specifies the pseudocount per bin.  Popular choices of <code>a</code> are:
</p>

<ul>
<li>
<p>a=0:maximum likelihood estimator (see <code>entropy.empirical</code>)  
</p>
</li>
<li>
<p>a=1/2:Jeffreys' prior; Krichevsky-Trovimov (1991) entropy estimator
</p>
</li>
<li>
<p>a=1:Laplace's prior
</p>
</li>
<li>
<p>a=1/length(y):Schurmann-Grassberger (1996) entropy estimator
</p>
</li>
<li>
<p>a=sqrt(sum(y))/length(y):minimax prior
</p>
</li>
</ul>
<p>The pseudocount <code>a</code> can also be a vector so that for each bin an 
individual pseudocount is added.
</p>


<h3>Value</h3>

<p><code>freqs.Dirichlet</code> returns the Bayesian estimates of the frequencies . 
</p>
<p><code>entropy.Dirichlet</code> returns the Bayesian estimate of the Shannon entropy. 
</p>
<p><code>KL.Dirichlet</code> returns the Bayesian estimate of the KL divergence. 
</p>
<p><code>chi2.Dirichlet</code> returns the Bayesian version of the chi-squared divergence. 
</p>
<p><code>mi.Dirichlet</code> returns the Bayesian estimate of the mutual information. 
</p>
<p><code>chi2indep.Dirichlet</code> returns the Bayesian version of the chi-squared divergence of independence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Agresti, A., and D. B. Hitchcock. 2005. Bayesian inference for categorical
data analysis. Stat. Methods. Appl. <b>14</b>:297â€“330.
</p>
<p>Krichevsky, R. E., and V. K. Trofimov. 1981. The performance of universal encoding.
IEEE Trans. Inf. Theory <b>27</b>: 199-207.  
</p>
<p>Schurmann, T., and P. Grassberger. 1996. Entropy estimation of symbol sequences.
Chaos <b>6</b>:41-427.
</p>


<h3>See Also</h3>

<p><code>entropy</code>, 
<code>entropy.shrink</code>,
<code>entropy.empirical</code>, 
<code>entropy.plugin</code>,
<code>mi.plugin</code>, <code>KL.plugin</code>, <code>discretize</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")


# a single variable

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# Dirichlet estimate of frequencies with a=1/2
freqs.Dirichlet(y, a=1/2)

# Dirichlet estimate of entropy with a=0
entropy.Dirichlet(y, a=0)

# identical to empirical estimate
entropy.empirical(y)

# Dirichlet estimate with a=1/2 (Jeffreys' prior)
entropy.Dirichlet(y, a=1/2)

# Dirichlet estimate with a=1 (Laplace prior)
entropy.Dirichlet(y, a=1)

# Dirichlet estimate with a=1/length(y)
entropy.Dirichlet(y, a=1/length(y))

# Dirichlet estimate with a=sqrt(sum(y))/length(y)
entropy.Dirichlet(y, a=sqrt(sum(y))/length(y))


# example with two variables

# observed counts for two random variables
y1 = c(4, 2, 3, 1, 10, 4)
y2 = c(2, 3, 7, 1, 4, 3)

# Bayesian estimate of Kullback-Leibler divergence (a=1/6)
KL.Dirichlet(y1, y2, a1=1/6, a2=1/6)

# half of the corresponding chi-squared divergence
0.5*chi2.Dirichlet(y1, y2, a1=1/6, a2=1/6)


## joint distribution example

# contingency table with counts for two discrete variables
y2d = rbind( c(1,2,3), c(6,5,4) )

# Bayesian estimate of mutual information (a=1/6)
mi.Dirichlet(y2d, a=1/6)

# half of the Bayesian chi-squared divergence of independence
0.5*chi2indep.Dirichlet(y2d, a=1/6)


</code></pre>


</div>