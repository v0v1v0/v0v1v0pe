<div class="container">

<table style="width: 100%;"><tr>
<td>ecospat.poolingEvaluation</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Evaluation of species distribution models using the pooling procedure
</h2>

<h3>Description</h3>

<p>This function evaluates species distribution models using 100% of the dataset by pooling the different runs of the cross validation as in Collart et al. 2021
</p>


<h3>Usage</h3>

<pre><code class="language-R">  ecospat.poolingEvaluation(fit,
                            calib,
                            resp,
                            AlgoName = NULL,
                            metrics = c("SomersD","AUC","MaxTSS","MaxKappa","Boyce"),
                            ensembleEvaluation=FALSE,
                            w=NULL,
                            metricToEnsemble = "MaxTSS")

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>

<p>a list containing <em>n</em> data.frame or matrix, where <em>n</em> corresponds to the number of algorithm you want to evaluate. The data.frames (matrices) need to contain the model predictions (ranging between 0 and 1) resulting from the different runs of cross-validation. These data.frame need to have the same number of rows as in the full dataset (100% of the occurrences and 100% of the absences or background points) and a number of column equal to the number of cross-validation runs
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>calib</code></td>
<td>

<p>a logical matrix with a number of rows equal to the full dataset and a number of column corresponding to the number of cross-validation runs. The value TRUE is to mention the elements that where used to calibrate the models whereas FALSE corresponds to the one that will be used for the evaluation (<em>NB</em> the points used to calibrate the models during a cross-validation run should be the same across algoritms)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>resp</code></td>
<td>

<p>a numeric vector where 1 corresponds to a species response and 0 to an absence (or background point) with a length corresponding to number of rows in calib
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AlgoName</code></td>
<td>

<p>a character vector for giving a name to each elements of fit. If NULL, the position in the list will be used instead.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p>a vector of evaluation metrics chosen among "SomersD", "AUC", "MaxTSS", "MaxKappa", "Boyce"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ensembleEvaluation</code></td>
<td>

<p>logical. If TRUE, the ensemble model will be evaluated applying a weighted mean across algorithms.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>

<p>a numeric vector of the weights used to realize the ensemble model. The length should match the number of algorithms.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metricToEnsemble</code></td>
<td>

<p>character. Metric to use to ensemble the models with a weighted mean when w is not given. The metric should be one in metrics</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Because a minimum sample size is needed to evaluate models (see Collart &amp; Guisan,2023; Jiménez-Valverde, 2020), this function uses the approach from Collart et al.(2021), which consists to pool the suitability values of the hold-out data (evaluation dataset) across replicates. As the same data point (presence or absence or background point) is presumably sampled in several replicates, the suitability values for each data point is consequently averaged across replicates where they were sampled. This procedure generates a series of independent suitability values with a size approximately equal (as some data points may not have been sampled by chance in any of the <em>n</em> replicates) to that of the number of data point.
</p>


<h3>Value</h3>

<p>a list containing:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>evaluations</code></td>
<td>

<p>a matrix with the evaluation scores based on the different modelling algorithms and based on the consensus across the modelling algorithms (called here "ensemble")
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>

<p>a matrix of predicted values resulting from the pooling procedure and used to compute the evaluation scores. The column <em>resp</em> is where the species occurs or not
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Flavien Collart <a href="mailto:flavien.collart@unil.ch">flavien.collart@unil.ch</a>
</p>
<p>with contributions of Olivier Broennimann <a href="mailto:olivier.broennimann@unil.ch">olivier.broennimann@unil.ch</a>
</p>


<h3>References</h3>

<p>Collart, F., &amp; Guisan, A. (2023). Small to train, small to test: Dealing with low sample size in model evaluation. <em>Ecological Informatics</em>. <b>75</b>, 102106. <a href="https://doi.org/10.1016/j.ecoinf.2023.102106">doi:10.1016/j.ecoinf.2023.102106</a>
</p>
<p>Collart, F., Hedenäs, L., Broennimann, O., Guisan, A. and Vanderpoorten, A. 2021. Intraspecific differentiation: Implications for niche and distribution modelling. <em>Journal of Biogeography</em>. <b>48</b>, 415-426. <a href="https://doi.org/10.1111/jbi.14009">doi:10.1111/jbi.14009</a>
</p>
<p>Jiménez-Valverde, A. 2020. Sample size for the evaluation of presence-absence models. <em>Ecological Indicators</em>. <b>114</b>, 106289. <a href="https://doi.org/10.1016/j.ecolind.2020.106289">doi:10.1016/j.ecolind.2020.106289</a>
</p>


<h3>See Also</h3>

<p><code>ecospat.ESM.EnsembleEvaluation</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">  set.seed(42)
  resp &lt;- c(rep(1,15),rep(0,85)) #15 presences and 85 absences
  #Generating a fake fit object whith two algorithms and 3 cross-vaidation
  fit &lt;- list(matrix(0,nc=3,nr=100),
              matrix(0,nc=3,nr=100))
  fit[[1]][1:15,] = sample(seq(0,1, by=0.01),15*3,prob=c(rep(1,51),rep(10,50)),replace=TRUE)
  fit[[2]][1:15,] = sample(seq(0,1, by=0.01),15*3,prob=c(rep(1,51),rep(10,50)),replace=TRUE)
  fit[[1]][16:100,] = sample(seq(0,1, by=0.01),85*3,prob=c(rep(10,51),rep(1,50)),replace=TRUE)
  fit[[2]][16:100,] = sample(seq(0,1, by=0.01),85*3,prob=c(rep(10,51),rep(1,50)),replace=TRUE)
  
  # Generating a calib object where 80% of the dataset is used to calibrate the model 
  # and 20% to evaluate it
  calib &lt;- matrix(TRUE,nc=3,nr=100)
  calib[c(sample(1:15,3),sample(16:100,17)),1]=FALSE 
  calib[c(sample(1:15,3),sample(16:100,17)),2]=FALSE 
  calib[c(sample(1:15,3),sample(16:100,17)),3]=FALSE 
  
  # Evaluation via the pooling procedure
  eval &lt;- ecospat.poolingEvaluation(fit=fit,calib=calib,resp=resp,metrics=c("AUC","MaxTSS"))
  eval$evaluations
  
  # Evaluation including the ensemble model based on a weighted mean using MaxTSS
  evalEns &lt;- ecospat.poolingEvaluation(fit=fit,calib=calib,resp=resp,ensembleEvaluation=TRUE,
                                       metrics=c("AUC","MaxTSS"))
  evalEns$evaluations
  
  # Evaluation including the ensemble model based on a mean by giving the same weight for 
  # each algorithm
  evalEns &lt;- ecospat.poolingEvaluation(fit=fit,calib=calib,resp=resp,ensembleEvaluation=TRUE,
                                       metrics=c("AUC","MaxTSS"),w=c(1,1))
  evalEns$evaluations
</code></pre>


</div>