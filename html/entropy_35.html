<div class="container">

<table style="width: 100%;"><tr>
<td>mi.plugin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Plug-In Estimator of Mutual Information and of the Chi-Squared Statistic of Independence</h2>

<h3>Description</h3>

<p><code>mi.plugin</code> computes the mutual information 
of two discrete random variables from the specified joint probability mass function.
</p>
<p><code>chi2indep.plugin</code> computes the chi-squared divergence of independence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mi.plugin(freqs2d, unit=c("log", "log2", "log10"))
chi2indep.plugin(freqs2d, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>freqs2d</code></td>
<td>
<p>matrix of joint bin frequencies (joint probability mass function).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The mutual information of two random variables <code class="reqn">X</code> and <code class="reqn">Y</code>
is the Kullback-Leibler divergence between the joint density/probability
mass function and the product independence density of the marginals.
</p>
<p>It can also defined using entropy as <code class="reqn">MI = H(X) + H(Y) - H(X, Y)</code>. 
</p>
<p>Similarly, the chi-squared divergence of independence is the chi-squared divergence
between the joint density and the product density. It is a second-order 
approximation of twice the mutual information.
</p>


<h3>Value</h3>

<p><code>mi.plugin</code> returns the mutual information.
</p>
<p><code>chi2indep.plugin</code> returns the chi-squared divergence of independence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code>mi.Dirichlet</code>, <code>mi.shrink</code>, <code>mi.empirical</code>, <code>KL.plugin</code>, <code>discretize2d</code>. </p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

# joint distribution of two discrete variables
freqs2d = rbind( c(0.2, 0.1, 0.15), c(0.1, 0.2, 0.25) )  

# corresponding mutual information
mi.plugin(freqs2d)

# MI computed via entropy
H1 = entropy.plugin(rowSums(freqs2d))
H2 = entropy.plugin(colSums(freqs2d))
H12 = entropy.plugin(freqs2d)
H1+H2-H12

# and corresponding (half) chi-squared divergence of independence
0.5*chi2indep.plugin(freqs2d)

</code></pre>


</div>