<div class="container">

<table style="width: 100%;"><tr>
<td>rus</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Implementation of RUSBoost</h2>

<h3>Description</h3>

<p>The function implements RUSBoost for binary classification. It returns a list of weak learners that are built on random under-sampled training-sets, and a vector of error estimations of each weak learner. The weak learners altogether consist the ensemble model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rus(formula, data, size, alg, ir = 1, rf.ntree = 50, svm.ker = "radial")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula specify predictors and target variable. Target variable should be a factor of 0 and 1. Predictors can be either numerical and categorical.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data frame used for training the model, i.e. training set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>Ensemble size, i.e. number of weak learners in the ensemble model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alg</code></td>
<td>
<p>The learning algorithm used to train weak learners in the ensemble model. <em>cart</em>, <em>c50</em>, <em>rf</em>, <em>nb</em>, and <em>svm</em> are available. Please see Details for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ir</code></td>
<td>
<p>Imbalance ratio. Specifying how many times the under-sampled majority instances are over minority instances. Interger is not required and so such as ir = 1.5 is allowed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rf.ntree</code></td>
<td>
<p>Number of decision trees in each forest of the ensemble model when using <em>rf</em> (Random Forest) as base learner. Integer is required.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>svm.ker</code></td>
<td>
<p>Specifying kernel function when using svm as base algorithm. Four options are available: <b>linear</b>, <b>polynomial</b>, <b>radial</b>, and <b>sigmoid</b>. Default is radial. Equivalent to that in e1071::svm().</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Based on AdaBoost.M2, RUSBoost uses random under-sampling to reduce majority instances in each iteration of training weak learners. A 1:1 under-sampling ratio (i.e. equal numbers of majority and minority instances) is set as default.
</p>
<p>The function requires the target varible to be a factor of 0 and 1, where 1 indicates minority while 0 indicates majority instances. Only binary classification is implemented in this version.
</p>
<p>Argument <em>alg</em> specifies the learning algorithm used to train weak learners within the ensemble model. Totally five algorithms are implemented: <b>cart</b> (Classification and Regression Tree), <b>c50</b> (C5.0 Decision Tree), <b>rf</b> (Random Forest), <b>nb</b> (Naive Bayes), and <b>svm</b> (Support Vector Machine). When using Random Forest as base learner, the ensemble model is consisted of forests and each forest contains a number of trees.
</p>
<p><em>ir</em> refers to the intended imbalance ratio of training sets for manipulation. With ir = 1 (default), the numbers of majority and minority instances are equal after class rebalancing. With ir = 2, the number of majority instances is twice of that of minority instances. Interger is not required and so such as ir = 1.5 is allowed.
</p>
<p>The object class of returned list is defined as <em>modelBst</em>, which can be directly passed to predict() for predicting test instances.
</p>


<h3>Value</h3>

<p>The function returns a list containing two elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>weakLearners</code></td>
<td>
<p>A list of weak learners.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>errorEstimation</code></td>
<td>
<p>Error estimation of each weak learner. Calculated by using (pseudo_loss + smooth) / (1 - pseudo_loss + smooth). <em>smooth</em> helps prevent error rate = 0 resulted from perfect classfication during trainging iterations. For more information, please see Schapire et al. (1999) Section 4.2.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Seiffert,  C.,  Khoshgoftaar, T.,  Hulse,  J.,  and  Napolitano, A.  2010.  RUSBoost: A Hybrid Approach to Alleviating Class Imbalance. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans. 40(1), pp. 185-197.
</p>
<p>Galar, M., Fernandez, A., Barrenechea, E., Bustince,  H., and Herrera, F. 2012. A Review  on  Ensembles  for  the  Class  Imbalance  Problem:  Bagging-,  Boosting-,  and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 42(4), pp. 463-484.
</p>
<p>Freund, Y. and Schapire, R. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences. 55, pp. 119-139.
</p>
<p>Freund, Y.  and  Schapire,  R.  1996.  Experiments  with  a  new  boosting  algorithm. Machine Learning: In Proceedings of the 13th International Conference. pp. 148-156
</p>
<p>Schapire, R. and Singer, Y. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning. 37(3). pp. 297-336.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("iris")
iris &lt;- iris[1:70, ]
iris$Species &lt;- factor(iris$Species, levels = c("setosa", "versicolor"), labels = c("0", "1"))
model1 &lt;- rus(Species ~ ., data = iris, size = 10, alg = "c50", ir = 1)
model2 &lt;- rus(Species ~ ., data = iris, size = 20, alg = "rf", ir = 1, rf.ntree = 100)
model3 &lt;- rus(Species ~ ., data = iris, size = 40, alg = "svm", ir = 1, svm.ker = "sigmoid")
</code></pre>


</div>