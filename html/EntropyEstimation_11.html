<div class="container">

<table style="width: 100%;"><tr>
<td>KL.z</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>KL.z</h2>

<h3>Description</h3>

<p>Returns the Z estimator of Kullback-Leibler Divergence, which has exponentially decaying bias.  See Zhang and Grabchak (2014b) for details.</p>


<h3>Usage</h3>

<pre><code class="language-R">KL.z(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>Vector of counts from the first distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>Vector of counts from the second distribution. Must be integer valued. Each entry represents the number of observations of a distinct letter.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Lijuan Cao and Michael Grabchak</p>


<h3>References</h3>

<p>Z. Zhang and M. Grabchak (2014b). Nonparametric Estimation of Kullback-Leibler Divergence. Neural Computation, 26(11): 2570-2593.
</p>


<h3>Examples</h3>

<pre><code class="language-R"> x = c(1,3,7,4,8) 
 y = c(2,5,1,3,6) 
 KL.z(x,y)  
 KL.z(y,x)  
</code></pre>


</div>