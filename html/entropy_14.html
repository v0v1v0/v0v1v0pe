<div class="container">

<table style="width: 100%;"><tr>
<td>entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Estimating Entropy From Observed Counts</h2>

<h3>Description</h3>

<p><code>entropy</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code>.
</p>
<p><code>freqs</code> estimates bin frequencies from the counts <code>y</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy(y, lambda.freqs, method=c("ML", "MM", "Jeffreys", "Laplace", "SG",
    "minimax", "CS", "NSB", "shrink"), unit=c("log", "log2", "log10"), verbose=TRUE, ...)
freqs(y, lambda.freqs, method=c("ML", "MM", "Jeffreys", "Laplace", "SG",
    "minimax", "CS", "NSB", "shrink"), verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the method employed to estimate entropy (see Details).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.freqs</code></td>
<td>
<p>shrinkage intensity (for "shrink" option). </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>verbose option (for "shrink" option).  </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>option passed on to <code>entropy.NSB</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>entropy</code> function allows to estimate entropy from observed counts by a variety
of methods:
</p>

<ul>
<li>
<p><code>method="ML"</code>:maximum likelihood, see <code>entropy.empirical</code> 
</p>
</li>
<li>
<p><code>method="MM"</code>:bias-corrected maximum likelihood, see <code>entropy.MillerMadow</code> 
</p>
</li>
<li>
<p><code>method="Jeffreys"</code>:<code>entropy.Dirichlet</code> with <code>a=1/2</code> 
</p>
</li>
<li>
<p><code>method="Laplace"</code>:<code>entropy.Dirichlet</code> with <code>a=1</code> 
</p>
</li>
<li>
<p><code>method="SG"</code>:<code>entropy.Dirichlet</code> with <code>a=a=1/length(y)</code> 
</p>
</li>
<li>
<p><code>method="minimax"</code>:<code>entropy.Dirichlet</code> with <code>a=sqrt(sum(y))/length(y</code> 
</p>
</li>
<li>
<p><code>method="CS"</code>:see <code>entropy.ChaoShen</code> 
</p>
</li>
<li>
<p><code>method="NSB"</code>:see <code>entropy.NSB</code> 
</p>
</li>
<li>
<p><code>method="shrink"</code>:see <code>entropy.shrink</code> 
</p>
</li>
</ul>
<p>The <code>freqs</code> function estimates the underlying bin frequencies.  Note that
estimated frequencies are not
available for <code>method="MM"</code>, <code>method="CS"</code> and <code>method="NSB"</code>. In these
instances a vector containing NAs is returned.
</p>


<h3>Value</h3>

<p><code>entropy</code> returns an estimate of the Shannon entropy. 
</p>
<p><code>freqs</code> returns a vector with estimated bin frequencies (if available).
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code>entropy-package</code>, <code>discretize</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

entropy(y, method="ML")
entropy(y, method="MM")
entropy(y, method="Jeffreys")
entropy(y, method="Laplace")
entropy(y, method="SG")
entropy(y, method="minimax")
entropy(y, method="CS")
#entropy(y, method="NSB")
entropy(y, method="shrink")
</code></pre>


</div>