<div class="container">

<table style="width: 100%;"><tr>
<td>KL.plugin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Plug-In Estimator of the Kullback-Leibler divergence and of the Chi-Squared Divergence</h2>

<h3>Description</h3>

<p><code>KL.plugin</code> computes the Kullback-Leiber (KL) divergence between two discrete random variables <code class="reqn">x_1</code> and <code class="reqn">x_2</code>.  The corresponding probability mass functions are given by <code>freqs1</code> and <code>freqs2</code>. Note that the expectation is taken with regard to <code class="reqn">x_1</code> using <code>freqs1</code>.
</p>
<p><code>chi2.plugin</code> computes the chi-squared divergence between two discrete random variables <code class="reqn">x_1</code> and <code class="reqn">x_2</code> with <code>freqs1</code> and <code>freqs2</code> as corresponding probability mass functions.  Note that the denominator contains <code>freqs2</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KL.plugin(freqs1, freqs2, unit=c("log", "log2", "log10"))
chi2.plugin(freqs1, freqs2, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>freqs1</code></td>
<td>
<p>frequencies (probability mass function) for variable <code class="reqn">x_1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>freqs2</code></td>
<td>
<p>frequencies (probability mass function) for variable <code class="reqn">x_2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Kullback-Leibler divergence between the two discrete variables <code class="reqn">x_1</code>
to  <code class="reqn">x_2</code> is <code class="reqn"> \sum_k p_1(k) \log (p_1(k)/p_2(k)) </code>  where <code class="reqn">p_1</code> and <code class="reqn">p_2</code> are the probability mass functions of <code class="reqn">x_1</code> and <code class="reqn">x_2</code>, respectively, and <code class="reqn">k</code> is 
the index for the classes.
</p>
<p>The chi-squared divergence is given by <code class="reqn"> \sum_k (p_1(k)-p_2(k))^2/p_2(k) </code>.
</p>
<p>Note that both the KL divergence and the chi-squared divergence are not symmetric
in  <code class="reqn">x_1</code> and <code class="reqn">x_2</code>.    The chi-squared divergence can be derived as a 
quadratic approximation of twice the KL divergence.
</p>


<h3>Value</h3>

<p><code>KL.plugin</code> returns the KL divergence.
</p>
<p><code>chi2.plugin</code> returns the chi-squared divergence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code>KL.Dirichlet</code>, <code>KL.shrink</code>, <code>KL.empirical</code>, <code>mi.plugin</code>, <code>discretize2d</code>. </p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

# probabilities for two random variables
freqs1 = c(1/5, 1/5, 3/5)
freqs2 = c(1/10, 4/10, 1/2) 

# KL divergence between x1 to x2
KL.plugin(freqs1, freqs2)

# and corresponding (half) chi-squared divergence
0.5*chi2.plugin(freqs1, freqs2)

## relationship to Pearson chi-squared statistic

# Pearson chi-squared statistic and p-value
n = 30 # sample size (observed counts)
chisq.test(n*freqs1, p = freqs2) # built-in function

# Pearson chi-squared statistic from Pearson divergence
pcs.stat = n*chi2.plugin(freqs1, freqs2) # note factor n
pcs.stat

# and p-value
df = length(freqs1)-1 # degrees of freedom
pcs.pval = 1-pchisq(pcs.stat, df)
pcs.pval
</code></pre>


</div>