<div class="container">

<table style="width: 100%;"><tr>
<td>entropy.empirical</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Empirical Estimators of Entropy and Mutual Information and Related Quantities</h2>

<h3>Description</h3>

<p><code>freqs.empirical</code> computes the empirical frequencies from counts <code>y</code>.
</p>
<p><code>entropy.empirical</code> estimates the Shannon entropy H 
of the random variable Y from the corresponding observed counts <code>y</code>
by plug-in of the empirical frequencies.
</p>
<p><code>KL.empirical</code> computes the empirical Kullback-Leibler (KL) divergence 
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>chi2.empirical</code> computes the empirical chi-squared divergence
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>mi.empirical</code> computes the empirical mutual information from a table of counts <code>y2d</code>.
</p>
<p><code>chi2indep.empirical</code> computes the empirical chi-squared divergence of independence
from a table of counts <code>y2d</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">freqs.empirical(y)
entropy.empirical(y, unit=c("log", "log2", "log10"))
KL.empirical(y1, y2, unit=c("log", "log2", "log10"))
chi2.empirical(y1, y2, unit=c("log", "log2", "log10"))
mi.empirical(y2d, unit=c("log", "log2", "log10"))
chi2indep.empirical(y2d, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y1</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2d</code></td>
<td>
<p>matrix of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The empirical entropy estimator is a plug-in estimator: 
in the definition of the Shannon entropy the
bin probabilities are replaced by the respective empirical frequencies.
</p>
<p>The empirical entropy estimator is the maximum likelihood estimator.
If there are many zero counts and the sample size is small
it is very inefficient and also strongly biased.
</p>


<h3>Value</h3>

<p><code>freqs.empirical</code> returns the empirical frequencies.
</p>
<p><code>entropy.empirical</code> returns an estimate of the Shannon entropy. 
</p>
<p><code>KL.empirical</code> returns an estimate of the KL divergence. 
</p>
<p><code>chi2.empirical</code> returns the empirical chi-squared divergence. 
</p>
<p><code>mi.empirical</code> returns an estimate of the mutual information. 
</p>
<p><code>chi2indep.empirical</code> returns the empirical chi-squared divergence of independence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code>entropy</code>, <code>entropy.plugin</code>, <code>KL.plugin</code>,
<code>chi2.plugin</code>, <code>mi.plugin</code>, <code>chi2indep.plugin</code>,
<code>Gstat</code>, <code>Gstatindep</code>, <code>chi2stat</code>, 
<code>chi2statindep</code>, <code>discretize</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")


## a single variable: entropy

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# empirical frequencies
freqs.empirical(y)

# empirical estimate of entropy
entropy.empirical(y)


## examples with two variables: KL and chi-squared divergence

# observed counts for first random variables (observed)
y1 = c(4, 2, 3, 1, 6, 4)
n = sum(y1) # 20

# counts for the second random variable (expected)
freqs.expected = c(0.10, 0.15, 0.35, 0.05, 0.20, 0.15)
y2 = n*freqs.expected

# empirical Kullback-Leibler divergence
KL.div = KL.empirical(y1, y2)
KL.div

# empirical chi-squared divergence
cs.div = chi2.empirical(y1, y2)
cs.div 
0.5*cs.div  # approximates KL.div

## note: see also Gstat and chi2stat


## joint distribution of two discrete random variables

# contingency table with counts for two discrete variables
y.mat = matrix(c(4, 5, 1, 2, 4, 4), ncol = 2)  # 3x2 example matrix of counts
n.mat = sum(y.mat) # 20

# empirical estimate of mutual information
mi = mi.empirical(y.mat)
mi

# empirical chi-squared divergence of independence
cs.indep = chi2indep.empirical(y.mat)
cs.indep
0.5*cs.indep # approximates mi

## note: see also Gstatindep and chi2statindep

</code></pre>


</div>