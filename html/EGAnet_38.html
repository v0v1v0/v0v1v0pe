<div class="container">

<table style="width: 100%;"><tr>
<td>information</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Information Theory Metrics</h2>

<h3>Description</h3>

<p>A general function to compute several different information theory metrics
</p>


<h3>Usage</h3>

<pre><code class="language-R">information(
  data,
  base = 2.718282,
  bins = floor(sqrt(nrow(data)/5)),
  statistic = c("entropy", "joint.entropy", "conditional.entropy", "total.correlation",
    "dual.total.correlation", "o.information")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Matrix or data frame.
Should consist only of variables to be used in the analysis</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>Numeric (length = 1).
Base of logarithm to use for entropy.
Common options include:
</p>

<ul>
<li> <p><code>2</code> — bits
</p>
</li>
<li> <p><code>2.718282</code> — nats
</p>
</li>
<li> <p><code>10</code> — bans
</p>
</li>
</ul>
<p>Defaults to <code>exp(1)</code> or <code>2.718282</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bins</code></td>
<td>
<p>Numeric (length = 1).
Number of bins if data are not discrete.
Defaults to <code>floor(sqrt(nrow(data) / 5))</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>statistic</code></td>
<td>
<p>Character.
Information theory statistics to compute.
Available options:
</p>

<ul>
<li> <p><code>"entropy"</code> — Shannon's entropy (Shannon, 1948) for each variable in <code>data</code>.
Values range from <code>0</code> to <code>log(k)</code> where <code>k</code> is the number of categories for the variable
</p>
</li>
<li> <p><code>"joint.entropy"</code> — shared uncertainty over all variables in <code>data</code>.
Values range from the maximum of the individual entropies to the sum of individual entropies
</p>
</li>
<li> <p><code>"conditional.entropy"</code> — uncertainty remaining after considering all other
variables in <code>data</code>. Values range from <code>0</code> to the individual entropy of the
conditioned variable
</p>
</li>
<li> <p><code>"total.correlation"</code> — generalization of mutual information to more than
two variables (Watanabe, 1960). Quantifies the redundancy of information in <code>data</code>.
Values range from <code>0</code> to the sum of individual entropies minus the maximum of the
individual entropies
</p>
</li>
<li> <p><code>"dual.total.correlation"</code> — "shared randomness" or total uncertainty remaining in
the <code>data</code> (Han, 1978). Values range from <code>0</code> to joint entropy
</p>
</li>
<li> <p><code>"o.information"</code> — quantifies the extent to which the <code>data</code> is represented
by lower-order (<code>&gt; 0</code>; redundancy) or higher-order (<code>&lt; 0</code>; synergy) constraint
(Crutchfield, 1994)
</p>
</li>
</ul>
<p>By default, all statistics are computed</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns list containing <em>only</em> requested <code>statistic</code>
</p>


<h3>Author(s)</h3>

<p>Hudson F. Golino &lt;hfg9s at virginia.edu&gt; and Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>References</h3>

<p><strong>Shannon's entropy</strong> <br>
Shannon, C. E. (1948). A mathematical theory of communication.
<em>The Bell System Technical Journal</em>, <em>27</em>(3), 379-423.
</p>
<p><strong>Formalization of total correlation</strong> <br>
Watanabe, S. (1960).
Information theoretical analysis of multivariate correlation.
<em>IBM Journal of Research and Development</em> <em>4</em>, 66-82.
</p>
<p><strong>Applied implementation of total correlation</strong> <br>
Felix, L. M., Mansur-Alves, M., Teles, M., Jamison, L., &amp; Golino, H. (2021).
Longitudinal impact and effects of booster sessions in a cognitive training program for healthy older adults.
<em>Archives of Gerontology and Geriatrics</em>, <em>94</em>, 104337.
</p>
<p><strong>Formalization of dual total correlation</strong> <br>
Te Sun, H. (1978).
Nonnegative entropy measures of multivariate symmetric correlations.
<em>Information and Control</em>, <em>36</em>, 133-156.
</p>
<p><strong>Formalization of O-information</strong> <br>
Crutchfield, J. P. (1994). The calculi of emergence: Computation, dynamics and induction.
<em>Physica D: Nonlinear Phenomena</em>, <em>75</em>(1-3), 11-54.
</p>
<p><strong>Applied implementation of O-information</strong> <br>
Marinazzo, D., Van Roozendaal, J., Rosas, F. E., Stella, M., Comolatti, R., Colenbier, N., Stramaglia, S., &amp; Rosseel, Y. (2024).
An information-theoretic approach to build hypergraphs in psychometrics.
<em>Behavior Research Methods</em>, 1-23.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># All measures
information(wmt2[,7:24])

# One measures
information(wmt2[,7:24], statistic = "joint.entropy")

</code></pre>


</div>