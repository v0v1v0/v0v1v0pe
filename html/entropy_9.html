<div class="container">

<table style="width: 100%;"><tr>
<td>entropy.shrink</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Shrinkage Estimators of Entropy, Mutual Information and Related Quantities</h2>

<h3>Description</h3>

<p><code>freq.shrink</code> estimates the bin frequencies from the counts <code>y</code>
using a James-Stein-type shrinkage estimator, where the shrinkage target is the uniform distribution.
</p>
<p><code>entropy.shrink</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> by plug-in of shrinkage estimate
of the bin frequencies.
</p>
<p><code>KL.shrink</code> computes a shrinkage estimate of the Kullback-Leibler (KL) divergence 
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>chi2.shrink</code> computes a shrinkage version of the chi-squared divergence
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>mi.shrink</code> estimates a shrinkage estimate of mutual information of two random variables.
</p>
<p><code>chi2indep.shrink</code> computes a shrinkage version of the chi-squared divergence of independence
from a table of counts <code>y2d</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">freqs.shrink(y, lambda.freqs, verbose=TRUE)
entropy.shrink(y, lambda.freqs, unit=c("log", "log2", "log10"), verbose=TRUE)
KL.shrink(y1, y2, lambda.freqs1, lambda.freqs2, unit=c("log", "log2", "log10"),
            verbose=TRUE)
chi2.shrink(y1, y2, lambda.freqs1, lambda.freqs2, unit=c("log", "log2", "log10"),
            verbose=TRUE)
mi.shrink(y2d, lambda.freqs, unit=c("log", "log2", "log10"), verbose=TRUE)
chi2indep.shrink(y2d, lambda.freqs, unit=c("log", "log2", "log10"), verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y1</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2</code></td>
<td>
<p>vector of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2d</code></td>
<td>
<p>matrix of counts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is "nats" (natural units). For 
computing entropy in "bits" set <code>unit="log2"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.freqs</code></td>
<td>
<p>shrinkage intensity.  If not specified (default) it is estimated in a James-Stein-type fashion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.freqs1</code></td>
<td>
<p>shrinkage intensity for first random variable.  If not specified (default) it is estimated in a James-Stein-type fashion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.freqs2</code></td>
<td>
<p>shrinkage intensity for second random variable.  If not specified (default) it is estimated in a James-Stein-type fashion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>report shrinkage intensity.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The shrinkage estimator is a James-Stein-type estimator.  It is essentially
a  <code>entropy.Dirichlet</code> estimator, where the pseudocount is
estimated from the data.
</p>
<p>For details see Hausser and Strimmer (2009).
</p>


<h3>Value</h3>

<p><code>freqs.shrink</code> returns a shrinkage estimate of the frequencies.
</p>
<p><code>entropy.shrink</code> returns a shrinkage estimate of the Shannon entropy. 
</p>
<p><code>KL.shrink</code> returns a shrinkage estimate of the KL divergence. 
</p>
<p><code>chi2.shrink</code> returns a shrinkage version of the chi-squared divergence. 
</p>
<p><code>mi.shrink</code> returns a shrinkage estimate of the mutual information. 
</p>
<p><code>chi2indep.shrink</code> returns a shrinkage version of the chi-squared divergence of independence. 
</p>
<p>In all instances the estimated shrinkage intensity is attached to the returned
value as attribute <code>lambda.freqs</code>.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Hausser, J., and K. Strimmer. 2009.  Entropy inference and the James-Stein
estimator, with application to nonlinear gene association networks. 
J. Mach. Learn. Res. <b>10</b>: 1469-1484.  Available online from
<a href="https://jmlr.csail.mit.edu/papers/v10/hausser09a.html">https://jmlr.csail.mit.edu/papers/v10/hausser09a.html</a>.
</p>


<h3>See Also</h3>

<p><code>entropy</code>, <code>entropy.Dirichlet</code>, 
<code>entropy.plugin</code>,  
<code>KL.plugin</code>, <code>mi.plugin</code>, <code>discretize</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

# a single variable

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# shrinkage estimate of frequencies
freqs.shrink(y)

# shrinkage estimate of entropy
entropy.shrink(y)


# example with two variables

# observed counts for two random variables
y1 = c(4, 2, 3, 1, 10, 4)
y2 = c(2, 3, 7, 1, 4, 3)

# shrinkage estimate of Kullback-Leibler divergence
KL.shrink(y1, y2)

# half of the shrinkage chi-squared divergence
0.5*chi2.shrink(y1, y2)


## joint distribution example

# contingency table with counts for two discrete variables
y2d = rbind( c(1,2,3), c(6,5,4) )

# shrinkage estimate of mutual information
mi.shrink(y2d)

# half of the shrinkage chi-squared divergence of independence
0.5*chi2indep.shrink(y2d)


</code></pre>


</div>