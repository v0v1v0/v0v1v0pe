<div class="container">

<table style="width: 100%;"><tr>
<td>gofTest</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Goodness-of-Fit Test
</h2>

<h3>Description</h3>

<p>Perform a goodness-of-fit test to determine whether a data set
appears to come from a specified probability distribution or if two
data sets appear to come from the same distribution.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gofTest(y, ...)

## S3 method for class 'formula'
gofTest(y, data = NULL, subset,
  na.action = na.pass, ...)

## Default S3 method:
gofTest(y, x = NULL,
  test = ifelse(is.null(x), "sw", "ks"),
  distribution = "norm", est.arg.list = NULL,
  alternative = "two.sided", n.classes = NULL,
  cut.points = NULL, param.list = NULL,
  estimate.params = ifelse(is.null(param.list), TRUE, FALSE),
  n.param.est = NULL, correct = NULL, digits = .Options$digits,
  exact = NULL, ws.method = "normal scores", warn = TRUE, keep.data = TRUE,
  data.name = NULL, data.name.x = NULL, parent.of.data = NULL,
  subset.expression = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>an object containing data for the goodness-of-fit test.  In the default
method, the argument <code>y</code> must be numeric vector of observations.
In the formula method, <code>y</code> must be a formula of the form <code>y ~ 1</code>
or <code>y ~ x</code>.  The form <code>y ~ 1</code> indicates use the observations in
the vector <code>y</code> for a one-sample goodness-of-fit test.  The form
<code>y ~ x</code> is only relevant to the case of the two-sample
Kolmogorov-Smirnov test (<code>test="ks"</code>) and indicates use the
observations in the vector <code>y</code> as the second sample and use the
observations in the vector <code>x</code> as the first sample.  Note that
for the formula method, <code>x</code> and <code>y</code> must be the same length but
this is not a requirement of the test and you can use vectors of different
lengths via the default method.
Missing (<code>NA</code>), undefined (<code>NaN</code>),
and infinite (<code>Inf</code>, <code>-Inf</code>) values are allowed but will be
removed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>specifies an optional data frame, list or environment (or object coercible
by <code>as.data.frame</code> to a data frame) containing the variables in the
model.  If not found in <code>data</code>, the variables are taken from
<code>environment(formula)</code>, typically the environment from which
<code>gofTest</code> is called.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>

<p>specifies an optional vector specifying a subset of observations to be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>

<p>specifies a function which indicates what should happen when the data contain <code>NA</code>s.
The default is <code>na.pass</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>numeric vector of values for the first sample in the case of a two-sample
Kolmogorov-Smirnov goodness-of-fit test (<code>test="ks"</code>).
Missing (<code>NA</code>), undefined (<code>NaN</code>), and infinite (<code>Inf</code>,
<code>-Inf</code>) values are allowed but will be removed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>

<p>character string defining which goodness-of-fit test to perform.  Possible values are:
</p>

<ul>
<li> <p><code>"sw"</code>. Shapiro-Wilk; the default when <code>x</code> is NOT supplied.
</p>
</li>
<li> <p><code>"sf"</code>. Shapiro-Francia.
</p>
</li>
<li> <p><code>"ppcc"</code>. Probability Plot Correlation Coefficient.
</p>
</li>
<li> <p><code>"ad"</code>.  Anderson-Darling.
</p>
</li>
<li> <p><code>"cmv"</code>. Cramer-von Mises.
</p>
</li>
<li> <p><code>"lillie"</code>. Lilliefor.
</p>
</li>
<li> <p><code>"skew"</code>. Zero-skew.
</p>
</li>
<li> <p><code>"chisq"</code>. Chi-squared.
</p>
</li>
<li> <p><code>"ks"</code>. Kolmogorov-Smirnov; the default when <code>x</code> IS supplied.
</p>
</li>
<li> <p><code>"ws"</code>. Wilk-Shapiro test for Uniform [0, 1] distribution.
</p>
</li>
<li> <p><code>"proucl.ad.gamma"</code>. Anderson-Darling test for a gamma distribution using
ProUCL critical values.
</p>
</li>
<li> <p><code>"proucl.ks.gamma"</code>. Kolmogorov-Smirnov test for a gamma distribution using
ProUCL critical values.
</p>
</li>
</ul>
<p>When the argument <code>x</code> is supplied, you must set <code>test="ks"</code>, which is what <code>gofTest</code>
does by default.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution</code></td>
<td>

<p>a character string denoting the distribution abbreviation.  See the help file for
<code>Distribution.df</code> for a list of distributions and their abbreviations.
The default value is <code>distribution="norm"</code> (Normal distribution).
</p>
<p>When <code>test="sw"</code>, <code>test="sf"</code>, or <code>test="ppcc"</code>, any continuous
distribuiton is allowed (e.g., <code>"norm"</code> (normal), <code>"lnorm"</code> (lognormal),
<code>"gamma"</code> (gamma), etc.), as well as mixed distributions involving the normal distribution
(i.e., <code>"zmnorm"</code> (zero-modified normal), <code>"zmlnorm"</code> (zero-modified lognormal (delta)),
and <br><code>"zmlnormAlt"</code> (zero-modified lognormal with alternative parameterization)).
</p>
<p>When <code>test="ad"</code>, <code>test="cvm"</code>, <code>test="lillie"</code>, or <code>test="skew"</code>,
only the values <code>"norm"</code> (normal), <code>"lnorm"</code> (lognormal),
<code>"lnormAlt"</code> (lognormal with alternative parameterization),
<code>"zmnorm"</code> (zero-modified normal), <code>"zmlnorm"</code> (zero-modified lognormal (delta)), and <br><code>"zmlnormAlt"</code> (zero-modified lognormal with alternative parameterization) are allowed.
</p>
<p>When <code>test="ks"</code>, any continuous distribution is allowed.
</p>
<p>When <code>test="chisq"</code>, any distribuiton is allowed.
</p>
<p>When <code>test="ws"</code>, this argument is ignored.
</p>
<p>When <code>test="proucl.ad.gamma"</code> or <code>test="proucl.ks.gamma"</code>, you must set
<code>distribution="gamma"</code> or <code>distribution="gammaAlt"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>est.arg.list</code></td>
<td>

<p>a list of arguments to be passed to the function estimating the distribution parameters.
For example, if <code>test="sw"</code> and <code>distribution="gamma"</code>, setting <br><code>est.arg.list=list(method="bcmle")</code> indicates using the bias-corrected <br>
maximum-likelihood
estimators of shape and scale (see the help file for <code>egamma</code>).
See the help file
Estimating Distribution Parameters for a list of estimating functions.
The default value is <code>est.arg.list=NULL</code> so that all default values for the
estimating function are used.  This argument is ignored if <br><code>estimate.params=FALSE</code>.
</p>
<p>When <code>test="sw"</code>, <code>test="sf"</code>, <code>test="ppcc"</code>,
<code>test="ad"</code>, <code>test="cvm"</code>, <code>test="lillie"</code>, or <code>test="skew"</code>,
and you are testing for some form of normality (i.e., Normal, Lognormal,
Three-Parameter Lognormal,
Zero-Modified Normal, or
Zero-Modified Lognormal (Delta)),
the estimated parameters are provided in the
output merely for information, and the choice of the method of estimation has no effect
on the goodness-of-fit test statistic or p-value.
</p>
<p>When <code>test="ks"</code>, <code>x</code> is not supplied, and
<code>estimate.params=TRUE</code>, the estimated parameters are used to
specify the null hypothesis of which distribution
the data are assumed to come from.
</p>
<p>When <code>test="chisq"</code> and <code>estimate.params=TRUE</code>,
the estimated parameters are used to specify the null hypothesis of which distribution
the data are assumed to come from.
</p>
<p>When <code>test="ws"</code>, <code>test="proucl.ad.gamma"</code>, or <code>test="proucl.ks.gamma"</code>,
this argument is ignored.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alternative</code></td>
<td>

<p>for the case when <code>test="ks"</code>, <code>test="skew"</code>, or <code>test="ws"</code>,
character string specifying the alternative hypothesis.  When <code>test="ks"</code> or
<code>test="skew"</code>, the possible values are <code>"two-sided"</code> (the default),
<code>"greater"</code>, or <code>"less"</code>.  When <code>test="ws"</code>, the possible values are
<code>"greater"</code> (the default), or <code>"less"</code>.  See the DETAILS section
of the help file for <code>ks.test</code> for more explanation of the
meaning of this argument.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.classes</code></td>
<td>

<p>for the case when <code>test="chisq"</code>, the number of cells into which the observations
are to be allocated.  If the argument <code>cut.points</code> is supplied, then <code>n.classes</code>
is set to <code>length(cut.points)-1</code>.  The default value is <br><code>ceiling(2* (length(x)^(2/5)))</code> and is recommended by Moore (1986).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cut.points</code></td>
<td>

<p>for the case when <code>test="chisq"</code>, a vector of cutpoints that defines the cells.
The element <code>x[i]</code> is allocated to cell <code>j</code> if <br><code>cut.points[j]</code> &lt; <code>x[i]</code> <code class="reqn">\le</code> <code>cut.points[j+1]</code>.
If <code>x[i]</code> is less than or equal to the first cutpoint or
greater than the last cutpoint, then <code>x[i]</code> is treated as missing.  If the
hypothesized distribution is discrete, <code>cut.points</code> must be supplied.  The default
value is <code>cut.points=NULL</code>, in which case the cutpoints are determined by
<code>n.classes</code> equi-probable intervals.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>param.list</code></td>
<td>

<p>for the case when <code>test="ks"</code> and <code>x</code> is not supplied, or when
<code>test="chisq"</code>,
a list with values for the parameters of the specified distribution.  See the help file
for <code>Distribution.df</code> for the names and possible values of the parameters
associated with each distribution.  The default value is <code>param.list=NULL</code>, which forces
estimation of the distribution parameters.  This argument is ignored if
<code>estimate.params=TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimate.params</code></td>
<td>

<p>for the case when <code>test="ks"</code> and <code>x</code> is not supplied, or when
<code>test="chisq"</code>, a logical scalar indicating whether to perform the goodness-of-fit test based on
estimating the distribution parameters (<code>estimate.params=TRUE</code>) or using the
user-supplied distribution parameters specified by <code>param.list</code> <br>
(<code>estimate.params=FALSE</code>).  The default value of <code>estimate.params</code> is
<code>TRUE</code> if <code>param.list=NULL</code>, otherwise it is <code>FALSE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.param.est</code></td>
<td>

<p>for the case when <code>test="ks"</code> and <code>x</code> is not supplied, or when
<code>test="chisq"</code>,
an integer indicating the number of parameters estimated from the data.  <br>
If <code>estimate.params=TRUE</code>, the default value is the number of parameters associated
with the distribution specified by <code>distribution</code> (e.g., 2 for a normal distribution).
If <code>estimate.params=FALSE</code>, the default value is <code>n.param.est=0</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>correct</code></td>
<td>

<p>for the case when <code>test="chisq"</code>, a logical scalar indicating whether to use the
continuity correction.  The default value is <code>correct=FALSE</code> unless <br><code>n.classes=2</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>digits</code></td>
<td>

<p>for the case when <code>test="ks"</code> and <code>x</code> is not supplied, or when
<code>test="chisq"</code>, and <code>param.list</code> is supplied,
a scalar indicating how many significant digits to print out for the parameters
associated with the hypothesized distribution.  The default value is
<code>.Options$digits</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exact</code></td>
<td>

<p>for the case when <code>test="ks"</code>, <code>exact=NULL</code> by default, but can be set to
a logical scalar indicating whether an exact p-value should be computed.
See the help file for <code>ks.test</code> for more information.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ws.method</code></td>
<td>

<p>for the case when <code>test="ws"</code>, this argument specifies whether to perform the test
based on normal scores (<code>ws.method="normal scores"</code>, the default) or
chi-square scores (<code>ws.method="chi-square scores"</code>).  See the DETAILS section
for more information.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warn</code></td>
<td>

<p>logical scalar indicating whether to print a warning message when
observations with <code>NA</code>s, <code>NaN</code>s, or <code>Inf</code>s in
<code>y</code> or <code>x</code> are removed.  The default value is <code>warn=TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.data</code></td>
<td>

<p>logical scalar indicating whether to return the data used for the goodness-of-fit test.
The default value is <code>keep.data=TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data.name</code></td>
<td>

<p>character string indicating the name of the data used for argument <code>y</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data.name.x</code></td>
<td>

<p>character string indicating the name of the data used for argument <code>x</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parent.of.data</code></td>
<td>

<p>character string indicating the source of the data used for the
goodness-of-fit test.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset.expression</code></td>
<td>

<p>character string indicating the expression used to subset the data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>additional arguments affecting the goodness-of-fit test.
</p>
</td>
</tr>
</table>
<h3>Details</h3>


<ul>
<li> <p><b>Shapiro-Wilk Goodness-of-Fit Test</b> (<code>test="sw"</code>).
</p>
<p>The Shapiro-Wilk goodness-of-fit test (Shapiro and Wilk, 1965; Royston, 1992a)
is one of the most commonly used goodness-of-fit tests for normality.
You can use it to test the following hypothesized distributions:
Normal, Lognormal, Three-Parameter Lognormal,
Zero-Modified Normal, or
Zero-Modified Lognormal (Delta).
<b>In addition, you can also use it to test the null hypothesis of any
continuous distribution that is available</b> (see the help file for
<code>Distribution.df</code>, and see explanation below).
<br></p>
<p><b>Shapiro-Wilk W-Statistic and P-Value for Testing Normality</b> <br>
Let <code class="reqn">X</code> denote a random variable with cumulative distribution function (cdf)
<code class="reqn">F</code>.  Suppose we want to test the null hypothesis that <code class="reqn">F</code> is the cdf of
a normal (Gaussian) distribution with some arbitrary mean
<code class="reqn">\mu</code> and standard deviation <code class="reqn">\sigma</code> against the alternative hypothesis
that <code class="reqn">F</code> is the cdf of some other distribution.  The table below shows the
random variable for which <code class="reqn">F</code> is the assumed cdf, given the value of the
argument <code>distribution</code>.
</p>

<table>
<tr>
<td style="text-align: left;">
    <b>Value of</b>     </td>
<td style="text-align: left;">                                         </td>
<td style="text-align: left;"> <b>Random Variable for</b> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>distribution</code> </td>
<td style="text-align: left;"> <b>Distribution Name</b>                </td>
<td style="text-align: left;"> <b>which</b> <code class="reqn">F</code> <b>is the cdf</b> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"norm"</code>       </td>
<td style="text-align: left;"> Normal                                  </td>
<td style="text-align: left;"> <code class="reqn">X</code> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"lnorm"</code>      </td>
<td style="text-align: left;"> Lognormal (Log-space)                   </td>
<td style="text-align: left;"> <code class="reqn">log(X)</code> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"lnormAlt"</code>   </td>
<td style="text-align: left;"> Lognormal (Untransformed)               </td>
<td style="text-align: left;"> <code class="reqn">log(X)</code> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"lnorm3"</code>     </td>
<td style="text-align: left;"> Three-Parameter Lognormal               </td>
<td style="text-align: left;"> <code class="reqn">log(X-\gamma)</code> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"zmnorm"</code>     </td>
<td style="text-align: left;"> Zero-Modified Normal                    </td>
<td style="text-align: left;"> <code class="reqn">X | X &gt; 0</code> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"zmlnorm"</code>    </td>
<td style="text-align: left;"> Zero-Modified Lognormal (Log-space)     </td>
<td style="text-align: left;"> <code class="reqn">log(X) | X &gt; 0</code> </td>
</tr>
<tr>
<td style="text-align: left;">
    <code>"zmlnormAlt"</code> </td>
<td style="text-align: left;"> Zero-Modified Lognormal (Untransformed) </td>
<td style="text-align: left;"> <code class="reqn">log(X) | X &gt; 0</code>
    </td>
</tr>
</table>
<p>Note that for the three-parameter lognormal distribution, the symbol <code class="reqn">\gamma</code>
denotes the threshold parameter.
</p>
<p>Let <code class="reqn">\underline{x} = (x_1, x_2, \ldots, x_n)</code> denote the vector of
<code class="reqn">n</code> <b><em>ordered</em></b> observations assumed to come from a normal
distribution.
<br></p>
<p><em>The Shapiro-Wilk W-Statistic</em> <br>
Shapiro and Wilk (1965) introduced the following statistic to test
the null hypothesis that <code class="reqn">F</code> is the cdf of a normal distribution:
</p>
<p style="text-align: center;"><code class="reqn">W = \frac{(\sum_{i=1}^n a_i x_i)^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \;\;\;\;\;\; (1)</code>
</p>

<p>where the quantity <code class="reqn">a_i</code> is the <code class="reqn">i</code>'th element of the vector
<code class="reqn">\underline{a}</code> defined by:
</p>
<p style="text-align: center;"><code class="reqn">\underline{a} = \frac{\underline{m}^T V^{-1}}{[\underline{m}^T V^{-1} V^{-1} \underline{m}]^{1/2}} \;\;\;\;\;\; (2)</code>
</p>

<p>where <code class="reqn">T</code> denotes the transpose operator, and <code class="reqn">\underline{m}</code> is the vector
of expected values and <code class="reqn">V</code> is the variance-covariance matrix of the order
statistics of a random sample of size <code class="reqn">n</code> from a standard normal distribution.
That is, the values of <code class="reqn">\underline{a}</code> are the expected values of the standard
normal order statistics weighted by their variance-covariance matrix, and
normalized so that
</p>
<p style="text-align: center;"><code class="reqn">\underline{a}^T \underline{a} = 1 \;\;\;\;\;\; (3)</code>
</p>

<p>It can be shown that the coefficients <code class="reqn">\underline{a}</code> are antisymmetric, that
is,
</p>
<p style="text-align: center;"><code class="reqn">a_i = -a_{n-i+1} \;\;\;\;\;\; (4)</code>
</p>

<p>and for odd <code class="reqn">n</code>,
</p>
<p style="text-align: center;"><code class="reqn">a_{(n+1)/2} = 0 \;\;\;\;\;\; (5)</code>
</p>

<p>Now because
</p>
<p style="text-align: center;"><code class="reqn">\bar{a} = \frac{1}{n} \sum_{i=1}^n a_i = 0 \;\;\;\;\;\ (6)</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^n (a_i - \bar{a})^2 = \sum_{i=1}^n a_i^2 = \underline{a}^T \underline{a} = 1 \;\;\;\;\;\; (7)</code>
</p>

<p>the <code class="reqn">W</code>-statistic in Equation (1) is the same as the square of the sample
product-moment correlation between the vectors <code class="reqn">\underline{a}</code> and
<code class="reqn">\underline{x}</code>:
</p>
<p style="text-align: center;"><code class="reqn">W = r(\underline{a}, \underline{x})^2 \;\;\;\;\;\; (8)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">r(\underline{x}, \underline{y}) = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{[\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n (y_i - \bar{y})^2]^{1/2}} \;\;\;\;\;\;\; (9)</code>
</p>

<p>(see the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> help file for <code>cor</code>).
</p>
<p>The Shapiro-Wilk <code class="reqn">W</code>-statistic is also simply the ratio of two estimators of
variance, and can be rewritten as
</p>
<p style="text-align: center;"><code class="reqn">W = \frac{\hat{\sigma}_{BLUE}^2}{\hat{\sigma}_{MVUE}^2} \;\;\;\;\;\; (10)</code>
</p>

<p>where the numerator is the square of the best linear unbiased estimate (BLUE) of
the standard deviation, and the denominator is the minimum variance unbiased
estimator (MVUE) of the variance:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}_{BLUE} = \frac{\sum_{i=1}^n a_i x_i}{\sqrt{n-1}} \;\;\;\;\;\; (11)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\sigma}_{MVUE}^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1} \;\;\;\;\;\; (12)</code>
</p>

<p>Small values of <code class="reqn">W</code> indicate the null hypothesis is probably not true.
Shapiro and Wilk (1965) computed the values of the coefficients <code class="reqn">\underline{a}</code>
and the percentage points for <code class="reqn">W</code> (based on smoothing the empirical null
distribution of <code class="reqn">W</code>) for sample sizes up to 50.  Computation of the
<code class="reqn">W</code>-statistic for larger sample sizes can be cumbersome, since computation of
the coefficients <code class="reqn">\underline{a}</code> requires storage of at least
<code class="reqn">n + [n(n+1)/2]</code> reals followed by <code class="reqn">n \times n</code> matrix inversion
(Royston, 1992a).
<br></p>
<p><em>The Shapiro-Francia W'-Statistic</em> <br>
Shapiro and Francia (1972) introduced a modification of the <code class="reqn">W</code>-test that
depends only on the expected values of the order statistics (<code class="reqn">\underline{m}</code>)
and not on the variance-covariance matrix (<code class="reqn">V</code>):
</p>
<p style="text-align: center;"><code class="reqn">W' = \frac{(\sum_{i=1}^n b_i x_i)^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \;\;\;\;\;\; (13)</code>
</p>

<p>where the quantity <code class="reqn">b_i</code> is the <code class="reqn">i</code>'th element of the vector
<code class="reqn">\underline{b}</code> defined by:
</p>
<p style="text-align: center;"><code class="reqn">\underline{b} = \frac{\underline{m}}{[\underline{m}^T \underline{m}]^{1/2}} \;\;\;\;\;\; (14)</code>
</p>

<p>Several authors, including Ryan and Joiner (1973), Filliben (1975), and Weisberg
and Bingham (1975), note that the <code class="reqn">W'</code>-statistic is intuitively appealing
because it is the squared Pearson correlation coefficient associated with a normal
probability plot.  That is, it is the squared correlation between the ordered
sample values <code class="reqn">\underline{x}</code> and the expected normal order statistics
<code class="reqn">\underline{m}</code>:
</p>
<p style="text-align: center;"><code class="reqn">W' = r(\underline{b}, \underline{x})^2  = r(\underline{m}, \underline{x})^2 \;\;\;\;\;\; (15)</code>
</p>

<p>Shapiro and Francia (1972) present a table of empirical percentage points for <code class="reqn">W'</code>
based on a Monte Carlo simulation.  It can be shown that the asymptotic null
distributions of <code class="reqn">W</code> and <code class="reqn">W'</code> are identical, but convergence is very slow
(Verrill and Johnson, 1988).
<br></p>
<p><em>The Weisberg-Bingham Approximation to the W'-Statistic</em> <br>
Weisberg and Bingham (1975) introduced an approximation of the Shapiro-Francia
<code class="reqn">W'</code>-statistic that is easier to compute.  They suggested using Blom scores
(Blom, 1958, pp.68–75) to approximate the element of <code class="reqn">\underline{m}</code>:
</p>
<p style="text-align: center;"><code class="reqn">\tilde{W}' = \frac{(\sum_{i=1}^n c_i x_i)^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \;\;\;\;\;\; (16)</code>
</p>

<p>where the quantity <code class="reqn">c_i</code> is the <code class="reqn">i</code>'th element of the vector
<code class="reqn">\underline{c}</code> defined by:
</p>
<p style="text-align: center;"><code class="reqn">\underline{c} = \frac{\underline{\tilde{m}}}{[\underline{\tilde{m}}^T \underline{\tilde{m}}]^{1/2}} \;\;\;\;\;\; (17)</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">\tilde{m}_i = \Phi^{-1}[\frac{i - (3/8)}{n + (1/4)}] \;\;\;\;\;\;  (18)</code>
</p>

<p>and <code class="reqn">\Phi</code> denotes the standard normal cdf.  That is, the values of the
elements of <code class="reqn">\underline{m}</code> in Equation (14) are replaced with their estimates
based on the usual plotting positions for a normal distribution.
<br></p>
<p><em>Royston's Approximation to the Shapiro-Wilk W-Test</em> <br>
Royston (1992a) presents an approximation for the coefficients <code class="reqn">\underline{a}</code>
necessary to compute the Shapiro-Wilk <code class="reqn">W</code>-statistic, and also a transformation
of the <code class="reqn">W</code>-statistic that has approximately a standard normal distribution
under the null hypothesis.
</p>
<p>Noting that, up to a constant, the components of <code class="reqn">\underline{b}</code> in
Equation (14) and <code class="reqn">\underline{c}</code> in Equation (17) differ from those of
<code class="reqn">\underline{a}</code> in Equation (2) mainly in the first and last two components,
Royston (1992a) used the approximation <code class="reqn">\underline{c}</code> as the basis for
approximating <code class="reqn">\underline{a}</code> using polynomial (quintic) regression analysis.
For <code class="reqn">4 \le n \le 1000</code>, the approximation gave the following equations for the
last two (and hence first two) components of <code class="reqn">\underline{a}</code>:
</p>
<p style="text-align: center;"><code class="reqn">\tilde{a}_n = c_n + 0.221157 y - 0.147981 y^2 - 2.071190 y^3 + 4.434685 y^4 - 2.706056 y^5 \;\;\;\;\;\; (19)</code>
</p>

<p style="text-align: center;"><code class="reqn">\tilde{a}_{n-1} = c_{n-1} + 0.042981 y - 0.293762 y^2 - 1.752461 y^3 + 5.682633 y^4 - 3.582633 y^5 \;\;\;\;\;\; (20)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">y = \sqrt{n} \;\;\;\;\;\; (21)</code>
</p>

<p>The other components are computed as:
</p>
<p style="text-align: center;"><code class="reqn">\tilde{a}_i = \frac{\tilde{m}_i}{\sqrt{\eta}} \;\;\;\;\;\; (22)</code>
</p>

<p>for <code class="reqn">i = 2, \ldots , n-1</code> if <code class="reqn">n \le 5</code>, or <code class="reqn">i = 3, \ldots, n-2</code> if
<code class="reqn">n &gt; 5</code>, where
</p>
<p style="text-align: center;"><code class="reqn">\eta = \frac{\underline{\tilde{m}}^T \underline{\tilde{m}} - 2 \tilde{m}_n^2}{1 - 2 \tilde{a}_n^2} \;\;\;\;\;\; (23)</code>
</p>

<p>if <code class="reqn">n \le 5</code>, and
</p>
<p style="text-align: center;"><code class="reqn">\eta = \frac{\underline{\tilde{m}}^T \underline{\tilde{m}} - 2 \tilde{m}_n^2 - 2 \tilde{m}_{n-1}^2}{1 - 2 \tilde{a}_n^2 - 2 \tilde{a}_{n-1}^2} \;\;\;\;\;\; (24)</code>
</p>

<p>if <code class="reqn">n &gt; 5</code>.
</p>
<p>Royston (1992a) found his approximation to <code class="reqn">\underline{a}</code> to be accurate to
at least <code class="reqn">\pm 1</code> in the third decimal place over all values of <code class="reqn">i</code> and
selected values of <code class="reqn">n</code>, and also found that critical percentage points of
<code class="reqn">W</code> based on his approximation agreed closely with the exact critical
percentage points calculated by Verrill and Johnson (1988).
<br></p>
<p><em>Transformation of the Null Distribution of W to Normality</em> <br>
In order to compute a p-value associated with a particular value of <code class="reqn">W</code>,
Royston (1992a) approximated the distribution of <code class="reqn">(1-W)</code> by a
three-parameter lognormal distribution for <code class="reqn">4 \le n \le 11</code>,
and the upper half of the distribution of <code class="reqn">(1-W)</code> by a two-parameter
lognormal distribution for <code class="reqn">12 \le n \le 2000</code>.
Setting
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{w - \mu}{\sigma} \;\;\;\;\;\; (25)</code>
</p>

<p>the p-value associated with <code class="reqn">W</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">p = 1 - \Phi(z) \;\;\;\;\;\; (26)</code>
</p>

<p>For <code class="reqn">4 \le n \le 11</code>, the quantities necessary to compute <code class="reqn">z</code> are given by:
</p>
<p style="text-align: center;"><code class="reqn">w = -log[\gamma - log(1 - W)] \;\;\;\;\;\; (27)</code>
</p>

<p style="text-align: center;"><code class="reqn">\gamma = -2.273 + 0.459 n \;\;\;\;\;\; (28)</code>
</p>

<p style="text-align: center;"><code class="reqn">\mu = 0.5440 - 0.39978 n + 0.025054 n^2 - 0.000671 n^3 \;\;\;\;\;\; (29)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma = exp(1.3822 - 0.77857 n + 0.062767 n^2 - 0.0020322 n^3) \;\;\;\;\;\; (30)</code>
</p>

<p>For <code class="reqn">12 \le n \le 2000</code>, the quantities necessary to compute <code class="reqn">z</code> are given
by:
</p>
<p style="text-align: center;"><code class="reqn">w = log(1 - W) \;\;\;\;\;\; (31)</code>
</p>

<p style="text-align: center;"><code class="reqn">\gamma = log(n) \;\;\;\;\;\; (32)</code>
</p>

<p style="text-align: center;"><code class="reqn">\mu = -1.5861 - 0.31082 y - 0.083751 y^2 + 0.00038915 y^3 \;\;\;\;\;\; (33)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma = exp(-0.4803 - 0.082676 y + 0.0030302 y^2) \;\;\;\;\;\; (34)</code>
</p>

<p>For the last approximation when <code class="reqn">12 \le n \le 2000</code>, Royston (1992a) claims
this approximation is actually valid for sample sizes up to <code class="reqn">n = 5000</code>.
<br></p>
<p><em>Modification for the Three-Parameter Lognormal Distribution</em> <br>
When <code>distribution="lnorm3"</code>, the function <code>gofTest</code> assumes the vector
<code class="reqn">\underline{x}</code> is a random sample from a
three-parameter lognormal distribution.  It estimates the
threshold parameter via the zero-skewness method (see <code>elnorm3</code>), and
then performs the Shapiro-Wilk goodness-of-fit test for normality on
<code class="reqn">log(x-\hat{\gamma})</code> where <code class="reqn">\hat{\gamma}</code> is the estimated threshold
parmater.  Because the threshold parameter has to be estimated, however, the
p-value associated with the computed z-statistic will tend to be conservative
(larger than it should be under the null hypothesis).  Royston (1992b) proposed
the following transformation of the z-statistic:
</p>
<p style="text-align: center;"><code class="reqn">z' = \frac{z - \mu_z}{\sigma_z} \;\;\;\;\;\; (35)</code>
</p>

<p>where for <code class="reqn">5 \le n \le 11</code>,
</p>
<p style="text-align: center;"><code class="reqn">\mu_z = -3.8267 + 2.8242 u - 0.63673 u^2 - 0.020815 v \;\;\;\;\;\; (36)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma_z = -4.9914 + 8.6724 u - 4.27905 u^2 + 0.70350 u^3 - 0.013431 v \;\;\;\;\;\; (37)</code>
</p>

<p>and for <code class="reqn">12 \le n \le 2000</code>,
</p>
<p style="text-align: center;"><code class="reqn">\mu_z =  -3.7796 + 2.4038 u - 0.6675 u^2 - 0.082863 u^3 - 0.0037935 u^4 - 0.027027 v - 0.0019887 vu \;\;\;\;\;\; (38)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma_z =  2.1924 - 1.0957 u + 0.33737 u^2 - 0.043201 u^3 + 0.0019974 u^4 - 0.0053312 vu   \;\;\;\;\;\; (39)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">u = log(n) \;\;\;\;\;\; (40)</code>
</p>

<p style="text-align: center;"><code class="reqn">v = u (\hat{\sigma} - \hat{\sigma}^2) \;\;\;\;\;\; (41)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \;\;\;\;\;\; (42)</code>
</p>

<p style="text-align: center;"><code class="reqn">y_i = log(x_i - \hat{\gamma}) \;\;\;\;\;\; (43)</code>
</p>

<p>and <code class="reqn">\gamma</code> denotes the threshold parameter.  The p-value associated with
this test is then given by:
</p>
<p style="text-align: center;"><code class="reqn">p = 1 - \Phi(z') \;\;\;\;\;\; (44)</code>
</p>

<p><em>Testing Goodness-of-Fit for Any Continuous Distribution</em> <br>
The function <code>gofTest</code> extends the Shapiro-Wilk test to test for
goodness-of-fit for any continuous distribution by using the idea of
Chen and Balakrishnan (1995), who proposed a general purpose approximate
goodness-of-fit test based on the Cramer-von Mises or Anderson-Darling
goodness-of-fit tests for normality.  The function <code>gofTest</code> modifies the
approach of Chen and Balakrishnan (1995) by using the same first 2 steps, and then
applying the Shapiro-Wilk test:
</p>

<ol>
<li>
<p> Let <code class="reqn">\underline{x} = x_1, x_2, \ldots, x_n</code> denote the vector of
<code class="reqn">n</code> <b><em>ordered</em></b> observations.
Compute cumulative probabilities for each <code class="reqn">x_i</code> based on the
cumulative distribution function for the hypothesized distribution.  That is,
compute <code class="reqn">p_i = F(x_i, \hat{\theta})</code> where <code class="reqn">F(x, \theta)</code> denotes the
hypothesized cumulative distribution function with parameter(s) <code class="reqn">\theta</code>,
and <code class="reqn">\hat{\theta}</code> denotes the estimated parameter(s).
</p>
</li>
<li>
<p> Compute standard normal deviates based on the computed cumulative
probabilities: <br><code class="reqn">y_i = \Phi^{-1}(p_i)</code>
</p>
</li>
<li>
<p> Perform the Shapiro-Wilk goodness-of-fit test on the <code class="reqn">y_i</code>'s.
</p>
</li>
</ol>
</li>
<li> <p><b>Shapiro-Francia Goodness-of-Fit Test</b> (<code>test="sf"</code>).
</p>
<p>The Shapiro-Francia goodness-of-fit test (Shapiro and Francia, 1972;
Weisberg and Bingham, 1975; Royston, 1992c) is also one of the most commonly
used goodness-of-fit tests for normality.  You can use it to test the following
hypothesized distributions:
Normal, Lognormal, Zero-Modified Normal,
or Zero-Modified Lognormal (Delta).  In addition,
you can also use it to test the null hypothesis of any continuous distribution
that is available (see the help file for <code>Distribution.df</code>).  See the
section <em>Testing Goodness-of-Fit for Any Continuous Distribution</em> above for
an explanation of how this is done.
<br></p>
<p><em>Royston's Transformation of the Shapiro-Francia W'-Statistic to Normality</em> <br>
Equation (13) above gives the formula for the Shapiro-Francia W'-statistic, and
Equation (16) above gives the formula for Weisberg-Bingham approximation to the
W'-statistic (denoted <code class="reqn">\tilde{W}'</code>).  Royston (1992c) presents an algorithm
to transform the <code class="reqn">\tilde{W}'</code>-statistic so that its null distribution is
approximately a standard normal.  For <code class="reqn">5 \le n \le 5000</code>,
Royston (1992c) approximates the distribution of <code class="reqn">(1-\tilde{W}')</code> by a
lognormal distribution.  Setting
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{w-\mu}{\sigma} \;\;\;\;\;\; (45)</code>
</p>

<p>the p-value associated with <code class="reqn">\tilde{W}'</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">p = 1 - \Phi(z) \;\;\;\;\;\; (46)</code>
</p>

<p>The quantities necessary to compute <code class="reqn">z</code> are given by:
</p>
<p style="text-align: center;"><code class="reqn">w = log(1 - \tilde{W}') \;\;\;\;\;\; (47)</code>
</p>

<p style="text-align: center;"><code class="reqn">\nu = log(n) \;\;\;\;\;\; (48)</code>
</p>

<p style="text-align: center;"><code class="reqn">u = log(\nu) - \nu \;\;\;\;\;\; (49)</code>
</p>

<p style="text-align: center;"><code class="reqn">\mu = -1.2725 + 1.0521 u \;\;\;\;\;\; (50)</code>
</p>

<p style="text-align: center;"><code class="reqn">v = log(\nu) + \frac{2}{\nu} \;\;\;\;\;\; (51)</code>
</p>

<p style="text-align: center;"><code class="reqn">\sigma = 1.0308 - 0.26758 v \;\;\;\;\;\; (52)</code>
</p>

<p><em>Testing Goodness-of-Fit for Any Continuous Distribution</em> <br>
The function <code>gofTest</code> extends the Shapiro-Francia test to test for
goodness-of-fit for any continuous distribution by using the idea of
Chen and Balakrishnan (1995), who proposed a general purpose approximate
goodness-of-fit test based on the Cramer-von Mises or Anderson-Darling
goodness-of-fit tests for normality.  The function <code>gofTest</code> modifies the
approach of Chen and Balakrishnan (1995) by using the same first 2 steps, and then
applying the Shapiro-Francia test:
</p>

<ol>
<li>
<p> Let <code class="reqn">\underline{x} = x_1, x_2, \ldots, x_n</code> denote the vector of
<code class="reqn">n</code> <b><em>ordered</em></b> observations.
Compute cumulative probabilities for each <code class="reqn">x_i</code> based on the
cumulative distribution function for the hypothesized distribution.  That is,
compute <code class="reqn">p_i = F(x_i, \hat{\theta})</code> where <code class="reqn">F(x, \theta)</code> denotes the
hypothesized cumulative distribution function with parameter(s) <code class="reqn">\theta</code>,
and <code class="reqn">\hat{\theta}</code> denotes the estimated parameter(s).
</p>
</li>
<li>
<p> Compute standard normal deviates based on the computed cumulative
probabilities: <br><code class="reqn">y_i = \Phi^{-1}(p_i)</code>
</p>
</li>
<li>
<p> Perform the Shapiro-Francia goodness-of-fit test on the <code class="reqn">y_i</code>'s.
</p>
</li>
</ol>
</li>
<li> <p><b>Probability Plot Correlation Coefficient (PPCC) Goodness-of-Fit Test</b> (<code>test="ppcc"</code>).
</p>
<p>The PPPCC goodness-of-fit test (Filliben, 1975; Looney and Gulledge, 1985) can be
used to test the following hypothesized distributions:
Normal, Lognormal,
Zero-Modified Normal, or
Zero-Modified Lognormal (Delta).  In addition,
you can also use it to test the null hypothesis of any continuous distribution that
is available (see the help file for <code>Distribution.df</code>).
The function <code>gofTest</code> computes the PPCC test
statistic using Blom plotting positions.
</p>
<p>Filliben (1975) proposed using the correlation coefficient <code class="reqn">r</code> from a
normal probability plot to perform a goodness-of-fit test for
normality, and he provided a table of critical values for <code class="reqn">r</code> under the
for samples sizes between 3 and 100.  Vogel (1986) provided an additional table
for sample sizes between 100 and 10,000.
</p>
<p>Looney and Gulledge (1985) investigated the characteristics of Filliben's
probability plot correlation coefficient (PPCC) test using the plotting position
formulas given in Filliben (1975), as well as three other plotting position
formulas:  Hazen plotting positions, Weibull plotting positions, and Blom plotting
positions (see the help file for <code>qqPlot</code> for an explanation of these
plotting positions).  They concluded that the PPCC test based on Blom plotting
positions performs slightly better than tests based on other plotting positions, and
they provide a table of empirical percentage points for the distribution of <code class="reqn">r</code>
based on Blom plotting positions.
</p>
<p>The function <code>gofTest</code> computes the PPCC test statistic <code class="reqn">r</code> using Blom
plotting positions.  It can be shown that the square of this statistic is
equivalent to the Weisberg-Bingham Approximation to the Shapiro-Francia
W'-Test (Weisberg and Bingham, 1975; Royston, 1993).  Thus the PPCC
goodness-of-fit test is equivalent to the Shapiro-Francia goodness-of-fit test.
<br></p>
</li>
<li> <p><b>Anderson-Darling Goodness-of-Fit Test</b> (<code>test="ad"</code>).
</p>
<p>The Anderson-Darling goodness-of-fit test (Stephens, 1986a; Thode, 2002) can be used to test the
following hypothesized distributions:
Normal, Lognormal, Zero-Modified Normal,
or Zero-Modified Lognormal (Delta).
</p>
<p>When <code>test="ad"</code>, the function <code>gofTest</code> calls the function
<code>ad.test</code> in the package <span class="pkg">nortest</span>.  Documentation from that
package is as follows:
</p>
<p>The Anderson-Darling test is an EDF omnibus test for the composite hypothesis of normality.
The test statistic is:
</p>
<p style="text-align: center;"><code class="reqn">A = -n - \frac{1}{n} \sum_{i=1}^n [2i - 1][ln(p_{(i)}) + ln(1 - p_{(n-i+1)})]</code>
</p>

<p>where <code class="reqn">p_{(i)} = \Phi([x_{(i)} - \bar{x}]/s)</code>.  Here, <code class="reqn">\Phi</code> is the cumulative
distribution function of the standard normal distribution, and <code class="reqn">\bar{x}</code> and <code class="reqn">s</code>
are mean and standard deviation of the data values. The p-value is computed from the
modified statistic <code class="reqn">Z = A (1.0 + 0.75/n + 2.25/n^2)</code> according to Table 4.9 in
Stephens [(1986a)].
<br></p>
</li>
<li> <p><b>Cramer-von Mises Goodness-of-Fit Test</b> (<code>test="cvm"</code>).
</p>
<p>The Cramer-von Mises goodness-of-fit test (Stephens, 1986a; Thode, 2002) can be used to test the
following hypothesized distributions:
Normal, Lognormal, Zero-Modified Normal,
or Zero-Modified Lognormal (Delta).
</p>
<p>When <code>test="cvm"</code>, the function <code>gofTest</code> calls the function
<code>cvm.test</code> in the package <span class="pkg">nortest</span>.  Documentation from that
package is as follows:
</p>
<p>The Cramer-von Mises test is an EDF omnibus test for the composite hypothesis of normality.
The test statistic is:
</p>
<p style="text-align: center;"><code class="reqn">W = \frac{1}{12n} + \sum_{i=1}^n \left(p_{(i)} - \frac{2i-1}{2n}\right)^2</code>
</p>

<p>where <code class="reqn">p_{(i)} = \Phi([x_{(i)} - \bar{x}]/s)</code>.  Here, <code class="reqn">\Phi</code> is the cumulative
distribution function of the standard normal distribution, and <code class="reqn">\bar{x}</code> and <code class="reqn">s</code>
are mean and standard deviation of the data values. The p-value is computed from the
modified statistic <code class="reqn">Z = W (1.0 + 0.75/n)</code> according to Table 4.9 in Stephens [(1986a)].
<br></p>
</li>
<li> <p><b>Lilliefors Goodness-of-Fit Test</b> (<code>test="lillie"</code>).
</p>
<p>The Lilliefors goodness-of-fit test (Stephens, 1974; Dallal and Wilkinson, 1986; Thode, 2002)
can be used to test the following hypothesized distributions:
Normal, Lognormal, Zero-Modified Normal,
or Zero-Modified Lognormal (Delta).
</p>
<p>When <code>test="lillie"</code>, the function <code>gofTest</code> calls the function
<code>lillie.test</code> in the package <span class="pkg">nortest</span>.  Documentation from that
package is as follows:
</p>
<p>The Lilliefors (Kolmogorov-Smirnov) test is an EDF omnibus test for the
composite hypothesis of normality.  The test statistic is the
maximal absolute difference between empirical and hypothetical cumulative
distribution function.  It may be computed as <code class="reqn">D = max\{D^+, D^-\}</code> with
</p>
<p style="text-align: center;"><code class="reqn">D^+ = \max_{i = 1, \ldots, n} \{i/n - p_{(i)}\}, \;\; D^- = \max_{i = 1, \ldots, n} \{p_{(i)} - (i-1)/n\} </code>
</p>

<p>where <code class="reqn">p_{(i)} = \Phi([x_{(i)} - \bar{x}]/s)</code>.  Here, <code class="reqn">\Phi</code> is the cumulative
distribution function of the standard normal distribution, and <code class="reqn">\bar{x}</code> and <code class="reqn">s</code>
are mean and standard deviation of the data values. The p-value is computed from
the Dallal-Wilkinson (1986) formula, which is claimed to be only reliable when the
p-value is smaller than 0.1.  If the Dallal-Wilkinson p-value turns out to be
greater than 0.1, then the p-value is computed from the distribution of the
modified statistic <code class="reqn">Z = D (\sqrt{n} - 0.01 + 0.85/\sqrt{n})</code>, see Stephens (1974),
the actual p-value formula being obtained by a simulation and approximation process.
<br></p>
</li>
<li> <p><b>Zero-Skew Goodness-of-Fit Test</b> (<code>test="skew"</code>).
</p>
<p>The Zero-skew goodness-of-fit test (D'Agostino, 1970) can be used to test the
following hypothesized distributions:
Normal, Lognormal, Zero-Modified Normal,
or Zero-Modified Lognormal (Delta).
</p>
<p>When <code>test="skew"</code>, the function <code>gofTest</code> tests the null hypothesis
that the skew of the distribution is 0:
</p>
<p style="text-align: center;"><code class="reqn">H_0: \sqrt{\beta}_1 = 0 \;\;\;\;\;\; (53)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\sqrt{\beta}_1 = \frac{\mu_3}{\mu_2^{3/2}} \;\;\;\;\;\; (54)</code>
</p>

<p>and the quantity <code class="reqn">\mu_r</code> denotes the <code class="reqn">r</code>'th moment about the mean
(also called the <code class="reqn">r</code>'th central moment).  The quantity <code class="reqn">\sqrt{\beta_1}</code>
is called the coefficient of skewness, and is estimated by:
</p>
<p style="text-align: center;"><code class="reqn">\sqrt{b}_1 = \frac{m_3}{m_2^{3/2}} \;\;\;\;\;\; (55)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">m_r = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^r \;\;\;\;\;\; (56)</code>
</p>

<p>denotes the <code class="reqn">r</code>'th sample central moment.
</p>
<p>The possible alternative hypotheses are:
</p>
<p style="text-align: center;"><code class="reqn">H_a: \sqrt{\beta}_1 \ne 0 \;\;\;\;\;\; (57)</code>
</p>

<p style="text-align: center;"><code class="reqn">H_a: \sqrt{\beta}_1 &lt; 0 \;\;\;\;\;\; (58)</code>
</p>

<p style="text-align: center;"><code class="reqn">H_a: \sqrt{\beta}_1 &gt; 0 \;\;\;\;\;\; (59)</code>
</p>

<p>which correspond to <code>alternative="two-sided"</code>, <code>alternative="less"</code>, and <br><code>alternative="greater"</code>, respectively.
</p>
<p>To test the null hypothesis of zero skew, D'Agostino (1970) derived an
approximation to the distribution of <code class="reqn">\sqrt{b_1}</code> under the null hypothesis of
zero-skew, assuming the observations comprise a random sample from a normal
(Gaussian) distribution.  Based on D'Agostino's approximation, the statistic
<code class="reqn">Z</code> shown below is assumed to follow a standard normal distribution and is
used to compute the p-value associated with the test of <code class="reqn">H_0</code>:
</p>
<p style="text-align: center;"><code class="reqn">Z = \delta \;\; log\{ \frac{Y}{\alpha} + [(\frac{Y}{\alpha})^2 + 1]^{1/2} \} \;\;\;\;\;\; (60)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">Y = \sqrt{b_1} [\frac{(n+1)(n+3)}{6(n-2)}]^{1/2} \;\;\;\;\;\; (61)</code>
</p>

<p style="text-align: center;"><code class="reqn">\beta_2 = \frac{3(n^2 + 27n - 70)(n+1)(n+3)}{(n-2)(n+5)(n+7)(n+9)} \;\;\;\;\;\; (62)</code>
</p>

<p style="text-align: center;"><code class="reqn">W^2 = -1 + \sqrt{2\beta_2 - 2} \;\;\;\;\;\; (63)</code>
</p>

<p style="text-align: center;"><code class="reqn">\delta = 1 / \sqrt{log(W)} \;\;\;\;\;\; (64)</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha = [2 / (W^2 - 1)]^{1/2} \;\;\;\;\;\; (65)</code>
</p>

<p>When the sample size <code class="reqn">n</code> is at least 150, a simpler approximation may be
used in which <code class="reqn">Y</code> in Equation (61) is assumed to follow a standard normal
distribution and is used to compute the p-value associated with the hypothesis
test.
<br></p>
</li>
<li> <p><b>Kolmogorov-Smirnov Goodness-of-Fit Test</b> (<code>test="ks"</code>).
</p>
<p>When <code>test="ks"</code>, the function <code>gofTest</code> calls the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function
<code>ks.test</code> to compute the test statistic and p-value.  Note that for the
one-sample case, the distribution parameters
should be pre-specified and not estimated from the data, and if the distribution parameters
are estimated from the data you will receive a warning that this test is very conservative
(Type I error smaller than assumed; high Type II error) in this case.
<br></p>
</li>
<li> <p><b>ProUCL Kolmogorov-Smirnov Goodness-of-Fit Test for Gamma</b> (<code>test="proucl.ks.gamma"</code>).
</p>
<p>When <code>test="proucl.ks.gamma"</code>, the function <code>gofTest</code> calls the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function
<code>ks.test</code> to compute the Kolmogorov-Smirnov test statistic based on the
maximum likelihood estimates of the shape and scale parameters (see <code>egamma</code>).
The p-value is computed based on the simulated critical values given in
<code>ProUCL.Crit.Vals.for.KS.Test.for.Gamma.array</code> (USEPA, 2015).
The sample size must be between 5 and 1000, and the value of the maximum likelihood
estimate of the shape parameter must be between 0.025 and 50.  The critical value
for the test statistic is computed using the simulated critical values and
linear interpolation.
<br></p>
</li>
<li> <p><b>ProUCL Anderson-Darling Goodness-of-Fit Test for Gamma</b> (<code>test="proucl.ad.gamma"</code>).
</p>
<p>When <code>test="proucl.ad.gamma"</code>, the function <code>gofTest</code> computes the
Anderson-Darling test statistic (Stephens, 1986a, p.101) based on the
maximum likelihood estimates of the shape and scale parameters (see <code>egamma</code>).
The p-value is computed based on the simulated critical values given in
<code>ProUCL.Crit.Vals.for.AD.Test.for.Gamma.array</code> (USEPA, 2015).
The sample size must be between 5 and 1000, and the value of the maximum likelihood
estimate of the shape parameter must be between 0.025 and 50.  The critical value
for the test statistic is computed using the simulated critical values and
linear interpolation.
<br></p>
</li>
<li> <p><b>Chi-Squared Goodness-of-Fit Test</b> (<code>test="chisq"</code>).
</p>
<p>The method used by <code>gofTest</code> is a modification of what is used for <code>chisq.test</code>.
If the hypothesized distribution function is completely specified, the degrees of
freedom are <code class="reqn">m-1</code> where <code class="reqn">m</code> denotes the number of classes.  If any parameters
are estimated, the degrees of freedom depend on the method of estimation.
The function <code>gofTest</code> follows the convention of computing
degrees of freedom as <code class="reqn">m-1-k</code>, where <code class="reqn">k</code> is the number of parameters estimated.
It can be shown that if the parameters are estimated by maximum likelihood, the degrees of
freedom are bounded between <code class="reqn">m-1</code> and <code class="reqn">m-1-k</code>.  Therefore, especially when the
sample size is small, it is important to compare the test statistic to the chi-squared
distribution with both <code class="reqn">m-1</code> and <code class="reqn">m-1-k</code> degrees of freedom.  See
Kendall and Stuart (1991, Chapter 30) for a more complete discussion.
</p>
<p>The distribution theory of chi-square statistics is a large sample theory.
The expected cell counts are assumed to be at least moderately large.
As a rule of thumb, each should be at least 5.  Although authors have found this rule
to be conservative (especially when the class probabilities are not too different
from each other), the user should regard p-values with caution when expected cell
counts are small.
<br></p>
</li>
<li> <p><b>Wilk-Shapiro Goodness-of-Fit Test for Uniform [0, 1] Distribution</b> (<code>test="ws"</code>).
</p>
<p>Wilk and Shapiro (1968) suggested this test in the context of jointly testing several
independent samples for normality simultaneously.  If <code class="reqn">p_1, p_2, \ldots, p_n</code> denote
the p-values associated with the test for normality of <code class="reqn">n</code> independent samples, then
under the null hypothesis that all <code class="reqn">n</code> samples come from a normal distribution, the
p-values are a random sample of <code class="reqn">n</code> observations from a Uniform [0,1] distribution,
that is a Uniform distribution with minimum 0 and maximum 1.  Wilk and Shapiro (1968)
suggested two different methods for testing whether the p-values come from a Uniform [0, 1]
distribution:
</p>

<ul>
<li> <p><em>Test Based on Normal Scores.</em> Under the null hypothesis,
the normal scores
</p>
<p style="text-align: center;"><code class="reqn">\Phi^{-1}(p_1), \Phi^{-1}(p_2), \ldots, \Phi^{-1}(p_n)</code>
</p>

<p>are a random sample of <code class="reqn">n</code> observations from a standard normal distribution.
Wilk and Shapiro (1968) denote the <code class="reqn">i</code>'th normal score by
</p>
<p style="text-align: center;"><code class="reqn">G_i = \Phi^{-1}(p_i) \;\;\;\;\;\; (66)</code>
</p>

<p>and note that under the null hypothesis, the quantity <code class="reqn">G</code> defined as
</p>
<p style="text-align: center;"><code class="reqn">G = \frac{1}{\sqrt{n}} \, \sum^n_{1}{G_i} \;\;\;\;\;\; (67)</code>
</p>

<p>has a standard normal distribution.  Wilk and Shapiro (1968) were
interested in the alternative hypothesis that some of the <code class="reqn">n</code>
independent samples did not come from a normal distribution and hence
would be associated with smaller p-values than expected under the
null hypothesis, which translates to the alternative that the cdf for
the distribution of the p-values is greater than the cdf of a
Uniform [0, 1] distribution (<code>alternative="greater"</code>).  In terms
of the test statistic <code class="reqn">G</code>, this alternative hypothesis would
tend to make <code class="reqn">G</code> smaller than expected, so the p-value is given by
<code class="reqn">\Phi(G)</code>.  For the one-sided lower alternative that the cdf for the
distribution of p-values is less than the cdf for a Uniform [0, 1]
distribution, the p-value is given by
</p>
<p style="text-align: center;"><code class="reqn">p = 1 - \Phi(G) \;\;\;\;\;\; (68)</code>
</p>
<p>.
</p>
</li>
<li> <p><em>Test Based on Chi-Square Scores.</em>  Under the null hypothesis, the
chi-square scores
</p>
<p style="text-align: center;"><code class="reqn">-2 \, log(p_1), -2 \, log(p_2), \ldots, -2 \, log(p_n)</code>
</p>

<p>are a random sample of <code class="reqn">n</code> observations from a chi-square distribution with
2 degrees of freedom (Fisher, 1950).  Wilk and Shapiro (1968) denote the
<code class="reqn">i</code>'th chi-square score by
</p>
<p style="text-align: center;"><code class="reqn">C_i = -2 \, log(p_i) \;\;\;\;\;\; (69)</code>
</p>

<p>and note that under the null hypothesis, the quantity <code class="reqn">C</code> defined as
</p>
<p style="text-align: center;"><code class="reqn">C = \sum^n_{1}{C_i} \;\;\;\;\;\; (70)</code>
</p>

<p>has a chi-square distribution with <code class="reqn">2n</code> degrees of freedom.
Wilk and Shapiro (1968) were
interested in the alternative hypothesis that some of the <code class="reqn">n</code>
independent samples did not come from a normal distribution and hence
would be associated with smaller p-values than expected under the
null hypothesis, which translates to the alternative that the cdf for
the distribution of the p-values is greater than the cdf of a
Uniform [0, 1] distribution (<code>alternative="greater"</code>).  In terms
of the test statistic <code class="reqn">C</code>, this alternative hypothesis would
tend to make <code class="reqn">C</code> larger than expected, so the p-value is given by
</p>
<p style="text-align: center;"><code class="reqn">p = 1 - F_{2n}(C) \;\;\;\;\;\; (71)</code>
</p>

<p>where <code class="reqn">F_2n</code> denotes the cumulative distribution function of the
chi-square distribution with <code class="reqn">2n</code> degrees of freedom.
For the one-sided lower alternative that
the cdf for the distribution of p-values is less than the cdf for a
Uniform [0, 1] distribution, the p-value is given by
</p>
<p style="text-align: center;"><code class="reqn">p = F_{2n}(C) \;\;\;\;\;\; (72)</code>
</p>

</li>
</ul>
</li>
</ul>
<h3>Value</h3>

<p>a list of class <code>"gof"</code> containing the results of the goodness-of-fit test, unless
the two-sample <br>
Kolmogorov-Smirnov test is used, in which case the value is a list of
class <code>"gofTwoSample"</code>.  Objects of class <code>"gof"</code> and <code>"gofTwoSample"</code>
have special printing and plotting methods.  See the help files for <code>gof.object</code>
and <code>gofTwoSample.object</code> for details.
</p>


<h3>Note</h3>

<p>The Shapiro-Wilk test (Shapiro and Wilk, 1965) and the Shapiro-Francia test
(Shapiro and Francia, 1972) are probably the two most commonly used hypothesis tests to
test departures from normality.  The Shapiro-Wilk test is most powerful at detecting
short-tailed (platykurtic) and skewed distributions, and least powerful against
symmetric, moderately long-tailed (leptokurtic) distributions.  Conversely, the
Shapiro-Francia test is more powerful against symmetric long-tailed distributions and
less powerful against short-tailed distributions (Royston, 1992b; 1993).
In general, the Shapiro-Wilk and Shapiro-Francia tests outperform the Anderson-Darling
test, which in turn outperforms the Cramer-von Mises test, which in turn
outperforms the Lilliefors test (Stephens, 1986a; Razali and Wah, 2011; Romao et al., 2010).
</p>
<p>The zero-skew goodness-of-fit test for normality is one of several tests that have
been proposed to test the assumption of a normal distribution (D'Agostino, 1986b).
This test has been included mainly because it is called by <code>elnorm3</code>.
Ususally, the Shapiro-Wilk or Shapiro-Francia test is preferred to this test, unless
the direction of the alternative to normality (e.g., positive skew) is known
(D'Agostino, 1986b, pp. 405–406).
</p>
<p>Kolmogorov (1933) introduced a goodness-of-fit test to test the hypothesis that a
random sample of <code class="reqn">n</code> observations <b>x</b> comes from a specific hypothesized distribution
with cumulative distribution function <code class="reqn">H</code>.  This test is now usually called the
one-sample Kolmogorov-Smirnov goodness-of-fit test.  Smirnov (1939) introduced a
goodness-of-fit test to test the hypothesis that a random sample of <code class="reqn">n</code>
observations <b>x</b> comes from the same distribution as a random sample of
<code class="reqn">m</code> observations <b>y</b>.  This test is now usually called the two-sample
Kolmogorov-Smirnov goodness-of-fit test.  Both tests are based on the maximum
vertical distance between two cumulative distribution functions.  For the one-sample problem
with a small sample size, the Kolmogorov-Smirnov test may be preferred over the chi-squared
goodness-of-fit test since the KS-test is exact, while the chi-squared test is based on
an asymptotic approximation.
</p>
<p>The chi-squared test, introduced by Pearson in 1900, is the oldest and best known
goodness-of-fit test.  The idea is to reduce the goodness-of-fit problem to a
multinomial setting by comparing the observed cell counts with their expected values
under the null hypothesis.  Grouping the data sacrifices information, especially if the
hypothesized distribution is continuous.  On the other hand, chi-squared tests can be be
applied to any type of variable: continuous, discrete, or a combination of these.
</p>
<p>The Wilk-Shapiro (1968) tests for a Uniform [0, 1] distribution were introduced in the context
of testing whether several independent samples all come from normal distributions, with
possibly different means and variances.  The function <code>gofGroupTest</code> extends
this idea to allow you to test whether several independent samples come from the same
distribution (e.g., gamma, extreme value, etc.), with possibly different parameters.
</p>
<p>In practice, almost any goodness-of-fit test will <em>not</em> reject the null hypothesis
if the number of observations is relatively small.  Conversely, almost any goodness-of-fit
test <em>will</em> reject the null hypothesis if the number of observations is very large,
since “real” data are never distributed according to any theoretical distribution
(Conover, 1980, p.367).  For most cases, however, the distribution of “real” data
is close enough to some theoretical distribution that fairly accurate results may be
provided by assuming that particular theoretical distribution.  One way to asses the
goodness of the fit is to use goodness-of-fit tests.  Another way is to look at
quantile-quantile (Q-Q) plots (see <code>qqPlot</code>).
</p>


<h3>Author(s)</h3>

<p>Steven P. Millard (<a href="mailto:EnvStats@ProbStatInfo.com">EnvStats@ProbStatInfo.com</a>)
</p>
<p>Juergen Gross and Uwe Ligges for the Anderson-Darling, Carmer-von Mises, and Lilliefors tests called
from the package <span class="pkg">nortest</span>.
</p>


<h3>References</h3>

<p>Birnbaum, Z.W., and F.H. Tingey. (1951).
One-Sided Confidence Contours for Probability Distribution Functions.
<em>Annals of Mathematical Statistics</em> <b>22</b>, 592-596.
</p>
<p>Blom, G. (1958). <em>Statistical Estimates and Transformed Beta Variables</em>.
John Wiley and Sons, New York.
</p>
<p>Conover, W.J. (1980). <em>Practical Nonparametric Statistics</em>. Second Edition.
John Wiley and Sons, New York.
</p>
<p>Dallal, G.E., and L. Wilkinson. (1986).
An Analytic Approximation to the Distribution of Lilliefor's Test for Normality.
<em>The American Statistician</em> <b>40</b>, 294-296.
</p>
<p>D'Agostino, R.B. (1970). Transformation to Normality of the Null Distribution of <code class="reqn">g1</code>.
<em>Biometrika</em> <b>57</b>, 679-681.
</p>
<p>D'Agostino, R.B. (1971). An Omnibus Test of Normality for Moderate and Large Size Samples.
<em>Biometrika</em> <b>58</b>, 341-348.
</p>
<p>D'Agostino, R.B. (1986b). Tests for the Normal Distribution. In: D'Agostino, R.B., and M.A. Stephens, eds.
<em>Goodness-of Fit Techniques</em>. Marcel Dekker, New York.
</p>
<p>D'Agostino, R.B., and E.S. Pearson (1973). Tests for Departures from Normality.
Empirical Results for the Distributions of <code class="reqn">b2</code> and <code class="reqn">\sqrt{b1}</code>.
<em>Biometrika</em> <b>60</b>(3), 613-622.
</p>
<p>D'Agostino, R.B., and G.L. Tietjen (1973). Approaches to the Null Distribution of <code class="reqn">\sqrt{b1}</code>.
<em>Biometrika</em> <b>60</b>(1), 169-173.
</p>
<p>Fisher, R.A. (1950). <em>Statistical Methods for Research Workers</em>. 11'th Edition.
Hafner Publishing Company, New York, pp.99-100.
</p>
<p>Gibbons, R.D., D.K. Bhaumik, and S. Aryal. (2009).
<em>Statistical Methods for Groundwater Monitoring</em>, Second Edition.
John Wiley &amp; Sons, Hoboken.
</p>
<p>Kendall, M.G., and A. Stuart. (1991).
<em>The Advanced Theory of Statistics, Volume 2: Inference and Relationship</em>.
Fifth Edition. Oxford University Press, New York.
</p>
<p>Kim, P.J., and R.I. Jennrich. (1973).
Tables of the Exact Sampling Distribution of the Two Sample Kolmogorov-Smirnov Criterion.
In Harter, H.L., and D.B. Owen, eds. <em>Selected Tables in Mathematical Statistics, Vol. 1</em>.
American Mathematical Society, Providence, Rhode Island, pp.79-170.
</p>
<p>Kolmogorov, A.N. (1933). Sulla determinazione empirica di una legge di distribuzione.
<em>Giornale dell' Istituto Italiano degle Attuari</em> <b>4</b>, 83-91.
</p>
<p>Marsaglia, G., W.W. Tsang, and J. Wang. (2003). Evaluating Kolmogorov's distribution.
<em>Journal of Statistical Software</em>, <b>8</b>(18).
<a href="https://doi.org/10.18637/jss.v008.i18">doi:10.18637/jss.v008.i18</a>.
</p>
<p>Moore, D.S. (1986). Tests of Chi-Squared Type. In D'Agostino, R.B., and M.A. Stephens, eds.
<em>Goodness-of Fit Techniques</em>. Marcel Dekker, New York, pp.63-95.
</p>
<p>Pomeranz, J. (1973).
Exact Cumulative Distribution of the Kolmogorov-Smirnov Statistic for Small Samples (Algorithm 487).
<em>Collected Algorithms from ACM</em> ??, ???-???.
</p>
<p>Razali, N.M., and Y.B. Wah. (2011). Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov,
Lilliefors, and Anderson-Darling Tests.  <em>Journal of Statistical Modeling and Analytics</em>
<b>2</b>(1), 21–33.
</p>
<p>Romao, X., Delgado, R., and A. Costa. (2010). An Empirical Power Comparison of Univariate
Goodness-of-Fit Tests for Normality.  <em>Journal of Statistical Computation and Simulation</em>
<b>80</b>(5), 545–591.
</p>
<p>Royston, J.P. (1992a). Approximating the Shapiro-Wilk W-Test for Non-Normality.
<em>Statistics and Computing</em> <b>2</b>, 117-119.
</p>
<p>Royston, J.P. (1992b).
Estimation, Reference Ranges and Goodness of Fit for the Three-Parameter Log-Normal Distribution.
<em>Statistics in Medicine</em> <b>11</b>, 897-912.
</p>
<p>Royston, J.P. (1992c).
A Pocket-Calculator Algorithm for the Shapiro-Francia Test of Non-Normality: An Application to Medicine.
<em>Statistics in Medicine</em> <b>12</b>, 181-184.
</p>
<p>Royston, P. (1993). A Toolkit for Testing for Non-Normality in Complete and Censored Samples.
<em>The Statistician</em> <b>42</b>, 37-43.
</p>
<p>Ryan, T., and B. Joiner. (1973). <em>Normal Probability Plots and Tests for Normality</em>.
Technical Report, Pennsylvannia State University, Department of Statistics.
</p>
<p>Shapiro, S.S., and R.S. Francia. (1972). An Approximate Analysis of Variance Test for Normality.
<em>Journal of the American Statistical Association</em> <b>67</b>(337), 215-219.
</p>
<p>Shapiro, S.S., and M.B. Wilk. (1965). An Analysis of Variance Test for Normality (Complete Samples).
<em>Biometrika</em> <b>52</b>, 591-611.
</p>
<p>Smirnov, N.V. (1939).
Estimate of Deviation Between Empirical Distribution Functions in Two Independent Samples.
<em>Bulletin Moscow University</em> <b>2</b>(2), 3-16.
</p>
<p>Smirnov, N.V. (1948). Table for Estimating the Goodness of Fit of Empirical Distributions.
<em>Annals of Mathematical Statistics</em> <b>19</b>, 279-281.
</p>
<p>Stephens, M.A. (1970).
Use of the Kolmogorov-Smirnov, Cramer-von Mises and Related Statistics Without Extensive Tables.
<em>Journal of the Royal Statistical Society, Series B</em>, <b>32</b>, 115-122.
</p>
<p>Stephens, M.A. (1974). EDF Statistics for Goodness of Fit and Some Comparisons.
<em>Journal of the American Statistical Association</em> <b>69</b>, 730-737.
</p>
<p>Stephens, M.A. (1986a). Tests Based on EDF Statistics. In D'Agostino, R. B., and M.A. Stevens, eds.
<em>Goodness-of-Fit Techniques</em>. Marcel Dekker, New York.
</p>
<p>Thode Jr., H.C. (2002). <em>Testing for Normality</em>. Marcel Dekker, New York.
</p>
<p>USEPA. (2015).  <em>ProUCL Version 5.1.002 Technical Guide</em>.  EPA/600/R-07/041, October 2015.
Office of Research and Development. U.S. Environmental Protection Agency, Washington, D.C.
</p>
<p>Verrill, S., and R.A. Johnson. (1987).
The Asymptotic Equivalence of Some Modified Shapiro-Wilk Statistics – Complete and Censored Sample Cases.
<em>The Annals of Statistics</em> <b>15</b>(1), 413-419.
</p>
<p>Verrill, S., and R.A. Johnson. (1988).
Tables and Large-Sample Distribution Theory for Censored-Data Correlation Statistics for Testing Normality.
<em>Journal of the American Statistical Association</em> <b>83</b>, 1192-1197.
</p>
<p>Weisberg, S., and C. Bingham. (1975).
An Approximate Analysis of Variance Test for Non-Normality Suitable for Machine Calculation.
<em>Technometrics</em> <b>17</b>, 133-134.
</p>
<p>Wilk, M.B., and S.S. Shapiro. (1968). The Joint Assessment of Normality of Several Independent
Samples. <em>Technometrics</em>, <b>10</b>(4), 825-839.
</p>
<p>Zar, J.H. (2010). <em>Biostatistical Analysis</em>. Fifth Edition.
Prentice-Hall, Upper Saddle River, NJ.
</p>


<h3>See Also</h3>

<p><code>rosnerTest</code>, <code>gof.object</code>, <code>print.gof</code>,
<code>plot.gof</code>,
<code>shapiro.test</code>, <code>ks.test</code>, <code>chisq.test</code>,
Normal, Lognormal, Lognormal3,
Zero-Modified Normal, Zero-Modified Lognormal (Delta),
<code>enorm</code>, <code>elnorm</code>, <code>elnormAlt</code>,
<code>elnorm3</code>, <code>ezmnorm</code>, <code>ezmlnorm</code>,
<code>ezmlnormAlt</code>, <code>qqPlot</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">  # Generate 20 observations from a gamma distribution with
  # parameters shape = 2 and scale = 3 then run various
  # goodness-of-fit tests.
  # (Note:  the call to set.seed lets you reproduce this example.)

  set.seed(47)
  dat &lt;- rgamma(20, shape = 2, scale = 3)

  # Shapiro-Wilk generalized goodness-of-fit test
  #----------------------------------------------
  gof.list &lt;- gofTest(dat, distribution = "gamma")
  gof.list

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Shapiro-Wilk GOF Based on
  #                                 Chen &amp; Balakrisnan (1995)
  #
  #Hypothesized Distribution:       Gamma
  #
  #Estimated Parameter(s):          shape = 1.909462
  #                                 scale = 4.056819
  #
  #Estimation Method:               mle
  #
  #Data:                            dat
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  W = 0.9834958
  #
  #Test Statistic Parameter:        n = 20
  #
  #P-value:                         0.970903
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Gamma Distribution.

  dev.new()
  plot(gof.list)

  #----------

  # Redo the example above, but use the bias-corrected mle

  gofTest(dat, distribution = "gamma",
    est.arg.list = list(method = "bcmle"))

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Shapiro-Wilk GOF Based on
  #                                 Chen &amp; Balakrisnan (1995)
  #
  #Hypothesized Distribution:       Gamma
  #
  #Estimated Parameter(s):          shape = 1.656376
  #                                 scale = 4.676680
  #
  #Estimation Method:               bcmle
  #
  #Data:                            dat
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  W = 0.9834346
  #
  #Test Statistic Parameter:        n = 20
  #
  #P-value:                         0.9704046
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Gamma Distribution.

  #----------

  # Komogorov-Smirnov goodness-of-fit test (pre-specified parameters)
  #------------------------------------------------------------------

  gofTest(dat, test = "ks", distribution = "gamma",
    param.list = list(shape = 2, scale = 3))

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Kolmogorov-Smirnov GOF
  #
  #Hypothesized Distribution:       Gamma(shape = 2, scale = 3)
  #
  #Data:                            dat
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  ks = 0.2313878
  #
  #Test Statistic Parameter:        n = 20
  #
  #P-value:                         0.2005083
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Gamma(shape = 2, scale = 3)
  #                                 Distribution.

  #----------

  # ProUCL Version of Komogorov-Smirnov goodness-of-fit test
  # for a Gamma Distribution (estimated parameters)
  #---------------------------------------------------------

  gofTest(dat, test = "proucl.ks.gamma", distribution = "gamma")

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     ProUCL Kolmogorov-Smirnov Gamma GOF
  #
  #Hypothesized Distribution:       Gamma
  #
  #Estimated Parameter(s):          shape = 1.909462
  #                                 scale = 4.056819
  #
  #Estimation Method:               MLE
  #
  #Data:                            dat
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  D = 0.0988692
  #
  #Test Statistic Parameter:        n = 20
  #
  #Critical Values:                 D.0.01 = 0.228
  #                                 D.0.05 = 0.196
  #                                 D.0.10 = 0.180
  #
  #P-value:                         &gt;= 0.10
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Gamma Distribution.

  #----------

  # Chi-squared goodness-of-fit test (estimated parameters)
  #--------------------------------------------------------

  gofTest(dat, test = "chisq", distribution = "gamma", n.classes = 4)

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Chi-square GOF
  #
  #Hypothesized Distribution:       Gamma
  #
  #Estimated Parameter(s):          shape = 1.909462
  #                                 scale = 4.056819
  #
  #Estimation Method:               mle
  #
  #Data:                            dat
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  Chi-square = 1.2
  #
  #Test Statistic Parameter:        df = 1
  #
  #P-value:                         0.2733217
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Gamma Distribution.

  #----------
  # Clean up

  rm(dat, gof.list)
  graphics.off()

  #--------------------------------------------------------------------

  # Example 10-2 of USEPA (2009, page 10-14) gives an example of
  # using the Shapiro-Wilk test to test the assumption of normality
  # for nickel concentrations (ppb) in groundwater collected over
  # 4 years.  The data for this example are stored in
  # EPA.09.Ex.10.1.nickel.df.

  EPA.09.Ex.10.1.nickel.df
  #   Month   Well Nickel.ppb
  #1      1 Well.1       58.8
  #2      3 Well.1        1.0
  #3      6 Well.1      262.0
  #4      8 Well.1       56.0
  #5     10 Well.1        8.7
  #6      1 Well.2       19.0
  #7      3 Well.2       81.5
  #8      6 Well.2      331.0
  #9      8 Well.2       14.0
  #10    10 Well.2       64.4
  #11     1 Well.3       39.0
  #12     3 Well.3      151.0
  #13     6 Well.3       27.0
  #14     8 Well.3       21.4
  #15    10 Well.3      578.0
  #16     1 Well.4        3.1
  #17     3 Well.4      942.0
  #18     6 Well.4       85.6
  #19     8 Well.4       10.0
  #20    10 Well.4      637.0

  # Test for a normal distribution:
  #--------------------------------

  gof.list &lt;- gofTest(Nickel.ppb ~ 1, data = EPA.09.Ex.10.1.nickel.df)
  gof.list

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Shapiro-Wilk GOF
  #
  #Hypothesized Distribution:       Normal
  #
  #Estimated Parameter(s):          mean = 169.5250
  #                                 sd   = 259.7175
  #
  #Estimation Method:               mvue
  #
  #Data:                            Nickel.ppb
  #
  #Data Source:                     EPA.09.Ex.10.1.nickel.df
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  W = 0.6788888
  #
  #Test Statistic Parameter:        n = 20
  #
  #P-value:                         2.17927e-05
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Normal Distribution.

  dev.new()
  plot(gof.list)

  #----------

  # Test for a lognormal distribution:
  #-----------------------------------

  gofTest(Nickel.ppb ~ 1, data = EPA.09.Ex.10.1.nickel.df,
    dist = "lnorm")

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Shapiro-Wilk GOF
  #
  #Hypothesized Distribution:       Lognormal
  #
  #Estimated Parameter(s):          meanlog = 3.918529
  #                                 sdlog   = 1.801404
  #
  #Estimation Method:               mvue
  #
  #Data:                            Nickel.ppb
  #
  #Data Source:                     EPA.09.Ex.10.1.nickel.df
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  W = 0.978946
  #
  #Test Statistic Parameter:        n = 20
  #
  #P-value:                         0.9197735
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Lognormal Distribution.

  #----------

  # Test for a lognormal distribution, but use the
  # Mean and CV parameterization:
  #-----------------------------------------------

  gofTest(Nickel.ppb ~ 1, data = EPA.09.Ex.10.1.nickel.df,
    dist = "lnormAlt")

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     Shapiro-Wilk GOF
  #
  #Hypothesized Distribution:       Lognormal
  #
  #Estimated Parameter(s):          mean = 213.415628
  #                                 cv   =   2.809377
  #
  #Estimation Method:               mvue
  #
  #Data:                            Nickel.ppb
  #
  #Data Source:                     EPA.09.Ex.10.1.nickel.df
  #
  #Sample Size:                     20
  #
  #Test Statistic:                  W = 0.978946
  #
  #Test Statistic Parameter:        n = 20
  #
  #P-value:                         0.9197735
  #
  #Alternative Hypothesis:          True cdf does not equal the
  #                                 Lognormal Distribution.

  #----------
  # Clean up

  rm(gof.list)
  graphics.off()

  #---------------------------------------------------------------------------

  # Generate 20 observations from a normal distribution with mean=3 and sd=2, and
  # generate 10 observaions from a normal distribution with mean=2 and sd=2 then
  # test whether these sets of observations come from the same distribution.
  # (Note: the call to set.seed simply allows you to reproduce this example.)

  set.seed(300)
  dat1 &lt;- rnorm(20, mean = 3, sd = 2)
  dat2 &lt;- rnorm(10, mean = 1, sd = 2)
  gofTest(x = dat1, y = dat2, test = "ks")

  #Results of Goodness-of-Fit Test
  #-------------------------------
  #
  #Test Method:                     2-Sample K-S GOF
  #
  #Hypothesized Distribution:       Equal
  #
  #Data:                            x = dat1
  #                                 y = dat2
  #
  #Sample Sizes:                    n.x = 20
  #                                 n.y = 10
  #
  #Test Statistic:                  ks = 0.7
  #
  #Test Statistic Parameters:       n = 20
  #                                 m = 10
  #
  #P-value:                         0.001669561
  #
  #Alternative Hypothesis:          The cdf of 'dat1' does not equal
  #                                 the cdf of 'dat2'.

  #----------
  # Clean up

  rm(dat1, dat2)
</code></pre>


</div>