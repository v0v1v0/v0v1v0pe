<div class="container">

<table style="width: 100%;"><tr>
<td>discretize</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Discretize Continuous Random Variables</h2>

<h3>Description</h3>

<p><code>discretize</code> puts observations from a continuous random variable 
into bins and returns the corresponding vector of counts.
</p>
<p><code>discretize2d</code> puts observations from a pair of continuous random variables 
into bins and returns the corresponding table of counts.
</p>


<h3>Usage</h3>

<pre><code class="language-R">discretize( x, numBins, r=range(x) )
discretize2d( x1, x2, numBins1, numBins2, r1=range(x1), r2=range(x2) )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>vector of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x1</code></td>
<td>
<p>vector of observations for the first random variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x2</code></td>
<td>
<p>vector of observations for the second random variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numBins</code></td>
<td>
<p>number of bins.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numBins1</code></td>
<td>
<p>number of bins for the first random variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numBins2</code></td>
<td>
<p>number of bins for the second random variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r</code></td>
<td>
<p>range of the random variable (default: observed range).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r1</code></td>
<td>
<p>range of the first random variable (default: observed range).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r2</code></td>
<td>
<p>range of the second random variable (default: observed range).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The bins for a random variable all have the same width. It is determined by the length of the range divided by the number of bins.
</p>


<h3>Value</h3>

<p><code>discretize</code> returns a vector containing the counts for each bin.
</p>
<p><code>discretize2d</code> returns a matrix containing the counts for each bin. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code>entropy</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># load entropy library 
library("entropy")

### 1D example ####

# sample from continuous uniform distribution
x1 = runif(10000)
hist(x1, xlim=c(0,1), freq=FALSE)

# discretize into 10 categories
y1 = discretize(x1, numBins=10, r=c(0,1))
y1

# compute entropy from counts
entropy(y1) # empirical estimate near theoretical maximum
log(10) # theoretical value for discrete uniform distribution with 10 bins 

# sample from a non-uniform distribution 
x2 = rbeta(10000, 750, 250)
hist(x2, xlim=c(0,1), freq=FALSE)

# discretize into 10 categories and estimate entropy
y2 = discretize(x2, numBins=10, r=c(0,1))
y2
entropy(y2) # almost zero

### 2D example ####

# two independent random variables
x1 = runif(10000)
x2 = runif(10000)

y2d = discretize2d(x1, x2, numBins1=10, numBins2=10)
sum(y2d)

# joint entropy
H12 = entropy(y2d )
H12
log(100) # theoretical maximum for 10x10 table

# mutual information
mi.empirical(y2d) # approximately zero


# another way to compute mutual information

# compute marginal entropies
H1 = entropy(rowSums(y2d))
H2 = entropy(colSums(y2d))

H1+H2-H12 # mutual entropy

</code></pre>


</div>