<div class="container">

<table style="width: 100%;"><tr>
<td>energy.hclust</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Hierarchical Clustering by Minimum (Energy) E-distance </h2>

<h3>Description</h3>

<p>Performs hierarchical clustering by minimum (energy) E-distance method.
</p>


<h3>Usage</h3>

<pre><code class="language-R">    energy.hclust(dst, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>dst</code></td>
<td>
<p><code>dist</code> object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>distance exponent</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Dissimilarities are <code class="reqn">d(x,y) = \|x-y\|^\alpha</code>,
where the exponent <code class="reqn">\alpha</code> is in the interval (0,2].
This function performs agglomerative hierarchical clustering.
Initially, each of the n singletons is a cluster. At each of n-1 steps, the
procedure merges the pair of clusters with minimum e-distance.
The e-distance between two clusters <code class="reqn">C_i, C_j</code> of sizes <code class="reqn">n_i, n_j</code> 
is given by
</p>
<p style="text-align: center;"><code class="reqn">e(C_i, C_j)=\frac{n_i n_j}{n_i+n_j}[2M_{ij}-M_{ii}-M_{jj}],
    </code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">M_{ij}=\frac{1}{n_i n_j}\sum_{p=1}^{n_i} \sum_{q=1}^{n_j}
       \|X_{ip}-X_{jq}\|^\alpha,</code>
</p>

<p><code class="reqn">\|\cdot\|</code> denotes Euclidean norm, and <code class="reqn">X_{ip}</code> denotes the p-th observation in the i-th cluster.
</p>
<p>The return value is an object of class <code>hclust</code>, so <code>hclust</code>
methods such as print or plot methods, <code>plclust</code>, and <code>cutree</code>
are available. See the documentation for <code>hclust</code>.
</p>
<p>The e-distance measures both the heterogeneity between clusters and the
homogeneity within clusters. <code class="reqn">\mathcal E</code>-clustering
(<code class="reqn">\alpha=1</code>) is particularly effective in
high dimension, and is more effective than some standard hierarchical
methods when clusters have equal means (see example below).
For other advantages see the references.
</p>
<p><code>edist</code> computes the energy distances for the result (or any partition)
and returns the cluster distances in a <code>dist</code> object. See the <code>edist</code>
examples.
</p>


<h3>Value</h3>

<p>An object of class <code>hclust</code> which describes the tree produced by
the clustering process. The object is a list with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>merge:</code></td>
<td>
<p> an n-1 by 2 matrix, where row i of <code>merge</code> describes the
merging of clusters at step i of the clustering. If an element j in the
row is negative, then observation -j was merged at this
stage. If j is positive then the merge was with the cluster
formed at the (earlier) stage j of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>height:</code></td>
<td>
<p>the clustering height: a vector of n-1 non-decreasing
real numbers (the e-distance between merging clusters)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>order:</code></td>
<td>
<p> a vector giving a permutation of the indices of
original observations suitable for plotting, in the sense that a
cluster plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>labels:</code></td>
<td>
<p> labels for each of the objects being clustered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call:</code></td>
<td>
<p> the call which produced the result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method:</code></td>
<td>
<p> the cluster method that has been used (e-distance).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist.method:</code></td>
<td>
<p> the distance that has been used to create <code>dst</code>.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Currently <code>stats::hclust</code> implements Ward's method by <code>method="ward.D2"</code>,
which applies the squared distances. That method was previously <code>"ward"</code>. 
Because both <code>hclust</code> and energy use the same type of Lance-Williams recursive formula to update cluster distances, now with the additional option <code>method="ward.D"</code> in <code>hclust</code>, the
energy distance method is easily implemented by <code>hclust</code>. (Some "Ward" algorithms do not use Lance-Williams, however). Energy clustering (with <code>alpha=1</code>) and "ward.D" now return the same result, except that the cluster heights of energy hierarchical clustering with <code>alpha=1</code> are two times the heights from <code>hclust</code>. In order to ensure compatibility with hclust methods, <code>energy.hclust</code> now passes arguments through to <code>hclust</code> after possibly applying the optional exponent to distance.
</p>


<h3>Author(s)</h3>

<p> Maria L. Rizzo <a href="mailto:mrizzo@bgsu.edu">mrizzo@bgsu.edu</a> and
Gabor J. Szekely
</p>


<h3>References</h3>

<p>Szekely, G. J. and Rizzo, M. L. (2005) Hierarchical Clustering
via Joint Between-Within Distances: Extending Ward's Minimum
Variance Method, <em>Journal of Classification</em> 22(2) 151-183.
<br><a href="https://doi.org/10.1007/s00357-005-0012-9">doi:10.1007/s00357-005-0012-9</a>
</p>
<p>Szekely, G. J. and Rizzo, M. L. (2004) Testing for Equal
Distributions in High Dimension, <em>InterStat</em>, November (5).
</p>
<p>Szekely, G. J. (2000) Technical Report 03-05:
<code class="reqn">\mathcal{E}</code>-statistics: Energy of
Statistical Samples, Department of Mathematics and Statistics, Bowling
Green State University.
</p>


<h3>See Also</h3>

<p><code>edist</code> <code>ksample.e</code> <code>eqdist.etest</code> <code>hclust</code></p>


<h3>Examples</h3>

<pre><code class="language-R">   ## Not run: 
   library(cluster)
   data(animals)
   plot(energy.hclust(dist(animals)))

   data(USArrests)
   ecl &lt;- energy.hclust(dist(USArrests))
   print(ecl)
   plot(ecl)
   cutree(ecl, k=3)
   cutree(ecl, h=150)

   ## compare performance of e-clustering, Ward's method, group average method
   ## when sampled populations have equal means: n=200, d=5, two groups
   z &lt;- rbind(matrix(rnorm(1000), nrow=200), matrix(rnorm(1000, 0, 5), nrow=200))
   g &lt;- c(rep(1, 200), rep(2, 200))
   d &lt;- dist(z)
   e &lt;- energy.hclust(d)
   a &lt;- hclust(d, method="average")
   w &lt;- hclust(d^2, method="ward.D2")
   list("E" = table(cutree(e, k=2) == g), "Ward" = table(cutree(w, k=2) == g),
    "Avg" = table(cutree(a, k=2) == g))
  
## End(Not run)
 </code></pre>


</div>