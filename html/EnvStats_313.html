<div class="container">

<table style="width: 100%;"><tr>
<td>HoskingEtAl1985</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Abstract:  Hosking et al. (1985)
</h2>

<h3>Description</h3>

<p>Detailed abstract of the manuscript: <br><br>
Hosking, J.R.M., J.R. Wallis, and E.F. Wood. (1985).  Estimation of the 
Generalized Extreme-Value Distribution by the Method of Probability-Weighted 
Moments.  <em>Technometrics</em> <b>27</b>(3), 251–261.
</p>


<h3>Details</h3>

<p><b>Abstract</b> <br>
Hosking et al. (1985) use the method of probability-weighted moments, 
introduced by Greenwood et al. (1979), to estimate the parameters of the 
generalized extreme value distribution (GEVD) with parameters 
<code>location=</code><code class="reqn">\eta</code>, <code>scale=</code><code class="reqn">\theta</code>, and 
<code>shape=</code><code class="reqn">\kappa</code>.  Hosking et al. (1985) derive the asymptotic 
distributions of the probability-weighted moment estimators (PWME), and compare 
the asymptotic and small-sample statistical properties (via computer simulation) 
of the PWME with maximum likelihood estimators (MLE) and Jenkinson's (1969) 
method of sextiles estimators (JSE).  They also compare the statistical 
properties of quantile estimators (which are based on the distribution parameter 
estimators).  Finally, they derive a test of the null hypothesis that the 
shape parameter is zero, and assess its performance via computer simulation.
</p>
<p>Hosking et al. (1985) note that when <code class="reqn">\kappa \le -1</code>, the moments and 
probability-weighted moments of the GEVD do not exist.  They also note that in 
practice the shape parameter usually lies between -1/2 and 1/2.
</p>
<p>Hosking et al. (1985) found that the asymptotic efficiency of the PWME 
(the limit as the sample size approaches infinity of the ratio of the 
variance of the MLE divided by the variance of the PWME) tends to 0 as the 
shape parameter approaches 1/2 or -1/2.  For values of <code class="reqn">\kappa</code> within the 
range <code class="reqn">[-0.2, 0.2]</code>, however, the efficiency of the estimator of location 
is close to 100
are greater than 70
Hosking et al. (1985) found that the asymptotic efficiency of the PWME is poor for 
<code class="reqn">\kappa</code> outside the range <code class="reqn">[-0.2, 0.2]</code>.
</p>
<p>For the small sample results, Hosking et al. (1985) considered several possible 
forms of the PWME (see equations (8)-(10) below).  The best overall results were 
given by the plotting-position PWME defined by equations (9) and (10) with 
<code class="reqn">a=0.35</code> and <code class="reqn">b=0</code>.
</p>
<p>Small sample results for estimating the parameters show that for <code class="reqn">n \ge 50</code> 
all three methods give almost identical results.  For <code class="reqn">n &lt; 50</code> the results 
for the different estimators are a bit different, but not dramatically so.  The 
MLE tends to be slightly less biased than the other two methods.  For estimating 
the shape parameter, the MLE has a slightly larger standard deviation, and the 
PWME has consistently the smallest standard deviation.
</p>
<p>Small sample results for estimating large quantiles show that for <code class="reqn">n \ge 100</code> 
all three methods are comparable.  For <code class="reqn">n &lt; 100</code> the PWME and JSE are 
comparable and in general have much smaller standard deviations than the MLE.  
All three methods are very inaccurate for estimating large quantiles in small 
samples, especially when <code class="reqn">\kappa &lt; 0</code>.
</p>
<p>Hosking et al. (1985) derive a test of the null hypothesis <code class="reqn">H_0: \kappa=0</code> 
based on the PWME of <code class="reqn">\kappa</code>. The test is performed by computing the 
statistic:
</p>
<p style="text-align: center;"><code class="reqn">z = \frac{\hat{\kappa_{pwme}}}{\sqrt{0.5663/n}} \;\;\;\; (1)</code>
</p>
 
<p>and comparing <code class="reqn">z</code> to a standard normal distribution (see 
<code>zTestGevdShape</code>).  Based on computer simulations using the 
plotting-position PWME, they found that a sample size of <code class="reqn">n \ge 25</code> ensures 
an adequate normal approximation.  They also found this test has power comparable 
to the modified likelihood-ratio test, which was found by Hosking (1984) to be 
the best overall test of <code class="reqn">H_0: \kappa=0</code> of the thirteen tests he considered.
<br></p>
<p><b>More Details</b>
</p>
<p><em>Probability-Weighted Moments and Parameters of the GEVD</em> <br>
The definition of a probability-weighted moment, introduced by 
Greenwood et al. (1979), is as follows.  Let <code class="reqn">X</code> denote a random variable 
with cdf <code class="reqn">F</code>, and let <code class="reqn">x(p)</code> denote the <code class="reqn">p</code>'th quantile of the 
distribution.  Then the <code class="reqn">ijk</code>'th probability-weighted moment is given by:
</p>
<p style="text-align: center;"><code class="reqn">M(i, j, k) = E[X^i F^j (1 - F)^k] = \int^1_0 [x(F)]^i F^j (1 - F)^k \, dF \;\;\;\; (2)</code>
</p>

<p>where <code class="reqn">i</code>, <code class="reqn">j</code>, and <code class="reqn">k</code> are real numbers.
</p>
<p>Hosking et al. (1985) set
</p>
<p style="text-align: center;"><code class="reqn">\beta_j = M(i, j, 0) \;\;\;\; (3)</code>
</p>

<p>and Greenwood et al. (1979) show that
</p>
<p style="text-align: center;"><code class="reqn">\beta_j = \frac{1}{j+1} E[X_{j+1:j+1}] \;\;\;\; (4)</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">E[X_{j+1:j+1}]</code>
</p>

<p>denotes the expected value of the <code class="reqn">j+1</code>'th order statistic (i.e., the maximum) 
in a sample of size <code class="reqn">j+1</code>.  Hosking et al. (1985) show that if <code class="reqn">X</code> has a 
GEVD with parameters <code>location=</code><code class="reqn">\eta</code>, <code>scale=</code><code class="reqn">\theta</code>, and 
<code>shape=</code><code class="reqn">\kappa</code>, where <code class="reqn">\kappa \ne 0</code>, then
</p>
<p style="text-align: center;"><code class="reqn">\beta_j = \frac{1}{j+1} \{\eta + \frac{\theta [1 - (j+1)^{-\kappa} \Gamma(1+\kappa)]}{\kappa} \} \;\;\;\; (5)</code>
</p>

<p>for <code class="reqn">\kappa &gt; -1</code>, where <code class="reqn">\Gamma()</code> denotes the 
gamma function.  Thus,
</p>
<p style="text-align: center;"><code class="reqn">\beta_0 = \eta + \frac{\theta [1 - \Gamma(1+\kappa)]}{\kappa} \;\;\;\; (6)</code>
</p>

<p style="text-align: center;"><code class="reqn">2\beta_1 - \beta_0 = \frac{\theta [\Gamma(1+\kappa)] (1 - 2^{-\kappa})}{\kappa} \;\;\;\; (7)</code>
</p>

<p style="text-align: center;"><code class="reqn">\frac{3\beta_2 - \beta_0}{2\beta_1 - \beta_0} = \frac{1 - 3^{-\kappa}}{1 - 2^{-kappa}} \;\;\;\; (8)</code>
</p>

<p><em>Estimating Distribution Parameters</em> <br>
Using the results of Landwehr et al. (1979), Hosking et al. (1985) show that 
given a random sample of <code class="reqn">n</code> values from some arbitrary distribution, an 
unbiased, distribution-free, and parameter-free estimator of the 
probability-weighted moment <code class="reqn">\beta_j = M(i, j, 0)</code> defined above is given by:
</p>
<p style="text-align: center;"><code class="reqn">b_j = \frac{1}{n} \sum^n_{i=j+1} x_{i,n} \frac{{i-1 \choose j}}{{n-1 \choose j}} \;\;\;\; (9)</code>
</p>

<p>where the quantity <code class="reqn">x_{i,n}</code> denotes the <code class="reqn">i</code>'th order statistic in the 
random sample of size <code class="reqn">n</code>.  Hosking et al. (1985) note that this estimator is 
closely related to U-statistics (Hoeffding, 1948; Lehmann, 1975, pp. 362-371). 
</p>
<p>An alternative “plotting position” estimator is given by:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\beta}_j[p_{i,n}] = \frac{1}{n} \sum^n_{i=1} p^j_{i,n} x_{i,n} \;\;\;\; (10)</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">p_{i,n} = \hat{F}(x_{i,n}) \;\;\;\; (11)</code>
</p>

<p>denotes the plotting position of the <code class="reqn">i</code>'th order statistic in the random 
sample of size <code class="reqn">n</code>, that is, a distribution-free estimate of the cdf of 
<code class="reqn">X</code> evaluated at the <code class="reqn">i</code>'th order statistic.  Typically, plotting 
positions have the form:
</p>
<p style="text-align: center;"><code class="reqn">p_{i,n} = \frac{i-a}{n+b} \;\;\;\; (12)</code>
</p>

<p>where <code class="reqn">b &gt; -a &gt; -1</code>.  For this form of plotting position, the 
plotting-position estimators in (10) are asymptotically equivalent to the 
U-statistic estimators in (9).
</p>
<p>Although the unbiased and plotting position estimators are asymptotically 
equivalent (Hosking, 1990), Hosking and Wallis (1995) recommend using the 
unbiased estimator for almost all applications because of its superior 
performance in small and moderate samples.
</p>
<p>Using equations (6)-(8) above, i.e., the three equations involving 
<code class="reqn">\beta_0</code>, <code class="reqn">\beta_1</code>, and <code class="reqn">\beta_2</code>, Hosking et al. (1985) define 
the probability-weighted moment estimators of 
<code class="reqn">\eta</code>, <code class="reqn">\theta</code>, and <code class="reqn">\kappa</code> as the solutions to these three 
simultaneous equations, with the values of the probability-weighted moments 
replaced by their estimated values (using either the unbiased or plotting posistion 
estiamtors in (9) and (10) above).  Hosking et al. (1985) note that the third 
equation (equation (8)) must be solved iteratively for the PWME of <code class="reqn">\kappa</code>.  
Using the unbiased estimators of the PWMEs to solve for <code class="reqn">\kappa</code>, the PWMEs 
of <code class="reqn">\eta</code> and <code class="reqn">\theta</code> are given by:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\eta}_{pwme} = b_0 + \frac{\hat{\theta}_{pwme} [\Gamma(1 + \hat{\kappa}_{pwme}) - 1]}{\hat{\kappa}_{pwme}} \;\;\;\; (13)</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{\theta}_{pwme} = \frac{(2b_1 - b_0)\hat{\kappa}_{pwme}}{\Gamma(1 + \hat{\kappa}_{pwme}) (1 - 2^{-\hat{\kappa}_{pwme}})} \;\;\;\; (14)</code>
</p>

<p>Hosking et al. (1985) show that when the unbiased estimates of the PWMEs are used 
to estimate the probability-weighted moments, the estimates of <code class="reqn">\theta</code> and 
<code class="reqn">\kappa</code> satisfy the feasibility criteria
</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}_{pwme} &gt; 0; \, \hat{\kappa}_{pwme} &gt; -1</code>
</p>

<p>almost surely.
</p>
<p>Hosking et al. (1985) show that the asymptotic distribution of the PWME is 
multivariate normal with mean equal to <code class="reqn">(\eta, \theta, \kappa)</code>, and they 
derive the formula for the asymptotic variance-covariance matrix as:
</p>
<p style="text-align: center;"><code class="reqn">V_{\hat{\eta}, \hat{\theta}, \hat{\kappa}} = \frac{1}{n} G V_{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2} G^T \;\;\;\; (15)</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">V_{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2}</code>
</p>

<p>denotes the variance-covariance matrix of the estimators of the probability-weighted 
moments defined in either equation (9) or (10) above (recall that these two 
estimators are asymptotically equivalent), and the matrix <code class="reqn">G</code> is defined by:
</p>
<p style="text-align: center;"><code class="reqn">G_{i1} = \frac{\partial \eta}{\partial \beta_{i-1}}, \, G_{i2} = \frac{\partial \theta}{\partial \beta_{i-1}}, \, G_{i3} = \frac{\partial \kappa}{\partial \beta_{i-1}} \;\;\;\; (16)</code>
</p>

<p>for <code class="reqn">i = 1, 2, 3</code>.  Hosking et al. (1985) provide formulas for the matrix
</p>
<p style="text-align: center;"><code class="reqn">V_{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2}</code>
</p>

<p>in Appendix C of their manuscript.  Note that there is a typographical error in 
equation (C.11) (Jon Hosking, personal communication, 1996).  In the second line 
of this equation, the quantity <code class="reqn">-(r+s)^{-k}</code> should be replaced with 
<code class="reqn">-(r+s)^{-2k}</code>.
</p>
<p>The matrix <code class="reqn">G</code> in equation (16) is not easily computed.  Its inverse, however, 
is easy to compute and then can be inverted numerically (Jon Hosking, 1996, 
personal communication). The inverse of <code class="reqn">G</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">G^{-1}_{i1} = \frac{\partial \beta_{i-1}{\partial \eta}}, \, G^{-1}_{i2} = \frac{\partial \beta_{i-1}{\partial \theta}}, \, G^{-1}_{i3} = \frac{\partial \beta_{i-1}{\partial \kappa}} \;\;\;\; (17)</code>
</p>
  
<p>and by equation (5) above it can be shown that:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\partial \beta_j}{\partial \eta} = \frac{1}{j+1} \;\;\;\; (18)</code>
</p>

<p style="text-align: center;"><code class="reqn">\frac{\partial \beta_j}{\partial \theta} =\frac{1 - (j+1)^{-\kappa}\Gamma(1+\kappa)}{(j+1)\kappa} \;\;\;\; (19)</code>
</p>

<p style="text-align: center;"><code class="reqn">\frac{\partial \beta_j}{\partial \kappa} = \frac{\theta}{j+1} \{ \frac{(j+1)^{-\kappa}[log(j+1)\Gamma(1+\kappa)-\Gamma^{'}(1+\kappa)]}{\kappa} - \frac{1 - (j+1)^{-\kappa}\Gamma(1+\kappa)}{\kappa^2} \} \;\;\;\; (20)</code>
</p>

<p>for <code class="reqn">i = 1, 2, 3</code>.
</p>
<p><em>Estimating Distribution Quantiles</em> <br>
If <code class="reqn">X</code> has a GEVD with parameters <code>location=</code><code class="reqn">\eta</code>, 
<code>scale=</code><code class="reqn">\theta</code>, and <code>shape=</code><code class="reqn">\kappa</code>, where <code class="reqn">\kappa \ne 0</code>, 
then the <code class="reqn">p</code>'th quantile of the distribution is given by:
</p>
<p style="text-align: center;"><code class="reqn">x(p) = \eta + \frac{\theta \{1 - [-log(p)]^{\kappa} \}}{\kappa} \;\;\;\; (21)</code>
</p>

<p><code class="reqn">(0 \le p \le 1)</code>.  Given estimated values of the location, scale, and shape 
parameters, the <code class="reqn">p</code>'th quantile of the distribution is estimated as:
</p>
<p style="text-align: center;"><code class="reqn">\hat{x}(p) = \hat{\eta} + \frac{\hat{\theta} \{1 - [-log(p)]^{\hat{\kappa}} \}}{\hat{\kappa}} \;\;\;\; (22)</code>
</p>



<h3>Author(s)</h3>

<p>Steven P. Millard (<a href="mailto:EnvStats@ProbStatInfo.com">EnvStats@ProbStatInfo.com</a>)
</p>


<h3>References</h3>

<p>Forbes, C., M. Evans, N. Hastings, and B. Peacock. (2011).  Statistical Distributions. 
Fourth Edition. John Wiley and Sons, Hoboken, NJ.
</p>
<p>Greenwood, J.A., J.M. Landwehr, N.C. Matalas, and J.R. Wallis. (1979).  
Probability Weighted Moments: Definition and Relation to Parameters of Several 
Distributions Expressible in Inverse Form.  <em>Water Resources Research</em> 
<b>15</b>(5), 1049–1054.
</p>
<p>Hoeffding, W. (1948).  A Class of Statistics with Asymptotically Normal 
Distribution.  <em>Annals of Mathematical Statistics</em> <b>19</b>, 293–325.
</p>
<p>Hosking, J.R.M. (1985).  Algorithm AS 215: Maximum-Likelihood Estimation of the 
Parameters of the Generalized Extreme-Value Distribution.  
<em>Applied Statistics</em> <b>34</b>(3), 301–310.
</p>
<p>Hosking, J.R.M. (1990).  <code class="reqn">L</code>-Moments:  Analysis and Estimation of 
Distributions Using Linear Combinations of Order Statistics.  <em>Journal of 
the Royal Statistical Society, Series B</em> <b>52</b>(1), 105–124.
</p>
<p>Hosking, J.R.M., and J.R. Wallis (1995).  A Comparison of Unbiased and 
Plotting-Position Estimators of <code class="reqn">L</code> Moments.  <em>Water Resources 
Research</em> <b>31</b>(8), 2019–2025.
</p>
<p>Jenkinson, A.F. (1969).  Statistics of Extremes. <em>Technical Note 98</em>, 
World Meteorological Office, Geneva.
</p>
<p>Johnson, N. L., S. Kotz, and A.W. Kemp. (1992).  
<em>Univariate Discrete Distributions</em>.  Second Edition.  
John Wiley and Sons, New York, pp.4-8.
</p>
<p>Johnson, N. L., S. Kotz, and N. Balakrishnan. (1995). 
<em>Continuous Univariate Distributions, Volume 2</em>. 
Second Edition. John Wiley and Sons, New York.
</p>
<p>Lehmann, E.L. (1975).  <em>Nonparametrics:  Statistical Methods Based on Ranks</em>.  
Holden-Day, Oakland, CA, 457pp.
</p>


<h3>See Also</h3>

<p>Generalized Extreme Value Distribution, <code>egevd</code>.
</p>


</div>