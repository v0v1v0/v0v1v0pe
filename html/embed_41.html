<div class="container">

<table style="width: 100%;"><tr>
<td>step_discretize_xgb</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Discretize numeric variables with XgBoost</h2>

<h3>Description</h3>

<p><code>step_discretize_xgb()</code> creates a <em>specification</em> of a recipe step that will
discretize numeric data (e.g. integers or doubles) into bins in a supervised
way using an XgBoost model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">step_discretize_xgb(
  recipe,
  ...,
  role = NA,
  trained = FALSE,
  outcome = NULL,
  sample_val = 0.2,
  learn_rate = 0.3,
  num_breaks = 10,
  tree_depth = 1,
  min_n = 5,
  rules = NULL,
  skip = FALSE,
  id = rand_id("discretize_xgb")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>recipe</code></td>
<td>
<p>A recipe object. The step will be added to the sequence of
operations for this recipe.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>One or more selector functions to choose which variables are
affected by the step. See <code>selections()</code> for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>role</code></td>
<td>
<p>Defaults to <code>"predictor"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trained</code></td>
<td>
<p>A logical to indicate if the quantities for preprocessing have
been estimated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outcome</code></td>
<td>
<p>A call to <code>vars</code> to specify which variable is used as the
outcome to train XgBoost models in order to discretize explanatory
variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_val</code></td>
<td>
<p>Share of data used for validation (with early stopping) of
the learned splits (the rest is used for training). Defaults to 0.20.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>The rate at which the boosting algorithm adapts from
iteration-to-iteration. Corresponds to <code>eta</code> in the <span class="pkg">xgboost</span> package.
Defaults to 0.3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_breaks</code></td>
<td>
<p>The <em>maximum</em> number of discrete bins to bucket continuous
features. Corresponds to <code>max_bin</code> in the <span class="pkg">xgboost</span> package. Defaults
to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tree_depth</code></td>
<td>
<p>The maximum depth of the tree (i.e. number of splits).
Corresponds to <code>max_depth</code> in the <span class="pkg">xgboost</span> package. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_n</code></td>
<td>
<p>The minimum number of instances needed to be in each node.
Corresponds to <code>min_child_weight</code> in the <span class="pkg">xgboost</span> package. Defaults to
5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rules</code></td>
<td>
<p>The splitting rules of the best XgBoost tree to retain for each
variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>skip</code></td>
<td>
<p>A logical. Should the step be skipped when the recipe is baked by
<code>recipes::bake()</code>? While all operations are baked when <code>recipes::prep()</code> is
run, some operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when using <code>skip = TRUE</code> as it may affect the computations for subsequent operations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>id</code></td>
<td>
<p>A character string that is unique to this step to identify it.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>step_discretize_xgb()</code> creates non-uniform bins from numerical variables by
utilizing the information about the outcome variable and applying the xgboost
model. It is advised to impute missing values before this step. This step is
intended to be used particularly with linear models because thanks to
creating non-uniform bins it becomes easier to learn non-linear patterns from
the data.
</p>
<p>The best selection of buckets for each variable is selected using an internal
early stopping scheme implemented in the <span class="pkg">xgboost</span> package, which makes
this discretization method prone to overfitting.
</p>
<p>The pre-defined values of the underlying xgboost learns good and reasonably
complex results. However, if one wishes to tune them the recommended path
would be to first start with changing the value of <code>num_breaks</code> to e.g.: 20
or 30. If that doesn't give satisfactory results one could experiment with
modifying the <code>tree_depth</code> or <code>min_n</code> parameters. Note that it is not
recommended to tune <code>learn_rate</code> simultaneously with other parameters.
</p>
<p>This step requires the <span class="pkg">xgboost</span> package. If not installed, the step will
stop with a note about installing the package.
</p>
<p>Note that the original data will be replaced with the new bins.
</p>


<h3>Value</h3>

<p>An updated version of <code>recipe</code> with the new step added to the
sequence of any existing operations.
</p>


<h3>Tidying</h3>

<p>When you <code>tidy()</code> this step, a tibble is retruned with
columns <code>terms</code>, <code>value</code>, and <code>id</code>:
</p>

<dl>
<dt>terms</dt>
<dd>
<p>character, the selectors or variables selected</p>
</dd>
<dt>value</dt>
<dd>
<p>numeric, location of the splits</p>
</dd>
<dt>id</dt>
<dd>
<p>character, id of this step</p>
</dd>
</dl>
<h3>Tuning Parameters</h3>

<p>This step has 5 tuning parameters:
</p>

<ul>
<li> <p><code>sample_val</code>: Proportion of data for validation (type: double, default: 0.2)
</p>
</li>
<li> <p><code>learn_rate</code>: Learning Rate (type: double, default: 0.3)
</p>
</li>
<li> <p><code>num_breaks</code>: Number of Cut Points (type: integer, default: 10)
</p>
</li>
<li> <p><code>tree_depth</code>: Tree Depth (type: integer, default: 1)
</p>
</li>
<li> <p><code>min_n</code>: Minimal Node Size (type: integer, default: 5)
</p>
</li>
</ul>
<h3>Case weights</h3>

<p>This step performs an supervised operation that can utilize case weights.
To use them, see the documentation in recipes::case_weights and the examples on
<code>tidymodels.org</code>.
</p>


<h3>See Also</h3>

<p><code>step_discretize_cart()</code>, <code>recipes::recipe()</code>,
<code>recipes::prep()</code>, <code>recipes::bake()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(rsample)
library(recipes)
data(credit_data, package = "modeldata")

set.seed(1234)
split &lt;- initial_split(credit_data[1:1000, ], strata = "Status")

credit_data_tr &lt;- training(split)
credit_data_te &lt;- testing(split)

xgb_rec &lt;-
  recipe(Status ~ Income + Assets, data = credit_data_tr) %&gt;%
  step_impute_median(Income, Assets) %&gt;%
  step_discretize_xgb(Income, Assets, outcome = "Status")

xgb_rec &lt;- prep(xgb_rec, training = credit_data_tr)

bake(xgb_rec, credit_data_te, Assets)

</code></pre>


</div>