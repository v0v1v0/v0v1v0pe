<div class="container">

<table style="width: 100%;"><tr>
<td>pwMoment</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Estimate Probability-Weighted Moments
</h2>

<h3>Description</h3>

<p>Estimate the <code class="reqn">1jk</code>'th probability-weighted moment from a random sample, 
where either <code class="reqn">j = 0</code>, <code class="reqn">k = 0</code>, or both.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  pwMoment(x, j = 0, k = 0, method = "unbiased", 
    plot.pos.cons = c(a = 0.35, b = 0), na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>numeric vector of observations. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>j, k</code></td>
<td>

<p>non-negative integers specifying the order of the moment.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>character string specifying what method to use to compute the 
probability-weighted moment.  The possible values are <code>"unbiased"</code> 
(method based on the U-statistic; the default), or <code>"plotting.position"</code> 
(method based on the plotting position formula).  See the DETAILS section for 
more information.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot.pos.cons</code></td>
<td>

<p>numeric vector of length 2 specifying the constants used in the formula for the 
plotting positions when <code>method="plotting.position"</code>.  The default value is 
<code>plot.pos.cons=c(a=0.35, b=0)</code>.  If this vector has a names attribute with 
the value <code>c("a","b")</code> or <code>c("b","a")</code>, then the elements will be 
matched by name in the formula for computing the plotting positions.  Otherwise, 
the first element is mapped to the name <code>"a"</code> and the second element to the 
name <code>"b"</code>.  See the DETAILS section for more information.  This argument is 
ignored if <code>method="ubiased"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>

<p>logical scalar indicating whether to remove missing values from <code>x</code>.  
If <code>na.rm=FALSE</code> (the default) and <code>x</code> contains missing values, 
then a missing value (<code>NA</code>) is returned.  If <code>na.rm=TRUE</code>, missing 
values are removed from <code>x</code> prior to computing the probability-weighted 
moment.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The definition of a probability-weighted moment, introduced by 
Greenwood et al. (1979), is as follows.  Let <code class="reqn">X</code> denote a random variable 
with cdf <code class="reqn">F</code>, and let <code class="reqn">x(p)</code> denote the <code class="reqn">p</code>'th quantile of the 
distribution.  Then the <code class="reqn">ijk</code>'th probability-weighted moment is given by:
</p>
<p style="text-align: center;"><code class="reqn">M(i, j, k) = E[X^i F^j (1 - F)^k] = \int^1_0 [x(F)]^i F^j (1 - F)^k \, dF</code>
</p>

<p>where <code class="reqn">i</code>, <code class="reqn">j</code>, and <code class="reqn">k</code> are real numbers.  Note that if <code class="reqn">i</code> is a 
nonnegative integer, then <code class="reqn">M(i, 0, 0)</code> is the conventional <code class="reqn">i</code>'th moment 
about the origin.
</p>
<p>Greenwood et al. (1979) state that in the special case where <code class="reqn">i</code>, <code class="reqn">j</code>, and 
<code class="reqn">k</code> are nonnegative integers:
</p>
<p style="text-align: center;"><code class="reqn">M(i, j, k) = B(j + 1, k + 1) E[X^i_{j+1, j+k+1}]</code>
</p>

<p>where <code class="reqn">B(a, b)</code> denotes the beta function evaluated at 
<code class="reqn">a</code> and <code class="reqn">b</code>, and 
</p>
<p style="text-align: center;"><code class="reqn">E[X^i_{j+1, j+k+1}]</code>
</p>
     
<p>denotes the <code class="reqn">i</code>'th moment about the origin of the <code class="reqn">(j + 1)</code>'th order 
statistic for a sample of size <code class="reqn">(j + k + 1)</code>. In particular, 
</p>
<p style="text-align: center;"><code class="reqn">M(1, 0, k) = \frac{1}{k+1} E[X_{1, k+1}]</code>
</p>

<p style="text-align: center;"><code class="reqn">M(1, j, 0) = \frac{1}{j+1} E[X_{j+1, j+1}]</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">E[X_{1, k+1}]</code>
</p>

<p>denotes the expected value of the first order statistic (i.e., the minimum) in a 
sample of size <code class="reqn">(k + 1)</code>, and 
</p>
<p style="text-align: center;"><code class="reqn">E[X_{j+1, j+1}]</code>
</p>
 
<p>denotes the expected value of the <code class="reqn">(j+1)</code>'th order statistic (i.e., the maximum) 
in a sample of size <code class="reqn">(j+1)</code>.
</p>
<p><em>Unbiased Estimators</em> (<code>method="unbiased"</code>) <br>
Landwehr et al. (1979) show that, given a random sample of <code class="reqn">n</code> values from 
some arbitrary distribution, an unbiased, distribution-free, and parameter-free 
estimator of <code class="reqn">M(1, 0, k)</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">\hat{M}(1, 0, k) = \frac{1}{n} \sum^{n-k}_{i=1} x_{i,n} \frac{{n-i \choose k}}{{n-1 \choose k}}</code>
</p>

<p>where the quantity <code class="reqn">x_{i,n}</code> denotes the <code class="reqn">i</code>'th order statistic in the 
random sample of size <code class="reqn">n</code>.  Hosking et al. (1985) note that this estimator is 
closely related to U-statistics (Hoeffding, 1948; Lehmann, 1975, pp. 362-371).  
Hosking et al. (1985) note that an unbiased, distribution-free, and parameter-free 
estimator of <code class="reqn">M(1, j, 0)</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">\hat{M}(1, j, 0) = \frac{1}{n} \sum^n_{i=j+1} x_{i,n} \frac{{i-1 \choose j}}{{n-1 \choose j}}</code>
</p>

<p><br></p>
<p><em>Plotting-Position Estimators</em> (<code>method="plotting.position"</code>) <br>
Hosking et al. (1985) propose alternative estimators of <code class="reqn">M(1, 0, k)</code> and 
<code class="reqn">M(1, j, 0)</code> based on plotting positions:
</p>
<p style="text-align: center;"><code class="reqn">\hat{M}(1, 0, k) = \frac{1}{n} \sum^n_{i=1} (1 - p_{i,n})^k x_{i,n}</code>
</p>

<p style="text-align: center;"><code class="reqn">\hat{M}(1, j, 0) = \frac{1}{n} \sum^n_{i=1} p_{i,n}^j x_{i,n}</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">p_{i,n} = \hat{F}(x_{i,n})</code>
</p>

<p>denotes the plotting position of the <code class="reqn">i</code>'th order statistic in the random 
sample of size <code class="reqn">n</code>, that is, a distribution-free estimate of the cdf of 
<code class="reqn">X</code> evaluated at the <code class="reqn">i</code>'th order statistic.  Typically, plotting 
positions have the form:
</p>
<p style="text-align: center;"><code class="reqn">p_{i,n} = \frac{i-a}{n+b}</code>
</p>

<p>where <code class="reqn">b &gt; -a &gt; -1</code>.  For this form of plotting position, the 
plotting-position estimators are asymptotically equivalent to the U-statistic 
estimators.
</p>


<h3>Value</h3>

<p>A numeric scalar–the value of the <code class="reqn">1jk</code>'th probability-weighted moment 
as defined by Greenwood et al. (1979).
</p>


<h3>Note</h3>

<p>Greenwood et al. (1979) introduced the concept of probability-weighted moments 
as a tool to derive estimates of distribution parameters for distributions that 
can be (perhaps only be) expressed in inverse form.  The term “inverse form” 
simply means that instead of characterizing the distribution by the formula for 
its cumulative distribution function (cdf), the distribution is characterized by 
the formula for the <code class="reqn">p</code>'th quantile (<code class="reqn">0 \le p \le 1</code>).
</p>
<p>For distributions that can only be expressed in inverse form, moment estimates of 
their parameters are not available, and maximum likelihood estimates are not easy 
to compute.  Greenwood et al. (1979) show that in these cases, it is often possible 
to derive expressions for the distribution parameters in terms of 
probability-weighted moments.  Thus, for these cases the distribution parameters 
can be estimated based on the sample probability-weighted moments, which are fairly 
easy to compute.  Furthermore, for distributions whose parameters can be expressed 
as functions of conventional moments, the method of probability-weighted moments 
provides an alternative to method of moments and maximum likelihood estimators.
</p>
<p>Landwehr et al. (1979) use the method of probability-weighted moments to estimate 
the parameters of the Type I Extreme Value (Gumbel) distribution.
</p>
<p>Hosking et al. (1985) use the method of probability-weighted moments to estimate 
the parameters of the generalized extreme value distribution.
</p>
<p>Hosking (1990) and Hosking and Wallis (1995) show the relationship between 
probabiity-weighted moments and L-moments.
</p>
<p>Hosking and Wallis (1995) recommend using the unbiased estimators of 
probability-weighted moments for almost all applications.
</p>


<h3>Author(s)</h3>

<p>Steven P. Millard (<a href="mailto:EnvStats@ProbStatInfo.com">EnvStats@ProbStatInfo.com</a>)
</p>


<h3>References</h3>

<p>Greenwood, J.A., J.M. Landwehr, N.C. Matalas, and J.R. Wallis. (1979).  
Probability Weighted Moments: Definition and Relation to Parameters of Several 
Distributions Expressible in Inverse Form.  <em>Water Resources Research</em> 
<b>15</b>(5), 1049–1054. 
</p>
<p>Hoeffding, W. (1948).  A Class of Statistics with Asymptotically Normal 
Distribution.  <em>Annals of Mathematical Statistics</em> <b>19</b>, 293–325.
</p>
<p>Hosking, J.R.M. (1990).  L-Moments: Analysis and Estimation of Distributions 
Using Linear Combinations of Order Statistics.  <em>Journal of the Royal 
Statistical Society, Series B</em> <b>52</b>(1), 105–124.
</p>
<p>Hosking, J.R.M., and J.R. Wallis (1995).  A Comparison of Unbiased and 
Plotting-Position Estimators of L Moments.  <em>Water Resources Research</em> 
<b>31</b>(8), 2019–2025.
</p>
<p>Hosking, J.R.M., J.R. Wallis, and E.F. Wood. (1985).  Estimation of the 
Generalized Extreme-Value Distribution by the Method of 
Probability-Weighted Moments.  <em>Technometrics</em> <b>27</b>(3), 251–261.
</p>
<p>Landwehr, J.M., N.C. Matalas, and J.R. Wallis. (1979).  Probability Weighted 
Moments Compared With Some Traditional Techniques in Estimating Gumbel 
Parameters and Quantiles.  <em>Water Resources Research</em> <b>15</b>(5), 
1055–1064.
</p>
<p>Lehmann, E.L. (1975).  <em>Nonparametrics: Statistical Methods Based on Ranks</em>.  
Holden-Day, Oakland, CA, pp.362-371.
</p>


<h3>See Also</h3>

<p><code>eevd</code>, <code>egevd</code>, <code>lMoment</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">  # Generate 20 observations from a generalized extreme value distribution 
  # with parameters location=10, scale=2, and shape=.25, then compute the 
  # 0'th, 1'st and 2'nd probability-weighted moments. 
  # (Note: the call to set.seed simply allows you to reproduce this example.)

  set.seed(250) 
  dat &lt;- rgevd(20, location = 10, scale = 2, shape = 0.25) 

  pwMoment(dat) 
  #[1] 10.59556
 
  pwMoment(dat, 1) 
  #[1] 5.798481
  
  pwMoment(dat, 2) 
  #[1] 4.060574
  
  pwMoment(dat, k = 1) 
  #[1] 4.797081
 
  pwMoment(dat, k = 2) 
  #[1] 3.059173
 
  pwMoment(dat, 1, method = "plotting.position") 
  # [1] 5.852913
 
  pwMoment(dat, 1, method = "plotting.position", 
    plot.pos = c(.325, 1)) 
  #[1] 5.586817 

  #----------

  # Clean Up
  #---------
  rm(dat)
</code></pre>


</div>